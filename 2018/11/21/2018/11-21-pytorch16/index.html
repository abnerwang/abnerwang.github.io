<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（十六） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（十六） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（十六）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 21, 2018</div><div class="post-content"><p>1 . <code>gerattr</code>、<code>setattr</code> 与 <code>__getattr__</code>、<code>__setattr__</code>。  </p>
<ul>
<li><code>result = obj.name</code> 会调用 buildin 函数 <code>getattr(obj, &#39;name&#39;)</code>，如果该属性找不到，会调用 <code>obj.__getattr__(&#39;name&#39;)</code>，没有实现 <code>__getattr__</code> 或者 <code>__getattr__</code> 也无法处理的就会 <code>raise AttributeError</code>;</li>
<li><code>obj.name = value</code> 会调用 buildin 函数 <code>setattr(obj, &#39;name&#39;, value)</code>，如果 obj 对象实现了 <code>__setattr__</code> 方法，<code>setattr</code> 会直接调用 <code>obj.__setattr__(&#39;name&#39;, value)</code>。</li>
</ul>
<p><code>nn.Module</code> 实现了自定义的 <code>__setattr__</code> 函数，当执行 <code>module.name = value</code> 时，会在 <code>__setattr__</code> 中判断 value 是否为 <code>Parameter</code> 或 <code>nn.Module</code> 对象，如果是则将这些对象加到 <code>_parameters</code> 和 <code>_modules</code> 这两个字典中，而如果是其他类型的对象，如 <code>Variable</code>、<code>list</code>、<code>dict</code> 等，则调用默认的操作，将这个值保存在 <code>__dict__</code> 中。  </p>
<p> Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">module = nn.Module()</span><br><span class="line">module.param = nn.Parameter(t.ones(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(module._parameters)</span><br><span class="line"></span><br><span class="line">submodule1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">submodule2 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">module_list = [submodule1, submodule2]</span><br><span class="line"><span class="comment"># 对于 list 对象，调用 buildin 函数，保存在 __dict__ 中</span></span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'_modules: '</span>, module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']:"</span>, module.__dict__.get(<span class="string">'submodules'</span>))</span><br><span class="line"></span><br><span class="line">module_list = nn.ModuleList(module_list)</span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'ModuleList is instance of nn.Module: '</span>, isinstance(module_list, nn.Module))</span><br><span class="line">print(<span class="string">'_modules: '</span>, module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']: "</span>, module.__dict__.get(<span class="string">'submodules'</span>))</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542763477/Snip20181121_2.png" alt="">  </p>
<p>因 <code>_modules</code> 和 <code>_parameters</code> 中的 item 未保存在 <strong>dict</strong> 中，所以默认的 <code>getattr</code> 方法无法获取它，因而 <code>nn.Module</code> 实现了自定义的 <code>__getattr__</code> 方法，如果默认的 <code>getattr</code> 无法处理，就调用自定义的 <code>__getattr__</code> 方法，尝试从 <code>_modules</code>、<code>_parameters</code> 和 <code>_buffers</code> 这三个字典中获取。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(getattr(module, <span class="string">'training'</span>))  <span class="comment"># 等价于 module.training</span></span><br><span class="line"><span class="comment"># error</span></span><br><span class="line"><span class="comment"># print(module.__getattr__('training'))</span></span><br><span class="line"></span><br><span class="line">module.attr1 = <span class="number">2</span></span><br><span class="line">print(getattr(module, <span class="string">'attr1'</span>))</span><br><span class="line"><span class="comment"># error</span></span><br><span class="line"><span class="comment"># print(module.__getattr__('attr1'))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 即 module.param，会调用 module.__getattr__('param')</span></span><br><span class="line">print(getattr(module, <span class="string">'param'</span>))</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542764618/Snip20181121_3.png" alt=""><br><a id="more"></a></p>
<p>2 . 在 PyTorch 中保存模型十分简单，所有的 Module 对象都具有 <code>state_dict()</code> 函数，返回当前 Module 所有的状态数据，将这些状态数据保存后，下次使用模型时即可利用 <code>model.load_state_dict()</code> 函数将状态加载进来。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">t.save(net.state_dict(), <span class="string">'net.pth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载已保存的模型</span></span><br><span class="line">net2 = Net()</span><br><span class="line">net2.load_state_dict(t.load(<span class="string">'net.pth))</span></span><br></pre></td></tr></table></figure>
<p>3 . 将 Module 放在 GPU 上运行只需两步：  </p>
<ul>
<li><code>model = model.cuda()</code>：将模型的所有参数都转存到 GPU 上；</li>
<li><code>input.cuda()</code>：将输入数据也放置到 GPU 上。</li>
</ul>
<p>如何在多个 GPU 上并行计算，PyTorch 也提供了两个函数，可实现简单高效的并行 GPU 计算：  </p>
<ul>
<li><code>nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None</code>)</li>
<li><code>class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)</code></li>
</ul>
<p>二者的参数十分相似，通过 <code>device_ids</code> 参数可以指定在哪些 GPU 上进行优化，<code>output_device</code> 指定输出到哪个 GPU 上，唯一的不同就在于前者直接利用多GPU并行计算得出结果，而后者返回一个新的 Module，能够自动在多GPU上进行并行计算。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># method1</span></span><br><span class="line">output = nn.parallel.data_parallel(net, input, device_ids=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># method2</span></span><br><span class="line">new_net = nn.DataParallel(net, device_ids=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">output = new_net(input)</span><br></pre></td></tr></table></figure>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/06/19/2019/06-14-naive-bayes/" class="prev">上一篇</a><a href="/2018/11/20/2018/11-20-pytorch15/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/21/2018/11-21-pytorch16/';
var disqus_title = 'PyTorch 学习笔记（十六）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/21/2018/11-21-pytorch16/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>