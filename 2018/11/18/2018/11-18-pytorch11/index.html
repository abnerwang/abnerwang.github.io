<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（十一） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（十一） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（十一）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 18, 2018</div><div class="post-content"><p>1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  </p>
<p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">print(a.data)  <span class="comment"># 还是同一个 tensor</span></span><br><span class="line"></span><br><span class="line">print(a.data.requires_grad)  <span class="comment"># 但是已经独立于计算图之外了</span></span><br><span class="line"></span><br><span class="line">d = a.data.sigmoid_()  <span class="comment"># sigmoid_ 是一个 inplace 操作，会修改 a 自身的值</span></span><br><span class="line">print(a)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">print(a.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错</span></span><br><span class="line">tensor = a.detach()</span><br><span class="line">print(tensor.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计 tensor 的一些指标，不希望被记录</span></span><br><span class="line">mean = tensor.mean()</span><br><span class="line">std = tensor.std()</span><br><span class="line">maximum = tensor.max()</span><br><span class="line">print(mean, std, maximum)</span><br><span class="line"></span><br><span class="line">tensor[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># 下面会报错： RuntimeError: one of the variables needed for gradient</span></span><br><span class="line"><span class="comment">#              computation has been modified by an inplace operation.</span></span><br><span class="line"><span class="comment"># 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确</span></span><br><span class="line"><span class="comment"># c.sum().backward()</span></span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>Output:  </p>
<p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxc4jy2emjj20wu0edmy2.jpg" alt=""></p>
<p>2 . 在反向传播过程中非叶子节点的导数计算完之后即被清空，若想查看这些变量的梯度，有两种方法：  </p>
<ul>
<li>使用 <code>autograd.grad</code> 函数；</li>
<li>使用 <code>hook</code>。</li>
</ul>
<p>推荐使用 <code>hook</code> 方法，但是在实际应用中应尽量避免修改 grad 的值。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种方法：使用 grad 获取中间变量的梯度</span></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line">z = y.sum()</span><br><span class="line"><span class="comment"># z 对 y 的梯度，隐式调用 backward()</span></span><br><span class="line">print(t.autograd.grad(z, y))  <span class="comment"># (tensor([1., 1., 1.]),)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种方法：使用 hook</span></span><br><span class="line"><span class="comment"># hook 是一个函数，输入是梯度，无返回值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_hook</span><span class="params">(grad)</span>:</span></span><br><span class="line">    print(<span class="string">'y 的梯度：'</span>, grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line"><span class="comment"># 注册 hook</span></span><br><span class="line">hook_handle = y.register_hook(variable_hook)</span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除非你每次都要用 hook，否则用完之后记得移除 hook</span></span><br><span class="line">hook_handle.remove()</span><br></pre></td></tr></table></figure>
<p>3 . 看看 variable 中的 grad 属性和 backward 函数 grad_variables 参数的含义。  </p>
<ul>
<li>variable x 的梯度是目标函数 $f(x)$ 对 x 的梯度，$\frac{df(x)}{dx}=(\frac{df(x)}{dx_{0}},\frac{df(x)}{dx_{1}},…,\frac{df(x)}{dx_N})$，形状和 x 一致；</li>
<li>对于 <code>y.backward(grad_variables)</code> 中的 <code>grad_variables</code> 相当于链式求导法则 $\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\cdot \frac{\partial{y}}{\partial{x}}$ 中的 $\frac{\partial{z}}{\partial{y}}$，z 是目标函数，一般是一个标量，故而 $\frac{\partial{z}}{\partial{y}}$ 的形状与 variable y 的形状一致，<code>z.backward()</code> 在一定程度上等价于 <code>y.backward(grad_y)</code>。<code>z.backward()</code> 省略了 <code>grad_variables</code> 参数，因为 z 是一个标量，而 $\frac{\partial{z}}{\partial{z}}=1$。  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()  <span class="comment"># 从 z 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">y_gradient = t.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># dz/dy</span></span><br><span class="line">y.backward(y_gradient)  <span class="comment"># 从 y 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure>
<p>另外需要注意，只有对 variable 的操作才能使用 autograd，如果对 variable 的 data 直接进行操作，将无法使用反向传播，除了对参数初始化，一般我们不会修改 variable.data 的值。  </p>
<p><strong>总结</strong>  </p>
<p>PyTorch 中计算图的特点可总结如下：  </p>
<ul>
<li><code>autograd</code> 根据用户对 variable 的操作构建计算图，对变量的操作抽象为 <code>Function</code>；</li>
<li>对于那些不是任何函数的输出，由用户创建的节点称为叶子节点，叶子节点的 <code>grad_fn</code> 为 None，叶子节点中需要求导的 variable，具有 <code>AccumulateGrad</code> 标识，因其梯度是累加的；</li>
<li>variable 默认是不需要求导的，即 <code>requires_grad</code> 属性默认为 False，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都为 <code>True</code>；</li>
<li>variable 的 <code>volatitle</code> 属性默认为 <code>False</code>，如果某一个 variable 的 <code>volatitle</code> 属性被设置为 <code>True</code>，那么所有依赖它的节点的 <code>volatitle</code> 属性都为 <code>True</code>，<code>volatitle</code> 为 <code>True</code> 的节点不会求导，<code>volatitle</code> 的优先级比 <code>requires_grad</code> 高；</li>
<li>多次反向传播时，梯度是累加的，反向传播的中间缓存会被清空，为进行多次反向传播需指定 <code>retian_graph=True</code> 来保存这些缓存；</li>
<li>非叶子节点的梯度计算完之后即被清空，可以使用 <code>autograd.grad</code> 或 <code>hook</code> 技术获取非叶子节点值；</li>
<li>variable 的 grad 与 data 形状一致，应避免直接修改 variable.data，因为对 data 的直接操作无法利用 autograd 进行反向传播；</li>
<li>反向传播函数 <code>backward</code> 的参数 <code>grad_variables</code> 可以看成链式求导的中间结果，如果是标量，可以省略，默认为 1；</li>
<li>PyTorch 采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。   </li>
</ul>
<p>4 . 目前绝大多数函数都可以使用 <code>autograd</code> 实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？那就需要自己写一个 <code>Function</code>，实现它的前向传播和反向传播代码。  </p>
<p>此外实现了自己的 <code>Function</code> 之后，还可以使用 <code>gradcheck</code> 函数来检测实现是否正确，<code>gradcheck</code> 通过数值逼近来计算梯度，可能具有一定的误差，通过控制 <code>eps</code> 的大小可以控制容忍的误差。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyAdd</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, w, x, b)</span>:</span></span><br><span class="line">        ctx.save_for_backward(w, x)</span><br><span class="line">        output = w * x + b</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        w, x = ctx.saved_tensors</span><br><span class="line">        grad_w = grad_output * x</span><br><span class="line">        grad_x = grad_output * w</span><br><span class="line">        grad_b = grad_output * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> grad_w, grad_x, grad_b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始前向传播</span></span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line"><span class="comment"># 开始反向传播</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># x 不需要求导，中间过程还是会计算它的导数，但随后被清空</span></span><br><span class="line">print(x.grad, w.grad, b.grad)  <span class="comment"># (None, tensor([1.]), tensor([1.]))</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, x, )</span>:</span></span><br><span class="line">        output = <span class="number">1</span> / (<span class="number">1</span> + t.exp(-x))</span><br><span class="line">        ctx.save_for_backward(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        output, = ctx.saved_tensors</span><br><span class="line">        grad_x = output * (<span class="number">1</span> - output) * grad_output</span><br><span class="line">        <span class="keyword">return</span> grad_x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用数值逼近方式检验计算梯度的公式对不对</span></span><br><span class="line">test_input = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">test_input.requires_grad_()</span><br><span class="line">t.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/19/2018/11-19-pytorch12/" class="prev">上一篇</a><a href="/2018/11/17/2018/11-17-pytorch10/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/18/2018/11-18-pytorch11/';
var disqus_title = 'PyTorch 学习笔记（十一）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/18/2018/11-18-pytorch11/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>