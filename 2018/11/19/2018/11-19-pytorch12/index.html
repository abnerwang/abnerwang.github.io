<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（十二） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（十二） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（十二）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 19, 2018</div><div class="post-content"><p>1 . <code>torch.nn</code> 是专门为深度学习而设计的模块，它的核心数据结构是 <code>Module</code>，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 <code>nn.Module</code>，撰写自己的网络/层。 下面自定义一个全连接层。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer = Linear(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">output = layer(input)</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> layer.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br></pre></td></tr></table></figure>
<a id="more"></a> 
<p>有几点需要注意：  </p>
<ul>
<li>自定义层必须继承 <code>nn.Module</code>，并且在其构造函数中需调用 <code>nn.Module</code> 的构造函数；</li>
<li>在构造函数 <code>__init__</code> 中必须自己定义可学习的参数，并封装成 <code>nn.Parameter</code>，<code>Parameter</code> 是一种特殊的 Tensor，但其默认需要求导（requires_grad=True);</li>
<li><code>forward</code> 函数实现前向传播过程，其输入可以是一个或多个 tensor；</li>
<li>无需写反向传播函数，<code>nn.Module</code> 能够利用 autograd 自动实现反向传播；</li>
<li><code>Module</code> 中的可学习参数可以通过 <code>named_parameters()</code> 或者 <code>parameters()</code> 返回迭代器，前者会给每个 parameter 都附上名字，使其更具有辨识度。</li>
</ul>
<p>2 . <code>Module</code> 能够自动检测到自己的 <code>Parameter</code>，并将其作为学习参数，除了 <code>Parameter</code> 之外，<code>Module</code> 还包含子 <code>Module</code>，主 <code>Module</code> 能够递归查找子 <code>Module</code> 中的 <code>Parameter</code>，下面实现一个多层感知机。  </p>
<p>Input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, hidden_features, out_features)</span>:</span></span><br><span class="line">        super(Perceptron, self).__init__()</span><br><span class="line">        self.layer1 = Linear(in_features, hidden_features)</span><br><span class="line">        self.layer2 = Linear(hidden_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = t.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> self.layer2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perceptron = Perceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> perceptron.named_parameters():</span><br><span class="line">    print(name, parameter.size())</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxdg714iv6j20s704c3yx.jpg" alt=""></p>
<p><code>Module</code> 中 parameter 的命名规范：</p>
<ul>
<li>对于类似 <code>self.param_name = nn.Parameter(t.randn(3, 4))</code>，命名为 <code>param_name</code>;</li>
<li>对于子 <code>Module</code> 中的 <code>parameter</code>，会其名字之前加上当前 <code>Module</code> 的名字。如对于 <code>self.sub_module = SubModel()</code>，<code>SubModel</code>中有个 <code>parameter</code> 的名字叫做 <code>param_name</code>，那么二者拼接而成的 <code>parameter name</code> 就是 <code>sub_module.param_name</code>。  </li>
</ul>
<p>3 . 为方便用户使用，PyTorch 实现了神经网络中绝大多数的 layer，这些 layer 都继承于 <code>nn.Module</code>，封装了可学习参数 <code>Parameter</code>，并实现了 <code>forward</code> 函数。这些自定义的 layer 对输入形状都有假设：输入的不是单个数据，而是一个 batch，输入只有一个数据，则必须调用 <code>tensor.unsqueeze(0)</code> 或 <code>tensor[None]</code> 将数据伪装成 batch_size=1 的 batch。  </p>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/18/2018/11-18-pytorch11/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/19/2018/11-19-pytorch12/';
var disqus_title = 'PyTorch 学习笔记（十二）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/19/2018/11-19-pytorch12/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2018 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>