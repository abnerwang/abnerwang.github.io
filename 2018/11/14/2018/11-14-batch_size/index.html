<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 理解 batch_size · Qiyexuxu</title><meta name="description" content="理解 batch_size - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">理解 batch_size</h1><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/batch-size/" class="tag-title">#batch_size</a></div><div class="post-info">Nov 14, 2018</div><div class="post-content"><p>梯度下降主要有以下三种：  </p>
<ul>
<li>batch gradient descent：即批梯度下降，其计算开销大，计算速度慢，不支持在线学习；</li>
<li>stochastic gradient descent：即随机梯度下降，收敛性能不太好，可能 hit 不到最优点；</li>
<li>mini-batch gradient descent：即小批量梯度下降，通常所说的 batch_size 指的就是这个批量的大小。</li>
</ul>
<a id="more"></a>
<p>batch_size = 1 时称为在线学习。</p>
<p>batch_size 的选择，首先决定的是下降方向，如果数据集较小，完全可以采用全数据集的形式。</p>
<p>增大 batch_size 的好处：</p>
<ul>
<li>内存的利用率提高了，大矩阵乘法的并行化效率提高了；</li>
<li>跑完一次 epoch（即全数据集）所需迭代次数减少，对于相同数据量的处理速度进一步加快；</li>
<li>在一定范围内，batch_size 越大，其确定的下降方向就越准确，引起训练震荡越小。</li>
</ul>
<p>盲目增大 batch_size 的坏处：</p>
<ul>
<li>当数据集太大时，内存撑不住；</li>
<li>跑完全数据集所需迭代次数减少了，但要达到相同的精度，时间开销大，参数的修正更加缓慢；</li>
<li>batch_size 增大到一定程度，其确定的下降方向已基本不再变化。</li>
</ul>
<p><strong>总结：</strong>  </p>
<p>1) batch_size 太小，而类别又比较多的时候，可能会导致 loss 函数震荡而不收敛，尤其是网络比较复杂的时候；</p>
<p>2) 随着 batch_size 的增大，处理相同数据量的速度越快；</p>
<p>3) 随着 batch_size 的增大，达到相同的精度所需的训练 epoch 数量越多；</p>
<p>4) 由于上述两种因素的矛盾，batch_size 增加到某个时候，达到时间上的最优；</p>
<p>5) 由于最终收敛精度会陷入不同的局部极值，因此，batch_size 增大到某些时候，达到最终收敛精度上的最优；</p>
<p>6) 过大的 batch_size 会使网络收敛到一些不好的局部最优点，同样太小的 batch_size 会使网络收敛太慢、不易收敛等；</p>
<p>7) 具体 batch_size 的选取和训练集的样本数目有关。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/14/2018/11-14-pytorch3/" class="prev">PREV</a><a href="/2018/11/14/2018/11-14-pytorch2/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/14/2018/11-14-batch_size/';
var disqus_title = '理解 batch_size';
var disqus_url = 'http://blog.keeplearning.group/2018/11/14/2018/11-14-batch_size/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>