<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（二） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（二） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（二）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 14, 2018</div><div class="post-content"><p>1 . 在 Tensor 上的所有操作，<code>autograd</code> 都能为它们自动提供微分，避免了手动计算导数的复杂过程，只需要设置 <code>tensor.requires_grad=True</code> 即可。</p>
<p>Input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上一步等价于</span></span><br><span class="line"><span class="comment"># x = t.ones(2, 2)</span></span><br><span class="line"><span class="comment"># x.requires_grad = True</span></span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">y = x.sum()</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line">print(y.grad_fn)</span><br><span class="line"></span><br><span class="line">y.backward()   <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fx7rvm1w6sj214e05ojrw.jpg" alt=""></p>
<p><code>grad</code> 在反向传播过程中是累加的，每一次运行反向传播，梯度都会累加之前的梯度，因此反向传播之前需要把梯度清零。</p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下划线结束的函数是 inplace 操作，会修改自身的值</span></span><br><span class="line">x.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7s1w5mz3j213y05imx9.jpg" alt=""><br><a id="more"></a><br>2 . <code>torch.nn</code> 是专门为神经网络设计的模块化接口，<code>nn</code> 构建于 Autograd 之上，可用来定义和运行网络，<code>nn.Module</code> 是 <code>nn</code> 中最重要的类，可以把它看成是一个网络的封装，包含网络定义以及 <code>forward</code> 方法，调用 <code>forward(input)</code> 方法，可返回前向传播的结果，下面是 LeNet 的实现。</p>
<p>Input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># nn.Module 子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="comment"># 下式等价于 nn.Module.__init__(self)</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 卷积层 1 表示输入图片为单通道，6 表示输出通道数，5 表示卷积核为 5*5</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 卷积层</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 仿射层/全连接层， y=Wx+b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 卷积 --&gt; 激活 --&gt; 池化</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># reshape, -1 表示自适应</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fx7swean9dj214e06jdgl.jpg" alt=""></p>
<p>只要在 <code>nn.Module</code> 的子类中定义了 <code>forward</code> 函数，<code>backward</code> 函数就会自动被实现（利用 <code>autograd</code>），网络的可学习参数通过 <code>net.parameters()</code> 返回，<code>net.named_parameters</code> 可同时返回可学习的参数和名称。  </p>
<p>Input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameters <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, <span class="string">':'</span>, parameters.size())</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7t8t7uitj21470a3gn3.jpg" alt=""></p>
<p><code>forward</code> 函数的输入和输出都是 Tensor。</p>
<p>Input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out.size())</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx7tfza8rlj214l01amx0.jpg" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()   <span class="comment"># 所有参数的梯度清零</span></span><br><span class="line">out.backward(t.ones(<span class="number">1</span>, <span class="number">10</span>))    <span class="comment"># 反向传播</span></span><br></pre></td></tr></table></figure>
<p><code>torch.nn</code> 只支持 mini-batches，不支持一次只输入一个样本，即一次必须是一个 batch，但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code> 将 batch_size 设为 1。</p>
<p>3 . <code>nn</code> 实现了神经网络中大多数的损失函数，例如 <code>nn.MSELoss</code> 用来计算均方误差，<code>nn.CrossEntropyLoss</code> 用来计算交叉熵损失。</p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = t.arange(<span class="number">0</span>, <span class="number">10</span>).view(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target.float())</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<p>Output:</p>
<p><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx7uw9frqoj2149014aa1.jpg" alt=""></p>
<p><code>torch.LongTensor</code>   —-&gt;  <code>torch.FloatTensor</code>: <code>tensor.float()</code>  </p>
<p> <code>torch.FloatTensor</code>  —-&gt; <code>torch.LongTensor</code>: <code>tensor.long()</code>  </p>
<p> Input:  </p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行 backward，观察调用之前和调用之后的 grad</span></span><br><span class="line">net.zero_grad()    <span class="comment"># 把 net 中所有可学习参数的梯度清零</span></span><br><span class="line">print(<span class="string">'反向传播之前 conv1.bias 的梯度: '</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line">loss.backward()</span><br><span class="line">print(<span class="string">'反向传播之后 conv1.bias 的梯度: '</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>
<p> Output:</p>
<p> <img src="http://wx2.sinaimg.cn/mw690/79225320ly1fx7v5aj09kj214c03vjrw.jpg" alt=""></p>
<p>4 . 在反向传播计算完所有参数的梯度之后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降（SGD）的更新策略手动实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.data * learning_rate)</span><br></pre></td></tr></table></figure>
<p><code>torch.optim</code> 中实现了深度学习中绝大多数的优化方法，例如 RMSProp、Adam、SGD 等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建一个优化器，指定要调整的参数和学习率</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练过程中</span></span><br><span class="line"><span class="comment"># 先梯度清零（与 net.zero_grad() 效果一样）</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target.float())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/14/2018/11-14-batch_size/" class="prev">上一篇</a><a href="/2018/11/14/2018/11-14-pytorch1/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/14/2018/11-14-pytorch2/';
var disqus_title = 'PyTorch 学习笔记（二）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch2/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2018 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>