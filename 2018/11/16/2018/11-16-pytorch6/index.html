<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（六） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（六） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（六）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 16, 2018</div><div class="post-content"><p>1 . Tensor 支持与 <code>numpy.ndarray</code> 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>])    <span class="comment"># 第 0 行，下标从 0 开始</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="number">0</span>])     <span class="comment"># 第 0 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>, <span class="number">2</span>])   <span class="comment"># 第 0 行第 2 个元素，等价于 a[0][2]</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>][<span class="number">-1</span>])   <span class="comment"># 第 0 行最后一个元素</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>])    <span class="comment"># 前两行</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>])   <span class="comment"># 前两行，第 0,1 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>:<span class="number">1</span>, :<span class="number">2</span>])  <span class="comment"># 第 0 行，前两列</span></span><br><span class="line">print(a[<span class="number">0</span>, :<span class="number">2</span>])   <span class="comment"># 注意两者的区别，形状不同</span></span><br></pre></td></tr></table></figure>
<a id="more"></a>  
<p>Output:  </p>
<p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx9neqhzwaj214f0ddjt6.jpg" alt="">  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># None 类似于 np.newaxis，为 a 新增了一个轴</span></span><br><span class="line"><span class="comment"># 等价于 a.view(1, a.shape[0], a.shape[1])</span></span><br><span class="line">print(a[<span class="keyword">None</span>].shape)  <span class="comment"># 等价于 a[None,:,:]</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :].shape)</span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :, <span class="keyword">None</span>, <span class="keyword">None</span>].shape)</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx3.sinaimg.cn/mw690/79225320ly1fx9nscpu79j214h03kq33.jpg" alt=""></p>
<p>2 . 常用的选择函数。  </p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>index_select(input, dim, index)</code></td>
<td style="text-align:center">在指定维度 dim 上选取，比如选取某些行、某些列</td>
</tr>
<tr>
<td style="text-align:center"><code>masked_select(input, mask)</code></td>
<td style="text-align:center">使用 ByteTensor 进行选取</td>
</tr>
<tr>
<td style="text-align:center"><code>non_zero(input)</code></td>
<td style="text-align:center">非 0 元素的下标</td>
</tr>
<tr>
<td style="text-align:center"><code>gather(input, dim, index)</code></td>
<td style="text-align:center">根据 index，在 dim 维度上选取数据，输出的 size 与 index 一样</td>
</tr>
</tbody>
</table>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a &gt; <span class="number">1</span>)  <span class="comment"># 返回一个 ByteTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于 a.masked_select(a&gt;1)</span></span><br><span class="line">print(a[a &gt; <span class="number">1</span>])  <span class="comment"># 选择结果与原 tensor 不共享内存空间</span></span><br><span class="line"></span><br><span class="line">print(a[t.LongTensor([<span class="number">0</span>, <span class="number">1</span>])])  <span class="comment"># 第 0 行和第 1 行</span></span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">Output:  </span><br><span class="line">  </span><br><span class="line">![](http://wx3.sinaimg.cn/mw690/<span class="number">79225320</span>gy1fx9osqbau0j214807lgmc.jpg)  </span><br><span class="line">  </span><br><span class="line">`gather` 是一个比较复杂的操作，对于一个 <span class="number">2</span> 维的 tensor，输出的每个元素如下：</span><br></pre></td></tr></table></figure>
<p>out[i][j] = input[index[i][j]][j]   # dim=0<br>out[i][j] = input[i][index[i][j]]   # dim=1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">三维 tensor 的 `gather` 操作同理。  </span><br><span class="line">  </span><br><span class="line">`gather(input, dim, index)` 中的 dim 表示的就是第几维度，在二维的例子中，如果 dim=0，那么它表示的就是你接下来的操作是对第一维度进行的，也就是行；如果 dim=1，那么它表示的就是你接下来的操作是对第二个维度进行的，也就是列。index 的大小和 input 的大小是一样的，它表示的是你所选择的维度上的操作。特别注意，index 必须是 LongTensor 类型。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line">import torch as t</span><br><span class="line"></span><br><span class="line">a = t.arange(0, 16).view(4, 4)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># 选取对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3]])</span><br><span class="line">print(a.gather(0, index))</span><br><span class="line"></span><br><span class="line"># 选取反对角线上的元素</span><br><span class="line">index = t.LongTensor([[3, 2, 1, 0]]).t()</span><br><span class="line">print(a.gather(1, index))</span><br><span class="line"></span><br><span class="line"># 选取两个对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()</span><br><span class="line">b = a.gather(1, index)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p>
<p>Output:  </p>
<p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9purwv3xj214j0d8gmb.jpg" alt="">  </p>
<p>与 <code>gather</code> 相对应的逆操作是 <code>scatter_</code>，<code>gather</code> 把数据从 input 中按照 index 取出，而 <code>scatter_</code> 是把取出的数据再放回去（inplace 操作）。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把两个对角线的元素放回到指定位置</span></span><br><span class="line">c = t.zeros(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">b = b.float()  <span class="comment"># 将 b 转换成 FloatTensor</span></span><br><span class="line">c.scatter_(<span class="number">1</span>, index, b)</span><br><span class="line">print(c)</span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">对 tensor 的任何索引操作仍是一个 tensor，想要获取标准的 tensor 对象数值，需要调用 `tensor.item()`，这个方法只对包含一个元素的 tensor 适用。</span><br><span class="line">  </span><br><span class="line"><span class="number">3</span> . PyTorch 目前已支持绝大多数 numpy 的高级索引，高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的 Tensor 共享内存。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">27</span>).view(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">0</span>]])  <span class="comment"># x[1, 1, 2] 和 x[2, 2, 0]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])  <span class="comment"># x[2, 0, 1]，x[1, 0, 1], x[0, 0, 1]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">0</span>, <span class="number">2</span>], ...])   <span class="comment"># x[0] 和 x[2]</span></span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9wkft7f2j21570kf0tu.jpg" alt=""></p>
<p>4 . Tensor 有不同的数据类型，每种类型分别对应有 CPU 和GPU 版本（HalfTensor 除外），默认的 tensor 都是 FloatTensor，可通过 <code>torch.set_default_tensor_type</code> 来修改默认 tensor 类型，如果默认类型为 GPU tensor，则所有操作都在 GPU 上进行，HalftTensor 是专门为 GPU 版本设计的，同样的元素个数，显存占用只有 FloatTensor 的一半，所以可以极大缓解 GPU 显存不足的问题，但由于其数值大小和精度有限，所以可能出现溢出等问题。  </p>
<p>各数据类型之间可以相互转换，<code>type(new_type)</code> 是通用的做法，同时还有 <code>float</code>、<code>long</code>、<code>half</code> 等快捷方法，CPU tensor 和 GPU tensor 之间的相互转换通过 <code>tensor.cuda</code> 和 <code>tensor.cpu</code> 方法来实现，此外还可以使用 <code>tensor.to(device)</code>。  </p>
<p>Tensor 还有一个 <code>new</code> 方法，用法与 <code>t.Tensor</code> 一样，会调用该 tensor 对应类型的构造函数，生成与当前 tensor 类型一致的 tensor，<code>torch.*_like(tensor)</code> 可以生成和 tensor 拥有同样属性（类型、形状、CPU?GPU）的新 tensor。<code>tensor.new_*(new_shape)</code> 新建一个不同形状的 tensor。  </p>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/16/2018/11-16-pytorch7/" class="prev">上一篇</a><a href="/2018/11/15/2018/11-15-pytorch5/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/16/2018/11-16-pytorch6/';
var disqus_title = 'PyTorch 学习笔记（六）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch6/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2018 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>