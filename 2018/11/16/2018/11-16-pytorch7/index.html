<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（七） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（七） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（七）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 16, 2018</div><div class="post-content"><p>1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  </p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">abs/sqrt/div/exp/fmod/log/pow…</td>
<td style="text-align:center">绝对值/平方根/除法/指数/求余/求幂…</td>
</tr>
<tr>
<td style="text-align:center">cos/sin/asin/atan2/cosh…</td>
<td style="text-align:center">相关三角函数</td>
</tr>
<tr>
<td style="text-align:center">ceil/round/floor/trunc</td>
<td style="text-align:center">上取整/四舍五入/下取整/只保留整数部分</td>
</tr>
<tr>
<td style="text-align:center">clamp(input, min, max)</td>
<td style="text-align:center">超过 min 和 max 部分截断</td>
</tr>
<tr>
<td style="text-align:center">sigmod/tanh..</td>
<td style="text-align:center">激活函数</td>
</tr>
</tbody>
</table>
<p>对于很多操作，例如 <code>div</code>、<code>mul</code>、<code>pow</code>、<code>fmod</code> 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 <code>a ** 2</code> 等价于 <code>torch.pow(a, 2)</code>，<code>a * 2</code> 等价于 <code>torch.mul(a, 2)</code>。<br><a id="more"></a><br>2 . 归并操作会使输出形状小于输入形状，并可以沿着某一维度进行执行操作，如加法 <code>sum</code>，既可以计算整个 tensor 的和，也可以计算 tensor 中每一行或每一列的和，常用的归并操作如下表。  </p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mean/sum/median/mode</td>
<td style="text-align:center">均值/和/中位数/众数</td>
</tr>
<tr>
<td style="text-align:center">norm/dist</td>
<td style="text-align:center">范数/距离</td>
</tr>
<tr>
<td style="text-align:center">std/var</td>
<td style="text-align:center">标准差/方差</td>
</tr>
<tr>
<td style="text-align:center">cumsum/cumprod</td>
<td style="text-align:center">累加/累乘</td>
</tr>
</tbody>
</table>
<p>以上大多数函数都有一个参数 <code>dim</code>，用来指定这些操作是在哪个维度上执行的。  </p>
<p>假设输入的形状是 (m, n, k)：</p>
<ul>
<li>如果指定 <code>dim=0</code>，输出形状就是 (1, n ,k) 或者 (n, k)</li>
<li>如果指定 <code>dim=1</code>，输出形状就是 (m, 1, k) 或者 (m, k)</li>
<li>如果指定 <code>dim=2</code>，输出形状就是 (m, n, 1) 或者 (m, n)</li>
</ul>
<p>size 中是否有 “1”，取决于参数 <code>keepdim</code>,<code>keepdim=True</code> 会保留维度 1，注意，以上只是经验总结，并非所有的函数都符合这种形状变化方式。</p>
<p>Input:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">   </span><br><span class="line">b = t.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">True</span>))  <span class="comment"># 保留维度 1</span></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">False</span>))  <span class="comment"># 不保留维度 1</span></span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.cumsum(dim=<span class="number">1</span>))  <span class="comment"># 沿着行累加</span></span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxa4du79t4j216408e3ze.jpg" alt=""></p>
<p>3 . 常用的比较函数表。  </p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">gt/lt/ge/le/eq/ne</td>
<td style="text-align:center">大于/小于/大于等于/小于等于/等于/不等</td>
</tr>
<tr>
<td style="text-align:center">topk</td>
<td style="text-align:center">最大的 k 个数</td>
</tr>
<tr>
<td style="text-align:center">sort</td>
<td style="text-align:center">排序</td>
</tr>
<tr>
<td style="text-align:center">max/min</td>
<td style="text-align:center">比较两个 tensor 最大最小值</td>
</tr>
</tbody>
</table>
<p>表中第一行的比较操作已经实现了运算符重载，因此可以使用 <code>a&gt;=b</code>、<code>a&gt;b</code>、<code>a!=b</code>、<code>a==b</code>，其返回结果是一个 ByteTensor，可用来选取元素。max/min 这两个操作比较特殊，以 <code>max</code> 为例说明如下：</p>
<ul>
<li><code>torch.max(tensor)</code>：返回 tensor 中最大的那个数；</li>
<li><code>torch.max(tensor, dim)</code>：指定维度上最大的数，同时返回 tensor 和下标；</li>
<li><code>torch.max(tensor1, tensor2)</code>：返回两个 tensor 相比较大的元素。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = t.linspace(<span class="number">15</span>, <span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">print(a &gt; b)</span><br><span class="line"></span><br><span class="line">print(a[a &gt; b])  <span class="comment"># a 中对应位置大于 b 的元素</span></span><br><span class="line"></span><br><span class="line">print(t.max(a))</span><br><span class="line"></span><br><span class="line">print(t.max(b, dim=<span class="number">1</span>))  <span class="comment"># 分别返回对应维度的最大值和最大值所在的下标</span></span><br><span class="line"></span><br><span class="line">print(t.max(a, b))</span><br><span class="line"></span><br><span class="line">print(t.clamp(a, min=<span class="number">10</span>))  <span class="comment"># 比较 a 和 10 较大的元素</span></span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa5kkgcmbj215y0emq4l.jpg" alt=""></p>
<p>4 . 常用的线性代数函数表如下。  </p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">trace</td>
<td style="text-align:center">矩阵的迹</td>
</tr>
<tr>
<td style="text-align:center">diag</td>
<td style="text-align:center">对角线元素</td>
</tr>
<tr>
<td style="text-align:center">triu/tril</td>
<td style="text-align:center">矩阵的上三角/下三角，可指定偏移量</td>
</tr>
<tr>
<td style="text-align:center">mm/bmm</td>
<td style="text-align:center">矩阵乘法，batch 的矩阵乘法</td>
</tr>
<tr>
<td style="text-align:center">addmm/addbmm/addmv/addr/badbmm…</td>
<td style="text-align:center">矩阵运算</td>
</tr>
<tr>
<td style="text-align:center">t</td>
<td style="text-align:center">转置</td>
</tr>
<tr>
<td style="text-align:center">dot/cross</td>
<td style="text-align:center">內积/外积</td>
</tr>
<tr>
<td style="text-align:center">inverse</td>
<td style="text-align:center">求逆矩阵</td>
</tr>
<tr>
<td style="text-align:center">svd</td>
<td style="text-align:center">奇异值分解</td>
</tr>
</tbody>
</table>
<p>需要注意的是，矩阵的转置会导致存储空间不连续，需要调用 <code>.contiguous</code> 方法将其转为连续。  </p>
<p>5 . Numpy 和 Tensor 可以相互转换，共享内存，但是当 Numpy 的数据类型和 Tensor 的数据类型不一样的时候，数据会被复制，不会共享内存。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(a.dtype)</span><br><span class="line"></span><br><span class="line">b = t.Tensor(a)   <span class="comment"># 此处进行拷贝，不共享内存</span></span><br><span class="line">print(b.dtype)</span><br><span class="line"></span><br><span class="line">c = t.from_numpy(a)  <span class="comment"># 注意 c 的类型（DoubleTensor）</span></span><br><span class="line">print(c.dtype)</span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)   <span class="comment"># b 与 a 不共享内存，a 改变但 b 不变</span></span><br><span class="line">print(c)   <span class="comment"># c 与 a 共享内存</span></span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxa6jr9iznj20ue07ywf2.jpg" alt="">  </p>
<p>6 . 广播法则是科学计算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。  </p>
<p>Numpy 的广播法则定义如下：  </p>
<ul>
<li>让所有输入数组都向其中 shape 最长的数组看齐，shape 中不足的部分通过在前面加 1 补齐；</li>
<li>两个数组在某一维度的长度要么一致，要么其中一个为 1，否则不能计算；</li>
<li>当输入数组的某个维度的长度为 1 时，计算时沿此维度复制扩充成一样的形状。</li>
</ul>
<p>PyTorch 已经支持了自动广播法则，但是我们还是通过以下两个函数手动实现一下广播法则以加深理解吧。  </p>
<ul>
<li><code>unsqueeze</code> 或者 <code>view</code>，或者 <code>tensor[None]</code>，为数据某一维的形状补 1，实现法则 1；</li>
<li><code>expand</code> 或者 <code>expand_as</code>，重复数组，实现法则 3，该操作不会复制数组，所以不会占用额外的空间。</li>
</ul>
<p>注意：<code>repeat</code> 实现与 <code>expand</code> 相类似的功能，但是 <code>repeat</code> 会把相同数据复制多份，因此会占用额外的空间。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = t.zeros(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动广播法则</span></span><br><span class="line"><span class="comment"># 第一步： a 是 2 维，b 是 3 维，所以先在较小的 a 前面补 1，</span></span><br><span class="line"><span class="comment"># 即： a.unsqueeze(0)，a 的形状变为 (1, 3, 2)，b 的形状是 (2, 3, 1)</span></span><br><span class="line"><span class="comment"># 第二步： a 和 b 在第一维和第三维形状不一致，其中一个为 1，</span></span><br><span class="line"><span class="comment"># 可以利用广播法则，两者都扩展成 (2, 3, 2)</span></span><br><span class="line"></span><br><span class="line">print(a + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动广播法则</span></span><br><span class="line"><span class="comment"># 或者 a.view(1, 3, 2).expand(2, 3, 2) + b.expand(2, 3, 2)</span></span><br><span class="line">print(a[<span class="keyword">None</span>].expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>) + b.expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa7an8bd0j20ug0fkgm5.jpg" alt=""></p>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/16/2018/11-16-momentum/" class="prev">PREV</a><a href="/2018/11/16/2018/11-16-pytorch6/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/16/2018/11-16-pytorch7/';
var disqus_title = 'PyTorch 学习笔记（七）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch7/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2018 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>