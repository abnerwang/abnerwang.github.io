<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（十） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（十） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（十）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 17, 2018</div><div class="post-content"><p>1 . <code>torch.autograd</code> 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在创建 tensor 的时候指定 requires_grad</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">b = t.zeros(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">c = a.add(b)   <span class="comment"># 也可以写成 c = a + b</span></span><br><span class="line"></span><br><span class="line">d = c.sum()</span><br><span class="line">d.backward()   <span class="comment"># 反向传播</span></span><br><span class="line">print(d.requires_grad)   <span class="comment"># d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，</span></span><br><span class="line"><span class="comment"># 因此 c 的 requires_grad 属性会自动设置为 True</span></span><br><span class="line">print(a.requires_grad, b.requires_grad, c.requires_grad)   <span class="comment"># (True, True, True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否为叶子节点</span></span><br><span class="line">print(a.is_leaf, b.is_leaf, c.is_leaf)   <span class="comment"># (True, True, False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，</span></span><br><span class="line"><span class="comment"># 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了</span></span><br><span class="line">print(c.grad <span class="keyword">is</span> <span class="keyword">None</span>)    <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  </p>
<p>$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  </p>
<a id="more"></a>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''手动求导函数'''</span></span><br><span class="line">    dx = <span class="number">2</span> * x * t.exp(x) + x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line">y.backward(t.ones(y.size()))</span><br><span class="line">print(x.grad)</span><br><span class="line">print(gradf(x))</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxbcx53kogj20yc078gmn.jpg" alt="">  </p>
<p>3 . 每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个 variable 的梯度，这些函数的函数名通常以 Backward 结尾。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x  <span class="comment"># 等价于 y = w.mul(x)</span></span><br><span class="line">z = y + b  <span class="comment"># 等价于 z = y.add(b)</span></span><br><span class="line"></span><br><span class="line">print(x.requires_grad, b.requires_grad, w.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_fn 可以查看这个 variable 的反向传播函数，</span></span><br><span class="line"><span class="comment"># z 是 add 函数的输出，所以它的反向传播函数是 AddBackward</span></span><br><span class="line">print(z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># next_functions 保存 grad_fn 的输入，是一个 tuple，tuple 的元素也是 Function</span></span><br><span class="line"><span class="comment"># 第一个是 y，它是乘法（mul）的输出，所以对应的反向传播函数 y.grad_fn 是 MulBackward</span></span><br><span class="line"><span class="comment"># 第二个是 b，它是叶子节点，由用户创建，grad_fn 为 None，但是需要求导，其梯度是累加的</span></span><br><span class="line">print(z.grad_fn.next_functions)</span><br><span class="line">print(z.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>] == y.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个是 w，叶子节点，需要求导，梯度是累加的</span></span><br><span class="line"><span class="comment"># 第二个是 x，叶子节点，不需要求导，所以为 None</span></span><br><span class="line">print(y.grad_fn.next_functions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 叶子节点的 grad_fn 是 None</span></span><br><span class="line">print(w.grad_fn, x.grad_fn)</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxbdwxg9bwj214406owfg.jpg" alt="">  </p>
<p>计算 $w$ 的梯度的时候，需要用到 $x$ 的数值（$\frac{\partial y}{\partial w}=x$），这些数值在前向过程中会保存成 buffer，在计算完梯度之后会自动清空，为了能够多次反向传播需要指定 <code>retain_graph</code> 来保留这些 buffer。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多次反向传播，梯度会累加，这也就是 w 中 AccumulateGrad 标识的含义</span></span><br><span class="line">z.backward()</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([2.])</span></span><br></pre></td></tr></table></figure>
<p>4 . PyTorch 使用的是动态图，它的计算图在每次前向传播时都是从头开始构建。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x.data[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line">x = <span class="number">-1</span> * t.ones(<span class="number">1</span>)  <span class="comment"># 写成 x = -1 * t.ones(1, requires_grad=True) 时，x 不计算梯度</span></span><br><span class="line">x = x.requires_grad_()</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([-1.])</span></span><br></pre></td></tr></table></figure>
<p>变量的 <code>requires_grad</code> 属性默认 <code>False</code>，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都是 <code>True</code>。  </p>
<p>5 . 有时候可能不希望 autograd 对 tensor 求导，因为求导需要缓存许多中间结构，增加额外的内存/显存开销，同时降低运行速度，那么我们可以关闭自动求导，譬如在模型训练完毕转而进行测试推断的时候。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> t.no_grad():  <span class="comment"># 也可以使用 t.set_grad_enable(False) 设置（无需 with），并且以下代码无缩进</span></span><br><span class="line">    x = t.ones(<span class="number">1</span>)</span><br><span class="line">    w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    y = x * w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y 虽然依赖于 w 和 x，虽然 w.requires_grad=True，但是 y.requires_grad=False</span></span><br><span class="line">    print(x.requires_grad, w.requires_grad, y.requires_grad)  <span class="comment"># (False, True, False)</span></span><br></pre></td></tr></table></figure>
<p>关闭自动求导后可以使用 <code>t.set_grad_enable(True)</code> 恢复设置。</p>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/18/2018/11-18-pytorch11/" class="prev">上一篇</a><a href="/2018/11/17/2018/11-17-pytorch9/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/17/2018/11-17-pytorch10/';
var disqus_title = 'PyTorch 学习笔记（十）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch10/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>