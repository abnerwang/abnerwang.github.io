<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（八） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（八） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（八）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 17, 2018</div><div class="post-content"><p>1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。<br><a id="more"></a><br>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.storage())</span><br><span class="line"></span><br><span class="line">b = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个对象的 id 值可以看作它在内存中的地址</span></span><br><span class="line"><span class="comment"># storage 的内存地址一样，即是同一个 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># a 改变，b 也随之改变，因为它们共享 storage</span></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">c = a[<span class="number">2</span>:]</span><br><span class="line">print(c.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># data_ptr 返回 tensor 首元素的内存地址</span></span><br><span class="line"><span class="comment"># 可以看出相差 16，这是因为 2*8=16</span></span><br><span class="line"><span class="comment"># 相差两个元素，每个元素占 8 个字节（long）</span></span><br><span class="line">print(c.data_ptr(), a.data_ptr())</span><br><span class="line"></span><br><span class="line">c[<span class="number">0</span>] = <span class="number">-100</span>  <span class="comment"># c[0] 的内存地址对应 a[2]</span></span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">d = t.Tensor(c.storage().float())</span><br><span class="line">print(id(c.storage()) == id(d.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面 3 个 tensor 共享 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()) == id(c.storage))</span><br><span class="line"></span><br><span class="line">print(a.storage_offset(), c.storage_offset())</span><br><span class="line"></span><br><span class="line">e = b[::<span class="number">2</span>, ::<span class="number">2</span>]  <span class="comment"># 隔 2 行/列取一个元素</span></span><br><span class="line">print(id(e.storage()) == id(a.storage()))</span><br><span class="line"></span><br><span class="line">print(b.stride(), e.stride())</span><br><span class="line"></span><br><span class="line">print(e.is_contiguous())</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fxauj7ug2xj20wu0u0acf.jpg" alt="">  </p>
<p>可见绝大多数操作并不修改 tensor 的数据，而只是修改了 tensor 的头信息。这种做法更节省内存，同时提升了处理速度。此外有些操作会导致 tensor 不连续，这时需要调用 <code>tensor.contiguous</code> 方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享 storage。  </p>
<p>2 . tensor 可以随意地在 GPU/CPU 上传输，使用 <code>tensor.cuda(device_id)</code> 或者 <code>tensor.cpu()</code>，另外一个更通用的方法是 <code>tensor.to(device)</code>。  </p>
<ul>
<li>尽量使用 <code>tensor.to(device)</code>，将 <code>device</code> 设为一个可配置的参数，这样可以很轻松地使程序同时兼容 GPU 和 CPU；</li>
<li>数据在 GPU 之中传输的速度要远快于内存（CPU）到显存（GPU），所以尽量避免在内存和显存之间传输数据。</li>
</ul>
<p>3 . tensor 的保存和加载十分简单，使用 <code>torch.save</code> 和 <code>torch.load</code> 即可完成相应的功能。在 save/load 时可以指定使用的 pickle 模块，在 load 时还可以将 GPU tensor 映射到 CPU 或者其他 GPU 上。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> t.cuda.is_available():</span><br><span class="line">    a = a.cuda(<span class="number">1</span>)   <span class="comment"># 把 a 转为 GPU1 上的 tensor</span></span><br><span class="line">    t.save(a, <span class="string">'a.pth'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载 b，存储于 GPU1 上（因为保存时 tensor 就在 GPU1 上）</span></span><br><span class="line">    b = t.load(<span class="string">'a.pth'</span>)</span><br><span class="line">    <span class="comment"># 加载为 c，存储于 CPU 上</span></span><br><span class="line">    c = t.load(<span class="string">'a.pth'</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">    <span class="comment"># 加载为 d，存储于 GPU0 上</span></span><br><span class="line">    d = t.load(<span class="string">'a.pth'</span>, map_location=&#123;<span class="string">'cuda:1'</span>:<span class="string">'cuda:0'</span>&#125;)</span><br></pre></td></tr></table></figure>
<p>4 . 关于 tensor 还有几点需要注意：  </p>
<ul>
<li>大多数 <code>torch.function</code> 都有一个参数 <code>out</code>，这时候产生的结果将保存在 <code>out</code> 指定的 tensor 之中；</li>
<li><code>torch.set_num_threads</code> 可以设置 PyTorch 进行 CPU 多线程并行计算时候所占用的线程数，这个可以用来限制 PyTorch 所占用的 CPU 数目；</li>
<li><code>torch.set_printoptions</code> 可以用来设置打印 tensor 时的数值精度和格式。</li>
</ul>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">t.set_printoptions(precision=<span class="number">10</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxay2fzkx2j20sq04uaan.jpg" alt=""></p>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/17/2018/11-17-pytorch9/" class="prev">上一篇</a><a href="/2018/11/16/2018/11-16-momentum/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/17/2018/11-17-pytorch8/';
var disqus_title = 'PyTorch 学习笔记（八）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch8/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2018 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>