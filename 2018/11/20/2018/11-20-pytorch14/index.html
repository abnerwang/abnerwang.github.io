<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（十四） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（十四） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（十四）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 20, 2018</div><div class="post-content"><p>1 . PyTorch 实现了如今最常用的三种循环神经网络（RNN）：RNN(vanilla RNN)、LSTM 和 GRU，此外还有对应的三种 RNNCell，RNN 和 RNNCell 层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上 RNN 层的一种后端实现方式就是调用 RNNCell 来实现的。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 输入：batch_size=3，序列长度都为 2，序列中每个元素占 4 维</span></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># lstm 输入向量 4 维，隐藏元 3，1 层</span></span><br><span class="line">lstm = nn.LSTM(<span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 初始状态：1 层，batch_size=3，3 个隐藏元</span></span><br><span class="line">h0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">c0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out, hn = lstm(input, (h0, c0))</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 一个 LSTMCell 对应的层数只能是一层</span></span><br><span class="line">lstm = nn.LSTMCell(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">hx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">cx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out = []</span><br><span class="line"><span class="keyword">for</span> i_ <span class="keyword">in</span> input:</span><br><span class="line">    hx, cx = lstm(i_, (hx, cx))</span><br><span class="line">    out.append(hx)</span><br><span class="line">t.stack(out)</span><br></pre></td></tr></table></figure>
<p>词向量在自然语言中应用十分普及，PyTorch 同样提供了 Embedding 层。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有 4 个词，每个词用 5 维的向量表示</span></span><br><span class="line">embedding = nn.Embedding(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 可以用训练好的词向量初始化 embedding</span></span><br><span class="line">embedding.weight.data = t.arange(<span class="number">0</span>, <span class="number">20</span>).view(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = t.arange(<span class="number">3</span>, <span class="number">0</span>, <span class="number">-1</span>).long()</span><br><span class="line">output = embedding(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<a id="more"></a>  
<p>2 . 在深度学习中要用到各种各样的损失函数（loss function），这些损失函数可以看成是一种特殊的 layer，PyTorch 也将这些损失函数实现为 <code>nn.Module</code> 的子类，然而在实际应用中通常将这些 loss function 专门提取出来，和主模型互相独立。  </p>
<p>交叉熵损失（CrossEntropyLoss)：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch_size=3，计算对应每个类别的分数（只有两个类别）</span></span><br><span class="line">score = t.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 三个样本分别属于 1， 0， 1 类，label 必须是 LongTensor</span></span><br><span class="line">label = t.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]).long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss 与普通的 layer 无差异</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(score, label)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure>
<p>3 . PyTorch 将深度学习中常用的优化方法全部封装在 <code>torch.optim</code> 中，能够方便地扩展成自定义的优化方法，所有的优化方法都是继承基类 <code>optim.Optimizer</code>，并实现了自己的优化步骤，下面以随机梯度下降（SGD）说明：  </p>
<ul>
<li>优化方法的基本使用方法；</li>
<li>如何对模型的不同部分设置不同的学习率；</li>
<li>如何调整学习率。  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义一个 LeNet 网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(params=net.parameters(), lr=<span class="number">1</span>)</span><br><span class="line">optimizer.zero_grad()  <span class="comment"># 梯度清零，等价于 net.zero_grad()</span></span><br><span class="line"></span><br><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = net(input)</span><br><span class="line">output.backward(output)  <span class="comment"># fake backward</span></span><br><span class="line"></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<p>为不同的子网络设置不同的学习率，在 finetune 中经常用到，如果对某个参数不指定学习率，就使用最外层的默认学习率。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">1e-2</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure>
<p>只为两个全连接层设置较大的学习率，其余层的学习率较小。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">special_layers = nn.ModuleList([net.classifier[<span class="number">0</span>], net.classifier[<span class="number">3</span>]])</span><br><span class="line">special_layers_params = list(map(id, special_layers.parameters()))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> special_layers_params, net.parameters())</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: base_params&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: special_layers.parameters(), <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">], lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure>
<p><code>id</code>：用于获取对象的内存地址。<br><code>map(function, iterable,...)</code>: 第一个参数 <code>function</code> 以参数序列中的每一个元素调用 <code>function</code> 函数，返回包含每次 <code>function</code> 函数返回值的新列表。<br><code>filter(function, iterable)</code>：用于过滤掉不符合条件的元素，返回由符合条件的元素组成的新列表。<code>iterable</code> 是可迭代对象。  </p>
<p>对于如何调整学习率，主要有两种做法，一种是修改 <code>optimizer.param_groups</code> 中对应的学习率，另一种是更简单也是较为推荐的做法 —— 新建优化器，但是后者对于使用动量的优化器（如 Adam），会丢失动量状态信息，可能会造成损失函数的收敛出现震荡等情况。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：调整学习率，手动 decay，保存动量</span></span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">'lr'</span>] *= <span class="number">0.1</span>   <span class="comment"># 学习率为之前的 0.1 倍</span></span><br><span class="line">print(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：调整学习率，新建一个 optimizer</span></span><br><span class="line">old_lr = <span class="number">0.1</span></span><br><span class="line">optimizer1 = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>: old_lr * <span class="number">0.1</span>&#125;</span><br><span class="line">    ], lr=<span class="number">1e-5</span>)</span><br><span class="line">print(optimizer1)</span><br></pre></td></tr></table></figure>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/20/2018/11-20-pytorch15/" class="prev">PREV</a><a href="/2018/11/20/2018/11-20-pytorch13/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/20/2018/11-20-pytorch14/';
var disqus_title = 'PyTorch 学习笔记（十四）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch14/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2018 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>