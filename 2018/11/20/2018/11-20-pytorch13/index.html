<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（十三） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（十三） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（十三）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 20, 2018</div><div class="post-content"><p>1 . 图像的卷积操作。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line"></span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">to_pil = ToPILImage()</span><br><span class="line">lena = Image.open(<span class="string">'path to your image'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入是一个 batch，batch_size=1</span></span><br><span class="line">input = to_tensor(lena).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锐化卷积核</span></span><br><span class="line">kernel = t.ones(<span class="number">3</span>, <span class="number">3</span>)/<span class="number">-9</span></span><br><span class="line">kernel[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = conv(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<a id="more"></a>  
<p>2 . 池化层可以看作是一种特殊的卷积层，用来下采样，但池化层没有可学习的参数，其 weight 是固定的。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool = nn.AvgPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(list(pool.parameters()))  <span class="comment"># []</span></span><br><span class="line"></span><br><span class="line">out = pool(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>
<p>3 . 除了卷积层和池化层，深度学习中还将常用到以下几层：  </p>
<ul>
<li>Linear：全连接层；</li>
<li>BatchNorm：批规范化层，分为 1D、2D 和 3D。除了标准的 BatchNorm 之外，还有在风格迁移中常用到的 InstanceNorm 层；</li>
<li>Dropout：dropout 层用来防止过拟合，同样分为 1D、2D 和 3D。  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入 batch_size = 2，维度３</span></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">h = linear(input)</span><br><span class="line">print(h)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 channel，初始化标准差为 4，均值为 0</span></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">4</span>)</span><br><span class="line">bn.weight.data = t.ones(<span class="number">4</span>) * <span class="number">4</span></span><br><span class="line">bn.bias.data = t.zeros(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">bn_out = bn(h)</span><br><span class="line"><span class="comment"># 注意输出的均值和方差</span></span><br><span class="line"><span class="comment"># 方差是标准差的平方，计算无偏方差分母会减 1</span></span><br><span class="line"><span class="comment"># 使用 unbiased=False，分母不减 1</span></span><br><span class="line">print(bn_out.mean(<span class="number">0</span>), bn_out.var(<span class="number">0</span>, unbiased=<span class="keyword">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个元素以 0.5 的概率舍弃</span></span><br><span class="line">dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">o = dropout(bn_out)</span><br><span class="line">print(o)  <span class="comment"># 有一半的数变为０</span></span><br></pre></td></tr></table></figure>
<p>4 . PyTorch 实现了常见的激活函数，这些激活函数可作为独立的 layer 使用，这里介绍一下常用的激活函数 ReLU，其数学表达式为 $ReLU(x)=max(0, x)$。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(input)</span><br><span class="line">output = relu(input)  <span class="comment"># 等价于 input.clamp(min=0)</span></span><br><span class="line">print(output)   <span class="comment"># 小于 0  的被截断为 0</span></span><br></pre></td></tr></table></figure>
<p>ReLU 函数有个 inplace 参数，如果设置为 True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算 ReLU 的反向传播时，只需根据输出就能够推算反向传播的梯度。但是只有少数的 autograd 操作支持 inplace 操作（如 <code>tensor.sigmoid_()</code>)，除非你明确地知道自己在做什么，否则一般不要使用 inplace 操作。</p>
<p>5 . 在以上例子中，基本上都是将每一层的输出直接作为下一层的输入，这种网络称为前馈传播网络，对于此类网络如果每次都写复杂的 forward 函数会有些麻烦，有两种简化方式，<code>ModuleList</code> 和 <code>Sequential</code>，其中 <code>Sequential</code> 是一个特殊的 Module，它包含几个子 Module，前向传播时会将输入一层接一层地传递下去，<code>ModuleList</code> 也是一个极其特殊的 Module，可以包含几个子 Module，可以像 list 一样使用它，但不能直接把输入传递给 ModuleList。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequential 的三种写法</span></span><br><span class="line">net1 = nn.Sequential()</span><br><span class="line">net1.add_module(<span class="string">'conv'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">'batchnorm'</span>, nn.BatchNorm2d(<span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">'activation_layer'</span>, nn.ReLU())</span><br><span class="line"></span><br><span class="line">net2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">3</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">net3 = nn.Sequential(OrderedDict[</span><br><span class="line">                         (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">                         (<span class="string">'bn1'</span>, nn.BatchNorm2d(<span class="number">3</span>)),</span><br><span class="line">                         (<span class="string">'relu1'</span>, nn.ReLU())</span><br><span class="line">                     ])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'net1:'</span>, net1)</span><br><span class="line">print(<span class="string">'net2:'</span>, net2)</span><br><span class="line">print(<span class="string">'net3:'</span>, net3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可根据名字或序号取出子 Module</span></span><br><span class="line">print(net1.conv, net2[<span class="number">0</span>], net3.conv1)</span><br><span class="line"></span><br><span class="line">input = t.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">output = net1(input)</span><br><span class="line">output = net2(input)</span><br><span class="line">output = net3(input)</span><br><span class="line">output = net3.relu1(net1.batchnorm(net1.conv(input)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">modellist = nn.ModuleList([nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU(), nn.Linear(<span class="number">4</span>, <span class="number">2</span>)])</span><br><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> modellist:</span><br><span class="line">    input = model(input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会报错，因为 modellist 没有实现 forward 方法</span></span><br><span class="line"><span class="comment"># output = modellist(input)</span></span><br></pre></td></tr></table></figure>
<p><code>ModuleList</code> 是 <code>Module</code> 的子类，当在 <code>Module</code> 中使用它的时候，就能自动识别为子 module，而 python 自带的 list 则不行。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line">        self.list = [nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU()]</span><br><span class="line">        self.module_list = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), nn.ReLU()])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModule()</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(name, param)</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxec1gj8swj20u00xt0wx.jpg" alt="">  </p>
<p>可见，list 中的子 Module 并不能被主 Module 所识别，而 ModuleList 中的子 Module  能够被主 Module 所识别，这意味着如果用 list 保存子 Module，将无法调整其参数，因其未加入到主 Module 的参数中。  </p>
<p>在实际应用中，如果在构造函数 <code>__init__</code> 中用到 list、tuple、dict 等对象时，一定要思考是否应该用 ModuleList 或 ParameterList 代替。</p>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/20/2018/11-20-pytorch14/" class="prev">上一篇</a><a href="/2018/11/19/2018/11-19-pytorch12/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/20/2018/11-20-pytorch13/';
var disqus_title = 'PyTorch 学习笔记（十三）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch13/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>