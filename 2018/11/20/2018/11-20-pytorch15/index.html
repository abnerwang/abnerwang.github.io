<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> PyTorch 学习笔记（十五） · Qiyexuxu</title><meta name="description" content="PyTorch 学习笔记（十五） - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">PyTorch 学习笔记（十五）</h1><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 20, 2018</div><div class="post-content"><p>1 . <code>nn</code> 中的大多数 layer，在 <code>nn.functional</code> 中都有一个与之相对应的函数，<code>nn.functional</code> 中的函数和 <code>nn.Module</code> 的主要区别在于，用 <code>nn.Module</code> 实现的 layers 是一个特殊的类，都是由 <code>class layer(nn.Module)</code> 定义，会自动提取可学习的参数，而 <code>nn.functional</code> 中的函数更像是纯函数，由 <code>def function(input)</code> 定义。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">output1 = model(input)</span><br><span class="line">output2 = nn.functional.linear(input, model.weight, model.bias)</span><br><span class="line">print(output1 == output2)</span><br><span class="line"></span><br><span class="line">b = nn.functional.relu(input)</span><br><span class="line">b2 = nn.ReLU()(input)</span><br><span class="line">print(b == b2)</span><br></pre></td></tr></table></figure>
<p>Output:<br><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxeq3vk9axj20wy05a74n.jpg" alt="">  </p>
<p>如果模型有可学习的参数，最好用 <code>nn.Module</code>，否则既可以用 <code>nn.Module</code> 也可以使用 <code>nn.functional</code>，二者在性能上没有太大差异。但 dropout 操作虽然没有可学习的参数，但还是建议使用 <code>nn.Dropout</code> 而不是 <code>nn.functional.dropout</code>，因为 dropout 在训练和测试两个阶段的行为有所差异，使用 <code>nn.Module</code> 对象能够通过 <code>model.eval</code> 操作加以区分。  </p>
<p>在模型中搭配使用 <code>nn.Module</code> 和 <code>nn.functional</code>：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.pool(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.pool(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数 <code>__init__</code> 中。<br><a id="more"></a><br>2 . 在深度学习中参数的初始化非常重要，良好的初始化能让模型更快收敛，并达到更高水平，而糟糕的初始化则可能使模型迅速瘫痪。<code>nn.Module</code> 模块的参数都采取了较为合理的初始化策略，因此一般不需要我们考虑。而当我们使用 Parameter 时，自定义初始化尤其重要，PyTorch 中 <code>nn.init</code> 模块就是专门为初始化而设计的，如果某种初始化策略 <code>nn.init</code> 不提供，用户也可以自己直接初始化。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用 nn.init 初始化</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line">init.xavier_normal_(linear.weight)  <span class="comment"># 等价于 linear.weight.data.normal_(0, std)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接初始化</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xavier 初始化的计算公式</span></span><br><span class="line">std = math.sqrt(<span class="number">2</span>)/math.sqrt(<span class="number">7.</span>)</span><br><span class="line">linear.weight.data.normal_(<span class="number">0</span>, std)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对模型的所有参数进行初始化</span></span><br><span class="line"><span class="keyword">for</span> name, params <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> name.find(<span class="string">'linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="comment"># init linear</span></span><br><span class="line">        params[<span class="number">0</span>] <span class="comment"># weight</span></span><br><span class="line">        params[<span class="number">1</span>] <span class="comment"># bias</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'norm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>3 . <code>nn.Module</code> 基类的构造函数：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self._parameters = OrderedDict()</span><br><span class="line">    self._modules = OrderedDict()</span><br><span class="line">    self._buffers = OrderedDict()</span><br><span class="line">    self._backward_hooks = OrderedDict()</span><br><span class="line">    self._forward_hooks = OrderedDict()</span><br><span class="line">    self.training = <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>_parameters</code>：保存用户直接设置的 <code>Parameter</code>;</li>
<li><code>_modules</code>：指定的子 module 会保存于此；</li>
<li><code>_buffers</code>：缓存，如 batchnorm 使用 momentum 机制，每次前向传播需用到上一次前向传播的结果；</li>
<li><code>_backward_hooks</code> 与 <code>_forward_hooks</code>：钩子技术，用来提取中间变量，类似 variable 的 hook；</li>
<li><code>training</code>：BatchNorm 与 Dropout 层在训练阶段和测试阶段会分别采取不同的策略，通过判断 training 的值来决定前向传播策略。  </li>
</ul>
<p>上述几个属性中，<code>_parameters</code>、<code>_modules</code> 和 <code>_buffers</code> 这三个字典中的值，都可以通过 <code>self.key</code> 方式获得，效果等价于 <code>self._parameters[key]</code>。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 等价于 self.register_parameter('param1', nn.Patameter(t.rand(3, 3)))</span></span><br><span class="line">        self.param1 = nn.Parameter(t.rand(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.submodel1 = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        x = self.param1.mm(input)</span><br><span class="line">        x = self.submodel1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(<span class="string">'-------- Net 结构 --------'</span>)</span><br><span class="line">print(net)</span><br><span class="line">print(<span class="string">'-------- Net 结构 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的子 module --------'</span>)</span><br><span class="line">print(net._modules)</span><br><span class="line">print(<span class="string">'-------- Net 中的子 module --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的自定义参数 --------'</span>)</span><br><span class="line">print(net._parameters)</span><br><span class="line">print(<span class="string">'-------- Net 中的自定义参数 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中 para1 的参数 --------'</span>)</span><br><span class="line">print(net.param1)  <span class="comment"># 等价于 net._parameters['param1']</span></span><br><span class="line">print(<span class="string">'-------- Net 中 para1 的参数 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的参数大小 --------'</span>)</span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, parameter.size())</span><br><span class="line">print(<span class="string">'-------- Net 中的参数大小 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的子 module 及其名称 --------'</span>)</span><br><span class="line"><span class="keyword">for</span> name, submodel <span class="keyword">in</span> net.named_modules():</span><br><span class="line">    print(name, submodel)</span><br><span class="line">print(<span class="string">'-------- Net 中的子 module 及其名称 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">input = t.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">output = bn(input)</span><br><span class="line">print(bn._buffers)</span><br></pre></td></tr></table></figure>
<p><code>nn.Module</code> 在实际使用中可能层层嵌套，一个 module 包含若干个子 module，每一个子 module 又包含更多的子 module，<code>children</code> 方法可以查看直接子 module，<code>module</code> 函数可以查看所有的子 module（包含当前 module）。与之相对应的还有函数 <code>named_children</code> 和 <code>named_modules</code>，其能够在返回 module 列表的同时返回它们的名字。  </p>
<p>dropout 在训练和测试阶段采取不同策略举例：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input = t.arange(<span class="number">0</span>, <span class="number">12</span>).view(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">model = nn.Dropout()</span><br><span class="line"><span class="comment"># 在训练阶段，会有一半左右的数被随机置为 0</span></span><br><span class="line">print(model(input))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试阶段，dropout 什么都不做</span></span><br><span class="line">model.training = <span class="keyword">False</span></span><br><span class="line">model(input)</span><br></pre></td></tr></table></figure>
<p>如果一个模型具有多个 dropout 层，不需要为每个 dropout 层指定 training 属性，更为推荐的做法是调用 <code>model.train()</code> 函数，它会将当前 module 及其子 module 中的所有 training 属性都设置为 True，相应的，<code>model.eval()</code> 函数会把 <code>training</code> 属性都设为 False。  </p>
<p><code>register_forward_hook</code> 与 <code>register_backward_hook</code>，这两个函数的功能类似于 <code>variable</code> 函数的 <code>register_hook</code>，可在 module 前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：<code>hook(module, input, output) -&gt; None</code>，而反向传播则具有如下形式：<code>hook(module, grad_input, grad_output) -&gt; Tensor or None</code>。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在 forward 函数中，但如果在 forward 函数中专门加上这些处理，可能会使处理逻辑比较复杂，这时候使用钩子技术就更合适一些。  </p>
<p>下面考虑一种场景，有一个预训练好的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，但又不希望修改其原有的模型定义文件，这时就可以利用钩子函数。下面给出实现的伪代码。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = VGG()</span><br><span class="line">features = t.Tensor()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span><span class="params">(module, input, output)</span>:</span></span><br><span class="line">    <span class="string">'''把这层的输出拷贝到 features 中'''</span></span><br><span class="line">    features.copy_(output.data)</span><br><span class="line">    </span><br><span class="line">handle = model.layer8.register_forward_hook(hook)</span><br><span class="line">_ = model(input)</span><br><span class="line"><span class="comment"># 用完 hook 后删除</span></span><br><span class="line">handle.remove()</span><br></pre></td></tr></table></figure>
<hr>
<p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>
</div></article></div></main><footer><div class="paginator"><a href="/2018/11/21/2018/11-21-pytorch16/" class="prev">PREV</a><a href="/2018/11/20/2018/11-20-pytorch14/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2018/11/20/2018/11-20-pytorch15/';
var disqus_title = 'PyTorch 学习笔记（十五）';
var disqus_url = 'http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch15/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>