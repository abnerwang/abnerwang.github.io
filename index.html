<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Qiyexuxu</title><meta name="description" content="王小平的个人博客"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link active">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2019/08/19/2019/08-19-logistic_regression/" class="post-title-link">logistic 回归模型</a></h2><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/logistic-回归模型/" class="tag-title">#logistic 回归模型</a></div><div class="post-info">Aug 19, 2019</div><div class="post-content"><p>logistic 回归模型用于分类问题，它是概率模型，同时属于对数线性模型。这是什么意思呢？概率模型，说明最后学到的模型形式是 $P(Y|X=x)$，其中 $X$ 是特征，$Y$ 是类别。对数线性模型，说明与学得的模型 $P(Y|X=x)$ 有关的某种对数形式与输入特征 $x$ 存在着某种线性关系。针对 logistic 回归模型，这种线性关系表现在 $P(Y|X=x)$ 的某种对数形式与函数 $w \cdot x + b$（$w$ 为权重参数，$b$ 为偏置参数）相等。事实上，基于这两点，我们可以推导出 logistic 回归模型的数学形式。</p>
<p>首先，我们从最简单的用于二分类的二项 logistic 回归模型说起，之后推广到用于多分类的多项 logistic 回归模型。</p>
<h3 id="二项-logistic-回归模型"><a href="#二项-logistic-回归模型" class="headerlink" title="二项 logistic 回归模型"></a>二项 logistic 回归模型</h3><p>对于二分类问题，设分类标签 $Y \in \{1, 0\}$。我们首先观察一下线性函数 $w \cdot x + b$，$w$ 和 $b$ 是需要学习的参数，对它们的取值范围不加任何限制，也就是说，其取值范围为 $(-\infty, +\infty)$，而 $P(Y|X=x) \in [0, 1]$，将一个 $[0, 1]$ 范围内的数映射到 $(-\infty, +\infty)$ 范围上，需要经过某种变换，在这里，我们使用对数几率变换，即 logit 变换。</p></div><a href="/2019/08/19/2019/08-19-logistic_regression/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2019/08/19/2019/08-19-lagrange_duality/" class="post-title-link">拉格朗日对偶</a></h2><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/拉格朗日对偶/" class="tag-title">#拉格朗日对偶</a></div><div class="post-info">Aug 19, 2019</div><div class="post-content"><p>拉格朗日对偶常用于带约束条件的最优化问题上，为了系统讲述这种方法的思想，我们首先引入一个带约束条件的最优化问题：</p>
<p>$$\quad min_xf(x) \\ s.t.\quad c_i(x) \leq 0 \quad (i=1, 2, …, k) \\ \quad h_j(x)=0 \quad (j=1, 2, …, l)$$</p>
<p>其中，$f(x)、c_i(x)、h_j(x)$ 分别是定义在 $x \in \mathbb{R}^n$ 上的连续可微函数。对于此类带约束条件的优化问题，一般我们考虑使用拉格朗日乘子法解决，首先引入拉格朗日乘子 $\alpha_i(\alpha_i \geq 0), \beta_j$，构造广义拉格朗日函数如下：  </p>
<p>$$L(x, \alpha, \beta)=f(x)+\Sigma_{i=1}^{k} \alpha_i \cdot c_i(x)+\Sigma_{j=1}^l \beta_j \cdot h_j(x)$$</p></div><a href="/2019/08/19/2019/08-19-lagrange_duality/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2019/08/16/2019/08-16-newton-method/" class="post-title-link">牛顿法与拟牛顿法</a></h2><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/牛顿法与拟牛顿法/" class="tag-title">#牛顿法与拟牛顿法</a></div><div class="post-info">Aug 16, 2019</div><div class="post-content"><p>本文首先从一个不带约束条件的最优化问题出发，导出用以解决此类问题的牛顿法，接着针对牛顿法涉及的计算复杂问题，阐述在此基础上改进得到的拟牛顿法的思路。  </p>
<h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>我们首先引入如下最优化问题：  </p>
<p>$$min_{x\in{\mathbb{R}^n}}f(x)$$</p>
<p>若 $f(x)$ 在其定义域内具有二阶连续偏导数，在其定义域内任取一点 $x^{(k)}$，将 $f(x)$ 在点 $x^{(k)}$ 处进行二阶泰勒展开，得：  </p>
<p>$$f(x)=f(x^{(k)})+g_k^T(x-x^{(k)})+\frac{1}{2}(x-x^{(k)})^TH_k(x-x^{(k)})$$</p>
<p>其中，${g_k}=\nabla_{x^{(k)}}f(x^{(k)})$，$H_k=(\frac{\partial^2 f(x^{(k)})}{\partial x_i \partial x_j})_{n \times n}$，该矩阵 $H_k$ 称为 Hessian 矩阵，当 $x \to x^{(k)}$ 时上式成立（即在 $x^{(k)}$ 的小邻域内成立）。由以上引入的最优化问题，我们需要得到 $x^{(k)}$ 小邻域内的极小值，而 $f(x)$ 在 $x^{(k)}$ 小邻域内取得极值的必要条件是 $\nabla_{x}f(x)=0$，在上式中令 $\nabla_{x}f(x)=0$，得：</p></div><a href="/2019/08/16/2019/08-16-newton-method/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2019/06/19/2019/06-14-naive-bayes/" class="post-title-link">朴素贝叶斯算法详解</a></h2><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/朴素贝叶斯算法/" class="tag-title">#朴素贝叶斯算法</a></div><div class="post-info">Jun 19, 2019</div><div class="post-content"><p>朴素贝叶斯算法主要解决什么问题？它是怎么解决这类问题的？如何证明这种解决方法是有效的？本节将从这三方面详细介绍朴素贝叶斯算法的相关内容。</p>
<h3 id="朴素贝叶斯算法主要解决什么问题"><a href="#朴素贝叶斯算法主要解决什么问题" class="headerlink" title="朴素贝叶斯算法主要解决什么问题"></a>朴素贝叶斯算法主要解决什么问题</h3><p>朴素贝叶斯算法主要用来处理分类问题，也就是说，我们给算法某个对象的输入特征（通常是向量形式），算法能够根据这个输入特征判断含有这种特征的对象属于预先定义的哪一类。我们所熟悉的垃圾邮件分类问题是朴素贝叶斯算法的典型应用场景。为方便下文论述，我们首先对使用朴素贝叶斯算法解决分类问题进行数学建模，设分类问题的输入空间 $\chi \subseteq \mathbb{R}^n$，输出空间 $\mathbb{y} = ${$c_1, c_2, …, c_K$}$ $，即输入空间定义为 $n$ 维实数向量的子空间，输出空间包含了 $K$ 个预先定义的类别。某一特定问题的输入 $x \in \chi$，输出 $y \in \mathbb{y}$，即朴素贝叶斯算法建立了一个从某个对象提取的特征向量（这里的特征向量和线性代数里面的特征向量是不同的概念，这里指的是能反映该对象特征的向量）到该对象所属类别的一个映射。 </p></div><a href="/2019/06/19/2019/06-14-naive-bayes/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/21/2018/11-21-pytorch16/" class="post-title-link">PyTorch 学习笔记（十六）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 21, 2018</div><div class="post-content"><p>1 . <code>gerattr</code>、<code>setattr</code> 与 <code>__getattr__</code>、<code>__setattr__</code>。  </p>
<ul>
<li><code>result = obj.name</code> 会调用 buildin 函数 <code>getattr(obj, &#39;name&#39;)</code>，如果该属性找不到，会调用 <code>obj.__getattr__(&#39;name&#39;)</code>，没有实现 <code>__getattr__</code> 或者 <code>__getattr__</code> 也无法处理的就会 <code>raise AttributeError</code>;</li>
<li><code>obj.name = value</code> 会调用 buildin 函数 <code>setattr(obj, &#39;name&#39;, value)</code>，如果 obj 对象实现了 <code>__setattr__</code> 方法，<code>setattr</code> 会直接调用 <code>obj.__setattr__(&#39;name&#39;, value)</code>。</li>
</ul>
<p><code>nn.Module</code> 实现了自定义的 <code>__setattr__</code> 函数，当执行 <code>module.name = value</code> 时，会在 <code>__setattr__</code> 中判断 value 是否为 <code>Parameter</code> 或 <code>nn.Module</code> 对象，如果是则将这些对象加到 <code>_parameters</code> 和 <code>_modules</code> 这两个字典中，而如果是其他类型的对象，如 <code>Variable</code>、<code>list</code>、<code>dict</code> 等，则调用默认的操作，将这个值保存在 <code>__dict__</code> 中。  </p>
<p> Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">module = nn.Module()</span><br><span class="line">module.param = nn.Parameter(t.ones(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(module._parameters)</span><br><span class="line"></span><br><span class="line">submodule1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">submodule2 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">module_list = [submodule1, submodule2]</span><br><span class="line"><span class="comment"># 对于 list 对象，调用 buildin 函数，保存在 __dict__ 中</span></span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'_modules: '</span>, module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']:"</span>, module.__dict__.get(<span class="string">'submodules'</span>))</span><br><span class="line"></span><br><span class="line">module_list = nn.ModuleList(module_list)</span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'ModuleList is instance of nn.Module: '</span>, isinstance(module_list, nn.Module))</span><br><span class="line">print(<span class="string">'_modules: '</span>, module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']: "</span>, module.__dict__.get(<span class="string">'submodules'</span>))</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542763477/Snip20181121_2.png" alt="">  </p>
<p>因 <code>_modules</code> 和 <code>_parameters</code> 中的 item 未保存在 <strong>dict</strong> 中，所以默认的 <code>getattr</code> 方法无法获取它，因而 <code>nn.Module</code> 实现了自定义的 <code>__getattr__</code> 方法，如果默认的 <code>getattr</code> 无法处理，就调用自定义的 <code>__getattr__</code> 方法，尝试从 <code>_modules</code>、<code>_parameters</code> 和 <code>_buffers</code> 这三个字典中获取。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(getattr(module, <span class="string">'training'</span>))  <span class="comment"># 等价于 module.training</span></span><br><span class="line"><span class="comment"># error</span></span><br><span class="line"><span class="comment"># print(module.__getattr__('training'))</span></span><br><span class="line"></span><br><span class="line">module.attr1 = <span class="number">2</span></span><br><span class="line">print(getattr(module, <span class="string">'attr1'</span>))</span><br><span class="line"><span class="comment"># error</span></span><br><span class="line"><span class="comment"># print(module.__getattr__('attr1'))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 即 module.param，会调用 module.__getattr__('param')</span></span><br><span class="line">print(getattr(module, <span class="string">'param'</span>))</span><br></pre></td></tr></table></figure>
<p>Output:  </p>
<p><img src="https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542764618/Snip20181121_3.png" alt=""><br></p></div><a href="/2018/11/21/2018/11-21-pytorch16/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/20/2018/11-20-pytorch15/" class="post-title-link">PyTorch 学习笔记（十五）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 20, 2018</div><div class="post-content"><p>1 . <code>nn</code> 中的大多数 layer，在 <code>nn.functional</code> 中都有一个与之相对应的函数，<code>nn.functional</code> 中的函数和 <code>nn.Module</code> 的主要区别在于，用 <code>nn.Module</code> 实现的 layers 是一个特殊的类，都是由 <code>class layer(nn.Module)</code> 定义，会自动提取可学习的参数，而 <code>nn.functional</code> 中的函数更像是纯函数，由 <code>def function(input)</code> 定义。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">output1 = model(input)</span><br><span class="line">output2 = nn.functional.linear(input, model.weight, model.bias)</span><br><span class="line">print(output1 == output2)</span><br><span class="line"></span><br><span class="line">b = nn.functional.relu(input)</span><br><span class="line">b2 = nn.ReLU()(input)</span><br><span class="line">print(b == b2)</span><br></pre></td></tr></table></figure>
<p>Output:<br><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxeq3vk9axj20wy05a74n.jpg" alt="">  </p>
<p>如果模型有可学习的参数，最好用 <code>nn.Module</code>，否则既可以用 <code>nn.Module</code> 也可以使用 <code>nn.functional</code>，二者在性能上没有太大差异。但 dropout 操作虽然没有可学习的参数，但还是建议使用 <code>nn.Dropout</code> 而不是 <code>nn.functional.dropout</code>，因为 dropout 在训练和测试两个阶段的行为有所差异，使用 <code>nn.Module</code> 对象能够通过 <code>model.eval</code> 操作加以区分。  </p>
<p>在模型中搭配使用 <code>nn.Module</code> 和 <code>nn.functional</code>：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.pool(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.pool(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数 <code>__init__</code> 中。<br></p></div><a href="/2018/11/20/2018/11-20-pytorch15/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/20/2018/11-20-pytorch14/" class="post-title-link">PyTorch 学习笔记（十四）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 20, 2018</div><div class="post-content"><p>1 . PyTorch 实现了如今最常用的三种循环神经网络（RNN）：RNN(vanilla RNN)、LSTM 和 GRU，此外还有对应的三种 RNNCell，RNN 和 RNNCell 层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上 RNN 层的一种后端实现方式就是调用 RNNCell 来实现的。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 输入：batch_size=3，序列长度都为 2，序列中每个元素占 4 维</span></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># lstm 输入向量 4 维，隐藏元 3，1 层</span></span><br><span class="line">lstm = nn.LSTM(<span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 初始状态：1 层，batch_size=3，3 个隐藏元</span></span><br><span class="line">h0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">c0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out, hn = lstm(input, (h0, c0))</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 一个 LSTMCell 对应的层数只能是一层</span></span><br><span class="line">lstm = nn.LSTMCell(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">hx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">cx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out = []</span><br><span class="line"><span class="keyword">for</span> i_ <span class="keyword">in</span> input:</span><br><span class="line">    hx, cx = lstm(i_, (hx, cx))</span><br><span class="line">    out.append(hx)</span><br><span class="line">t.stack(out)</span><br></pre></td></tr></table></figure>
<p>词向量在自然语言中应用十分普及，PyTorch 同样提供了 Embedding 层。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有 4 个词，每个词用 5 维的向量表示</span></span><br><span class="line">embedding = nn.Embedding(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 可以用训练好的词向量初始化 embedding</span></span><br><span class="line">embedding.weight.data = t.arange(<span class="number">0</span>, <span class="number">20</span>).view(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = t.arange(<span class="number">3</span>, <span class="number">0</span>, <span class="number">-1</span>).long()</span><br><span class="line">output = embedding(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure></div><a href="/2018/11/20/2018/11-20-pytorch14/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/20/2018/11-20-pytorch13/" class="post-title-link">PyTorch 学习笔记（十三）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 20, 2018</div><div class="post-content"><p>1 . 图像的卷积操作。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line"></span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">to_pil = ToPILImage()</span><br><span class="line">lena = Image.open(<span class="string">'path to your image'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入是一个 batch，batch_size=1</span></span><br><span class="line">input = to_tensor(lena).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锐化卷积核</span></span><br><span class="line">kernel = t.ones(<span class="number">3</span>, <span class="number">3</span>)/<span class="number">-9</span></span><br><span class="line">kernel[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = conv(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure></div><a href="/2018/11/20/2018/11-20-pytorch13/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/19/2018/11-19-pytorch12/" class="post-title-link">PyTorch 学习笔记（十二）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 19, 2018</div><div class="post-content"><p>1 . <code>torch.nn</code> 是专门为深度学习而设计的模块，它的核心数据结构是 <code>Module</code>，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 <code>nn.Module</code>，撰写自己的网络/层。 下面自定义一个全连接层。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer = Linear(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">output = layer(input)</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> layer.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br></pre></td></tr></table></figure></div><a href="/2018/11/19/2018/11-19-pytorch12/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/18/2018/11-18-pytorch11/" class="post-title-link">PyTorch 学习笔记（十一）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 18, 2018</div><div class="post-content"><p>1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  </p>
<p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">print(a.data)  <span class="comment"># 还是同一个 tensor</span></span><br><span class="line"></span><br><span class="line">print(a.data.requires_grad)  <span class="comment"># 但是已经独立于计算图之外了</span></span><br><span class="line"></span><br><span class="line">d = a.data.sigmoid_()  <span class="comment"># sigmoid_ 是一个 inplace 操作，会修改 a 自身的值</span></span><br><span class="line">print(a)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">print(a.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错</span></span><br><span class="line">tensor = a.detach()</span><br><span class="line">print(tensor.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计 tensor 的一些指标，不希望被记录</span></span><br><span class="line">mean = tensor.mean()</span><br><span class="line">std = tensor.std()</span><br><span class="line">maximum = tensor.max()</span><br><span class="line">print(mean, std, maximum)</span><br><span class="line"></span><br><span class="line">tensor[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># 下面会报错： RuntimeError: one of the variables needed for gradient</span></span><br><span class="line"><span class="comment">#              computation has been modified by an inplace operation.</span></span><br><span class="line"><span class="comment"># 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确</span></span><br><span class="line"><span class="comment"># c.sum().backward()</span></span><br></pre></td></tr></table></figure></p></div><a href="/2018/11/18/2018/11-18-pytorch11/" class="read-more">MORE</a></article></li></ul></main><footer><div class="paginator"><a href="/page/2/" class="next">NEXT</a></div><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>