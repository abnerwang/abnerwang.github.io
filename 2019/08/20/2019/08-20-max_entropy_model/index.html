<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 最大熵模型 · Qiyexuxu</title><meta name="description" content="最大熵模型 - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">最大熵模型</h1><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/最大熵模型/" class="tag-title">#最大熵模型</a></div><div class="post-info">Aug 20, 2019</div><div class="post-content"><p>将最大熵原理应用在分类问题上，就得到了最大熵模型，而为了讲述最大熵原理，我们首先介绍熵的概念。</p>
<h3 id="熵以及最大熵原理"><a href="#熵以及最大熵原理" class="headerlink" title="熵以及最大熵原理"></a>熵以及最大熵原理</h3><h4 id="熵的概念"><a href="#熵的概念" class="headerlink" title="熵的概念"></a>熵的概念</h4><p>设 $X$ 为可取有限个不确定值的离散型随机变量，它的概率分布为：</p>
<p>$$P(X=x_i)=p_i$$</p>
<p>则该随机变量 $X$ 的熵定义为：</p>
<p>$$H(P)=-\Sigma_xp_ilogp_i$$</p>
<p>特别地，令 $0log0=0$。上式中对数底为 $2$ 时，熵的单位为比特（bit），对数底为 $e$ 时，熵的单位为纳特（nat）。<strong>随机变量的熵反映了随机变量取值的不确定性程度</strong>。熵的取值范围如下：</p>
<p>$$0 \leq H(P) \leq log|X|$$</p>
<p>上式中，$|X|$ 代表随机变量 $X$ 的取值个数。当随机变量的概率分布为均匀分布时，其熵取得最大值。</p>
<a id="more"></a>
<h4 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h4><p>最大熵原理认为，在所有符合约束条件的概率模型中，熵最大的模型是最好的模型。</p>
<h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><p>最大熵模型用于处理分类问题，与 <a href="http://abnerwang.github.io/2019/08/19/2019/08-19-logistic_regression/" target="_blank" rel="noopener">logistic 回归模型</a> 一样，它是概率模型，也就是说，若设含 $n$ 个样本的训练数据集为 $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), …, (x^{(n)}, y^{(n)})\}$，那么通过该训练数据集最后学得的模型形式应为 $P(y|x)$，其中 $x$ 是特征，$y$ 是类别。同时，它也属于对数线性模型的范畴，只不过这里的模型对数不与显而易见的简单线性函数形式存在相等关系。</p>
<p>如上所述，我们最终学得的模型形式为 $P(y|x)$，将最大熵原理应用在这里的分类问题中，即我们需要 <strong>从所有满足条件的模型 $P(y|x)$ 中</strong>，找到其中熵最大的作为我们最终得到的最优模型，即为最大熵模型。$P(y|x)$ 是条件概率的形式，因此我们这里需要用到的熵为条件熵。条件熵的数学表达式如下：</p>
<p>$$H(P)=-\Sigma_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)$$</p>
<p>上式中，我们取对数底为自然对数，$\widetilde{P}(x)$ 为从训练数据集中得到的关于 $x$ 的经验概率，它等于特征 $x$ 在训练数据集中出现的频率。</p>
<p>以上条件熵的数学表达式实际上可以通过如下过程得到：<br>如果将这里的条件概率看作如上一部分中所述的一般概率形式，则随机变量 $Y$ 的熵为 $-\Sigma_yP(y|x)logP(y|x)$，但是真正来说，这里的条件概率将 $x$ 作为一个随机变量来对待，引入随机变量 $x$ 后的条件熵应该为随机变量 $X$ 关于以上随机变量 $Y$ 的熵的数学期望，即为：$-\Sigma_x\widetilde{P}(x)\Sigma_yP(y|x)logP(y|x)$，进一步化简，可得最终条件熵的数学表达式为：$-\Sigma_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)$。</p>
<p>让我们回到一开始的主题上来，如前所述，最大熵模型即为条件熵最大的模型，不仅如此，最大熵模型还必须满足训练数据集中的特征约束条件，为了表征这些特征约束条件，我们定义特征指示函数如下：</p>
<p>$$f(x, y)=\begin{cases}1，&amp; \text{$x$、$y$ 满足某一特征约束条件;}\\<br>0, &amp; \text{otherwise.} \end{cases}$$</p>
<p>设有 $n$ 个特征约束，则有 $n$ 个特征指示函数 $f_1(x, y), f_2(x, y), …, f_n(x, y)$， 我们希望某个特征在训练集中出现的概率与在总体样本中出现的概率相同，反映在数学上，也就是说，该特征在训练集中的数学期望与其在总体样本中的数学期望相等，即有：</p>
<p>$$\mathbb{E} _{\widetilde{P}}(f_i(x, y)) = \mathbb{E}_ P(f_i(x, y)) \quad i=1,2,…,n$$</p>
<p>也就是说：</p>
<p>$$\Sigma_{x,y}\widetilde{P}(x, y)f_i(x, y)=\Sigma_{x,y}P(x, y)f_i(x, y) \quad i=1,2,…,n$$</p>
<p>上式中，$\widetilde{P}(x, y)$ 为特征约束 $f_i(x, y)$ 在训练集中出现的经验概率，同样，它等于特征约束 $f_i(x, y)$ 在训练集中出现的频率。</p>
<p>进一步地：</p>
<p>$$\Sigma_{x,y}\widetilde{P}(x, y)f_i(x, y)=\Sigma_{x,y}\widetilde{P}(x)P(y|x)f_i(x, y) \quad i=1,2,…,n$$</p>
<p>由此，求最大熵模型的过程变为求解如下带约束条件的最优化问题：</p>
<p>$$max_P \quad H(P)=-\Sigma_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)\\<br>s.t. \quad \mathbb{E} _{\widetilde{P}}(f_i(x, y))=\mathbb{E}_ P(f_i(x, y)) \quad i=1,2,…,n\\<br>\quad \Sigma_yP(y|x)=1$$</p>
<p>按照求最优化问题的习惯，将以上最大化问题转换为最小化问题如下：</p>
<p>$$min_P \quad -H(P)=\Sigma_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)\\<br>s.t. \quad \mathbb{E} _{\widetilde{P}}(f_i(x, y))=\mathbb{E}_ P(f_i(x, y)) \quad i=1,2,…,n\\<br>\quad \Sigma_yP(y|x)=1$$</p>
<p>对于带约束条件的最优化问题，考虑使用拉格朗日乘子法，引进拉格朗日乘子 $w_0, w_1, w_2, …, w_n$，构造广义拉格朗日函数如下：</p>
<p>$$\mathcal{L}(P, w)=-H(P)+w_0(1-\Sigma_yP(y|x))+\Sigma_i^nw_i(\mathbb{E} _{\widetilde{P}}(f_i(x, y))-\mathbb{E}_ P(f_i(x, y)))\\<br>=\Sigma_{x,y}\widetilde{P}(x)P(y|x)logP(y|x)+w_0(1-P(y|x))+\\<br>\Sigma_1^nw_i(\Sigma_{x,y}\widetilde{P}(x, y)f_i(x, y)-\Sigma_{x,y}\widetilde{P}(x)P(y|x)f_i(x, y))$$</p>
<p>由 <a href="http://abnerwang.github.io/2019/08/19/2019/08-19-lagrange_duality/" target="_blank" rel="noopener">拉格朗日对偶</a> 知，与原始问题等价的优化问题为广义拉格朗日函数的极小极大化问题，即：</p>
<p>$$min_P\;max_w\;\mathcal{L}(P, w)$$</p>
<p>其对偶问题为：</p>
<p>$$max_w\;min_P\;\mathcal{L}(P, w)$$</p>
<p>由于 $-H(P)$ 是关于 $P$ 的凸函数，所以可以通过求解对偶问题的解从而求得原始问题的解。对于以上对偶问题，先考虑极小化问题 $min_P\;\mathcal{L}(P, w)$，广义拉格朗日函数 $\mathcal{L}(P, w)$ 针对 $P$ 取得极小值的必要条件为 $\nabla_P\mathcal{L}(P, w)=0$，即有：</p>
<p>$$<br>\nabla_P\mathcal{L}(P, w)=\Sigma_{x, y}\widetilde{P}(x)logP(y|x)+\Sigma_{x,y}\widetilde{P}(x)+w_0-\Sigma_{x,y}\widetilde{P}(x)\Sigma_1^nw_if_i(x, y)\\<br>=\Sigma_{x,y}\widetilde{P}(x)[logP(y|x)+1+\frac{w_0}{\Sigma_{x,y}\widetilde{P}(x)}-\Sigma_1^nw_if_i(x, y)]=0<br>$$</p>
<p>在 $\widetilde{P}(x)&gt;0$ 的情况下，则必有：$logP(y|x)+1+\frac{w_0}{\Sigma_{x,y}\widetilde{P}(x)}-\Sigma_1^nw_if_i(x, y)=0$，解得：</p>
<p>$$P_w(y|x)=exp(\Sigma_1^nw_if_i(x,y)+\frac{w_0}{\Sigma_{x,y}\widetilde{P}(x)}-1)\\<br>=\frac{exp\Sigma_1^nw_if_i(x, y)}{exp(1-\frac{w_0}{\Sigma_{x,y}\widetilde{P}(x)})}$$</p>
<p>符号 $P_w(y|x)$ 表示求得的 $P(y|x)$ 是关于 $w$ 的函数，令 $Z_w(x)=exp(1-\frac{w_0}{\Sigma_{x,y}\widetilde{P}(x)})$，则有：</p>
<p>$$P_w(y|x)=\frac{1}{Z_w(x)}exp(\Sigma_1^nw_if_i(x, y))$$</p>
<p>上式中，$w_i$ 为特征约束 $f_i(x, y)$ 的权重。实际上，上式两端取自然对数 $log$，得：$logP_w(y|x)=log\frac{exp(\Sigma_1^nw_if_i(x, y))}{Z_w(x)}=\Sigma_1^nw_if_i(x, y)-logZ_w(x)$，也就是说，模型 $P_w(y|x)$ 与特征函数 $f_i(x, y)$ 存在着以上这种线性关系，这也是最大熵模型是一种对数线性模型的原因。</p>
<p>又由于 $\Sigma_yP_w(y|x)=1$，代入上式，两边分别取 $\Sigma_y$，可解得：</p>
<p>$$Z_w(x)=\Sigma_yexp(\Sigma_1^nw_if_i(x, y))$$</p>
<p>接着考虑对偶问题最外部的极大化问题，即对偶问题的解为：$\mathcal{L}(P_w, w)$，取得以上最优解时 $w$ 对应的最优值 $w^*=argmax_w\;\mathcal{L}(P_w, w)$。可以采用改进的迭代尺度法和拟牛顿法求解这里的最优值，我们将在下一篇文章中分别介绍这两种算法，在这之前，我们先来看一下最大熵模型的极大似然估计与对偶函数的极大化之间有什么关系。</p>
<h3 id="最大熵模型的极大似然估计"><a href="#最大熵模型的极大似然估计" class="headerlink" title="最大熵模型的极大似然估计"></a>最大熵模型的极大似然估计</h3><p>设 $\widetilde{P}(x,y)$ 是训练数据集中关于样本特征 $x$ 和标签 $y$ 的联合概率分布，则条件分布 $P(y|x)$ 的似然函数为：</p>
<p>$$\mathcal{l}(P(y|x))=\Pi_{x,y}P(y|x)^{\widetilde{P}(x,y)}$$</p>
<p>取对数似然：</p>
<p>$$\mathcal{L}(P(y|x))=log\Pi_{x,y}P(y|x)^{\widetilde{P}(x,y)}=\Sigma_{x,y}\widetilde{P}(x,y)logP(y|x)$$</p>
<p>将上文中得到的最大熵模型 $P_w(y|x)$ 代入上式，得：</p>
<p>$$<br>\mathcal{L}(P_w(y|x))=\Sigma_{x,y}\widetilde{P}(x,y)logP_w(y|x)\\<br>=\Sigma_{x,y}\widetilde{P}(x,y)(\Sigma_1^nw_if_i(x, y)-logZ_w(x))\\<br>=\Sigma_{x,y}\widetilde{P}(x,y)\Sigma_1^nw_if_i(x, y)-\Sigma_x\widetilde{P}(x)\Sigma_y\widetilde{P}(y|x)logZ_w(x)\\<br>=\Sigma_{x,y}\widetilde{P}(x,y)\Sigma_1^nw_if_i(x, y)-\Sigma_x\widetilde{P}(x)logZ_w(x)<br>$$</p>
<p>我们回过头来看一下对偶函数的极大化过程，将最大熵模型 $P_w(y|x)$ 代入以上广义拉格朗日函数中（注意下面的 $\mathcal{L}$ 为广义拉格朗日函数而不是上文的对数似然函数），得：</p>
<p>$$\mathcal{L}(P_w, w)=\Sigma_{x,y}\widetilde{P}(x)P_w(y|x)logP_w(y|x)+w_0(1-\Sigma_yP_w(y|x))+\Sigma_1^nw_i(\Sigma_{x,y}\widetilde{P}(x, y)f_i(x, y)-\Sigma_{x,y}\widetilde{P}(x)P_w(y|x)f_i(x, y))\\<br>=\Sigma_{x,y}\widetilde{P}(x)P_w(y|x)(\Sigma_1^nw_if_i(x,y)-logZ_w(x))+\Sigma_{x,y}\widetilde{P}(x, y)\Sigma_1^nw_if_i(x, y)-\Sigma_{x,y}\widetilde{P}(x)P_w(y|x)\Sigma_1^nw_if_i(x, y)\\<br>=\Sigma_{x,y}\widetilde{P}(x, y)\Sigma_1^nw_if_i(x, y)-\Sigma_{x,y}\widetilde{P}(x)P_w(y|x)logZ_w(x)\\<br>=\Sigma_{x,y}\widetilde{P}(x, y)\Sigma_1^nw_if_i(x, y)-\Sigma_x\widetilde{P}(x)\Sigma_yP_w(y|x)logZ_w(x)\\<br>=\Sigma_{x,y}\widetilde{P}(x,y)\Sigma_1^nw_if_i(x, y)-\Sigma_x\widetilde{P}(x)logZ_w(x)$$</p>
<p>由此可见，对偶函数的极大化与最大熵模型的极大似然估计是等价的。因此，我们可以通过求解最大熵模型的极大似然估计问题来求解对偶函数的极大化，而要使用的方法就是我们下篇文章所述的改进的迭代尺度法和 <a href="http://abnerwang.github.io/2019/08/16/2019/08-16-newton-method/" target="_blank" rel="noopener">拟牛顿法</a>。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/08/19/2019/08-19-logistic_regression/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2019/08/20/2019/08-20-max_entropy_model/';
var disqus_title = '最大熵模型';
var disqus_url = 'http://blog.keeplearning.group/2019/08/20/2019/08-20-max_entropy_model/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>