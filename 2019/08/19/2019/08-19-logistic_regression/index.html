<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> logistic 回归模型 · Qiyexuxu</title><meta name="description" content="logistic 回归模型 - David Wang"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">logistic 回归模型</h1><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/logistic-回归模型/" class="tag-title">#logistic 回归模型</a></div><div class="post-info">Aug 19, 2019</div><div class="post-content"><p>logistic 回归模型用于分类问题，它是概率模型，同时属于对数线性模型。这是什么意思呢？概率模型，说明最后学到的模型形式是 $P(Y|X=x)$，其中 $X$ 是特征，$Y$ 是类别。对数线性模型，说明与学得的模型 $P(Y|X=x)$ 有关的某种对数形式与线性函数 $w \cdot x + b$（$w$ 为权重参数，$b$ 为偏置参数）相等。事实上，基于这两点，我们可以推导出 logistic 回归模型的数学形式。</p>
<p>首先，我们从最简单的用于二分类的二项 logistic 回归模型说起，之后推广到用于多分类的多项 logistic 回归模型。</p>
<h3 id="二项-logistic-回归模型"><a href="#二项-logistic-回归模型" class="headerlink" title="二项 logistic 回归模型"></a>二项 logistic 回归模型</h3><p>对于二分类问题，设分类标签 $Y \in \{1, 0\}$。我们首先观察一下线性函数 $w \cdot x + b$，$w$ 和 $b$ 是需要学习的参数，对它们的取值范围不加任何限制，也就是说，其取值范围为 $(-\infty, +\infty)$，而 $P(Y|X=x) \in [0, 1]$，将一个 $[0, 1]$ 范围内的数映射到 $(-\infty, +\infty)$ 范围上，需要经过某种变换，在这里，我们使用对数几率变换，即 logit 变换。</p>
<a id="more"></a>
<p>在介绍 logit 变换之前，先介绍一下一个事件发生的几率的概念。设一个事件发生的概率是 $p$，那么该事件不发生的概率为 $1-p$，将 $\frac{p}{1-p}$ 定义为该事件发生的几率，加上对数后得 $log \frac{p}{1-p}$（$log$ 代表以自然对数 $e$ 为底的对数），上式称为某事件发生的对数几率，与该式对应的变换称为 logit 变换。</p>
<p>将 logit 变换应用到二分类的概率模型 $P(Y|X=x)$ 上，得：</p>
<p>$$log\frac{P(Y=1|X=x)}{1-P(Y=1|X=x)}=w \cdot x + b$$</p>
<p>由上式求得：</p>
<p>$$P(Y=1|X=x)=\frac{exp(w \cdot x + b)}{1 + exp(w \cdot x + b)}$$</p>
<p>进而：</p>
<p>$$P(Y=0|X=x)=1-P(Y=1|X=x)=\frac{1}{1 + exp(w \cdot x + b)}$$</p>
<p>以上两式就是我们所说的二项 logistic 回归模型。</p>
<p>通常，我们将 $b$ 也纳入权重参数 $w$ 中，若 $w、x \in \mathbb{R}^n$，则纳入 $b$ 后 $w$ 为 $(w_1, w_2, …, w_n, b)$，$x$ 相应地变化为 $(x_1, x_2, …, x_n, 1)$，新的 $w \in \mathbb{R}^{n+1}$，新的 $x \in \mathbb{R}^{n+1}$。则以上二项 logistic 回归模型变为：</p>
<p>$$P(Y=1|X=x)=\frac{exp(w \cdot x)}{1 + exp(w \cdot x)}\\<br>P(Y=0|X=x)=\frac{1}{1 + exp(w \cdot x)}$$</p>
<p>对于以上问题，模型我们已知了，但是参数 $w$ 却是未知的，我们可以采用极大似然估计的方法得到这里的参数 $w$。</p>
<p>设训练集为 $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), …, (x^{(N)}, y^{(N)})\}$，共有 $N$ 个样本，$x^{(i)}$ 为样本特征，$y^{(i)}$ 为对应样本的类别标签，$y^{(i)} \in \{0, 1\}$。令样本 $i$ 是类别 $1$ 的概率为 $\pi(x^{(i)})=\frac{exp(w \cdot x^{(i)})}{1 + exp(w \cdot x^{(i)})}$，则其是类别 $0$ 的概率为 $1-\pi(x^{(i)})=\frac{1}{1 + exp(w \cdot x^{(i)})}$，由此可得似然函数为：</p>
<p>$$l(w)=\Pi_1^N\pi(x^{(i)})^{y^{(i)}}(1-\pi(x^{(i)}))^{1-y^{(i)}}$$</p>
<p>取对数似然：</p>
<p>$$L(w)=\Sigma_{i=1}^N [y^{(i)}log\pi(x^{(i)})+(1-y^{(i)})log(1-\pi(x^{(i)}))]\\<br>=\Sigma_{i=1}^N[y^{(i)}log \frac{\pi(x^{(i)})}{1-\pi(x^{(i)})}+log(1-\pi(x^{(i)})]\\<br>=\Sigma_{i=1}^N[y^{(i)} \cdot wx^{(i)}-log(1+exp(wx^{(i)}))]$$</p>
<p>由此，二项 logistic 回归模型的学习问题变为以上对数似然函数的极大化问题。logistic 回归模型的学习算法通常采用梯度下降法和 <a href="http://abnerwang.github.io/2019/08/16/2019/08-16-newton-method/" target="_blank" rel="noopener">拟牛顿法</a>。</p>
<h3 id="多项-logistic-回归模型"><a href="#多项-logistic-回归模型" class="headerlink" title="多项 logistic 回归模型"></a>多项 logistic 回归模型</h3><p>多项 logistic 回归模型用于多分类问题。设在多分类问题中的类别标签共有 $K$ 个，设样本 $k$ 属于某个类别的概率是 $p_k$，这里用到的 logit 变换为 $log\frac{p_k}{p_K}$。将该 logit 变换用到这里的多项 logistic 回归模型中，得：</p>
<p>$$log\frac{P(Y=k|X=x^{(k)})}{P(Y=K|X=x^{(K)})}=w^{(k)}x^{(k)}$$</p>
<p>即有：</p>
<p>$$\frac{P(Y=k|X=x^{(k)})}{P(Y=K|X=x^{(K)})}=exp(w^{(k)}x^{(k)})\\<br>\Rightarrow \frac{1-P(Y=K|X=x^{(K)})}{P(Y=K|X=x^{(K)})}=\Sigma_{k=1}^{K-1}exp(w^{(k)}x^{(k)})$$</p>
<p>综上，可求得：</p>
<p>$$P(Y=K|X=x^{(K)})=\frac{1}{1+\Sigma_{k=1}^{K-1}exp(w^{(k)}x^{(k)})}$$</p>
<p>将上式代入一开始通过 logistic 变换得到的式子中，求得：</p>
<p>$$P(Y=k|X=x^{(k)})=\frac{exp(w^{(k)}x^{(k)})}{1+\Sigma_{k=1}^{K-1}exp(w^{(k)}x^{(k)})}$$</p>
<p>以上两式即为多项 logistic 回归模型。二项 logistic 回归模型的参数估计方法可推广到多项 logistic 回归模型中，不再赘述。</p>
</div></article></div></main><footer><div class="paginator"><a href="/2019/08/19/2019/08-19-lagrange_duality/" class="next">NEXT</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'abnerwang';
var disqus_identifier = '2019/08/19/2019/08-19-logistic_regression/';
var disqus_title = 'logistic 回归模型';
var disqus_url = 'http://blog.keeplearning.group/2019/08/19/2019/08-19-logistic_regression/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//abnerwang.disqus.com/count.js" async></script><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>