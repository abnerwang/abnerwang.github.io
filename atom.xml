<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qiyexuxu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.keeplearning.group/"/>
  <updated>2018-11-20T03:55:36.593Z</updated>
  <id>http://blog.keeplearning.group/</id>
  
  <author>
    <name>David Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch 学习笔记（十三）</title>
    <link href="http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch13/"/>
    <id>http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch13/</id>
    <published>2018-11-20T00:56:00.000Z</published>
    <updated>2018-11-20T03:55:36.593Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 图像的卷积操作。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line"></span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">to_pil = ToPILImage()</span><br><span class="line">lena = Image.open(<span class="string">'path to your image'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入是一个 batch，batch_size=1</span></span><br><span class="line">input = to_tensor(lena).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锐化卷积核</span></span><br><span class="line">kernel = t.ones(<span class="number">3</span>, <span class="number">3</span>)/<span class="number">-9</span></span><br><span class="line">kernel[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = conv(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 池化层可以看作是一种特殊的卷积层，用来下采样，但池化层没有可学习的参数，其 weight 是固定的。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool = nn.AvgPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(list(pool.parameters()))  <span class="comment"># []</span></span><br><span class="line"></span><br><span class="line">out = pool(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>3 . 除了卷积层和池化层，深度学习中还将常用到以下几层：  </p><ul><li>Linear：全连接层；</li><li>BatchNorm：批规范化层，分为 1D、2D 和 3D。除了标准的 BatchNorm 之外，还有在风格迁移中常用到的 InstanceNorm 层；</li><li>Dropout：dropout 层用来防止过拟合，同样分为 1D、2D 和 3D。  </li></ul><pre><code class="python"><span class="comment"># 输入 batch_size = 2，维度３</span>input = t.randn(<span class="number">2</span>, <span class="number">3</span>)linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)h = linear(input)print(h)<span class="comment"># 4 channel，初始化标准差为 4，均值为 0</span>bn = nn.BatchNorm1d(<span class="number">4</span>)bn.weight.data = t.ones(<span class="number">4</span>) * <span class="number">4</span>bn.bias.data = t.zeros(<span class="number">4</span>)bn_out = bn(h)<span class="comment"># 注意输出的均值和方差</span><span class="comment"># 方差是标准差的平方，计算无偏方差分母会减 1</span><span class="comment"># 使用 unbiased=False，分母不减 1</span>print(bn_out.mean(<span class="number">0</span>), bn_out.var(<span class="number">0</span>, unbiased=<span class="keyword">False</span>))<span class="comment"># 每个元素以 0.5 的概率舍弃</span>dropout = nn.Dropout(<span class="number">0.5</span>)o = dropout(bn_out)print(o)  <span class="comment"># 有一半的数变为０</span></code></pre><p>4 . PyTorch 实现了常见的激活函数，这些激活函数可作为独立的 layer 使用，这里介绍一下常用的激活函数 ReLU，其数学表达式为 $ReLU(x)=max(0, x)$。 </p><pre><code class="python">relu = nn.ReLU(inplace=<span class="keyword">True</span>)input = t.randn(<span class="number">2</span>, <span class="number">3</span>)print(input)output = relu(input)  <span class="comment"># 等价于 input.clamp(min=0)</span>print(output)   <span class="comment"># 小于 0  的被截断为 0</span></code></pre><p>ReLU 函数有个 inplace 参数，如果设置为 True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算 ReLU 的反向传播时，只需根据输出就能够推算反向传播的梯度。但是只有少数的 autograd 操作支持 inplace 操作（如 <code>tensor.sigmoid_()</code>)，除非你明确地知道自己在做什么，否则一般不要使用 inplace 操作。</p><p>5 . 在以上例子中，基本上都是将每一层的输出直接作为下一层的输入，这种网络称为前馈传播网络，对于此类网络如果每次都写复杂的 forward 函数会有些麻烦，有两种简化方式，<code>ModuleList</code> 和 <code>Sequential</code>，其中 <code>Sequential</code> 是一个特殊的 Module，它包含几个子 Module，前向传播时会将输入一层接一层地传递下去，<code>ModuleList</code> 也是一个极其特殊的 Module，可以包含几个子 Module，可以像 list 一样使用它，但不能直接把输入传递给 ModuleList。  </p><pre><code class="python"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict<span class="keyword">import</span> torch <span class="keyword">as</span> t<span class="keyword">from</span> torch <span class="keyword">import</span> nn<span class="comment"># Sequential 的三种写法</span>net1 = nn.Sequential()net1.add_module(<span class="string">'conv'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))net1.add_module(<span class="string">'batchnorm'</span>, nn.BatchNorm2d(<span class="number">3</span>))net1.add_module(<span class="string">'activation_layer'</span>, nn.ReLU())net2 = nn.Sequential(    nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),    nn.BatchNorm2d(<span class="number">3</span>),    nn.ReLU())net3 = nn.Sequential(OrderedDict[                         (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),                         (<span class="string">'bn1'</span>, nn.BatchNorm2d(<span class="number">3</span>)),                         (<span class="string">'relu1'</span>, nn.ReLU())                     ])print(<span class="string">'net1:'</span>, net1)print(<span class="string">'net2:'</span>, net2)print(<span class="string">'net3:'</span>, net3)<span class="comment"># 可根据名字或序号取出子 Module</span>print(net1.conv, net2[<span class="number">0</span>], net3.conv1)input = t.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)output = net1(input)output = net2(input)output = net3(input)output = net3.relu1(net1.batchnorm(net1.conv(input)))</code></pre><pre><code class="python"><span class="keyword">import</span> torch <span class="keyword">as</span> t<span class="keyword">from</span> torch <span class="keyword">import</span> nnmodellist = nn.ModuleList([nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU(), nn.Linear(<span class="number">4</span>, <span class="number">2</span>)])input = t.randn(<span class="number">1</span>, <span class="number">3</span>)<span class="keyword">for</span> model <span class="keyword">in</span> modellist:    input = model(input)<span class="comment"># 下面会报错，因为 modellist 没有实现 forward 方法</span><span class="comment"># output = modellist(input)</span></code></pre><p><code>ModuleList</code> 是 <code>Module</code> 的子类，当在 <code>Module</code> 中使用它的时候，就能自动识别为子 module，而 python 自带的 list 则不行。  </p><p>Input:  </p><pre><code class="python"><span class="keyword">from</span> torch <span class="keyword">import</span> nn<span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span>    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span>        super(MyModule, self).__init__()        self.list = [nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU()]        self.module_list = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), nn.ReLU()])    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span>        <span class="keyword">pass</span>model = MyModule()print(model)<span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():    print(name, param)</code></pre><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxec1gj8swj20u00xt0wx.jpg" alt="">  </p><p>可见，list 中的子 Module 并不能被主 Module 所识别，而 ModuleList 中的子 Module  能够被主 Module 所识别，这意味着如果用 list 保存子 Module，将无法调整其参数，因其未加入到主 Module 的参数中。  </p><p>在实际应用中，如果在构造函数 <code>__init__</code> 中用到 list、tuple、dict 等对象时，一定要思考是否应该用 ModuleList 或 ParameterList 代替。</p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 图像的卷积操作。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; PIL &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Image&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torchvision.transforms &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; ToTensor, ToPILImage&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_tensor = ToTensor()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_pil = ToPILImage()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lena = Image.open(&lt;span class=&quot;string&quot;&gt;&#39;path to your image&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 输入是一个 batch，batch_size=1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = to_tensor(lena).unsqueeze(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 锐化卷积核&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kernel = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)/&lt;span class=&quot;number&quot;&gt;-9&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kernel[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conv = nn.Conv2d(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, (&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;), &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conv.weight.data = kernel.view(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;out = conv(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_pil(out.data.squeeze(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十二）</title>
    <link href="http://blog.keeplearning.group/2018/11/19/2018/11-19-pytorch12/"/>
    <id>http://blog.keeplearning.group/2018/11/19/2018/11-19-pytorch12/</id>
    <published>2018-11-19T06:08:00.000Z</published>
    <updated>2018-11-19T11:19:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>torch.nn</code> 是专门为深度学习而设计的模块，它的核心数据结构是 <code>Module</code>，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 <code>nn.Module</code>，撰写自己的网络/层。 下面自定义一个全连接层。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer = Linear(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">output = layer(input)</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> layer.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br></pre></td></tr></table></figure><a id="more"></a> <p>有几点需要注意：  </p><ul><li>自定义层必须继承 <code>nn.Module</code>，并且在其构造函数中需调用 <code>nn.Module</code> 的构造函数；</li><li>在构造函数 <code>__init__</code> 中必须自己定义可学习的参数，并封装成 <code>nn.Parameter</code>，<code>Parameter</code> 是一种特殊的 Tensor，但其默认需要求导（requires_grad=True);</li><li><code>forward</code> 函数实现前向传播过程，其输入可以是一个或多个 tensor；</li><li>无需写反向传播函数，<code>nn.Module</code> 能够利用 autograd 自动实现反向传播；</li><li><code>Module</code> 中的可学习参数可以通过 <code>named_parameters()</code> 或者 <code>parameters()</code> 返回迭代器，前者会给每个 parameter 都附上名字，使其更具有辨识度。</li></ul><p>2 . <code>Module</code> 能够自动检测到自己的 <code>Parameter</code>，并将其作为学习参数，除了 <code>Parameter</code> 之外，<code>Module</code> 还包含子 <code>Module</code>，主 <code>Module</code> 能够递归查找子 <code>Module</code> 中的 <code>Parameter</code>，下面实现一个多层感知机。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, hidden_features, out_features)</span>:</span></span><br><span class="line">        super(Perceptron, self).__init__()</span><br><span class="line">        self.layer1 = Linear(in_features, hidden_features)</span><br><span class="line">        self.layer2 = Linear(hidden_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = t.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> self.layer2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perceptron = Perceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> perceptron.named_parameters():</span><br><span class="line">    print(name, parameter.size())</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxdg714iv6j20s704c3yx.jpg" alt=""></p><p><code>Module</code> 中 parameter 的命名规范：</p><ul><li>对于类似 <code>self.param_name = nn.Parameter(t.randn(3, 4))</code>，命名为 <code>param_name</code>;</li><li>对于子 <code>Module</code> 中的 <code>parameter</code>，会其名字之前加上当前 <code>Module</code> 的名字。如对于 <code>self.sub_module = SubModel()</code>，<code>SubModel</code>中有个 <code>parameter</code> 的名字叫做 <code>param_name</code>，那么二者拼接而成的 <code>parameter name</code> 就是 <code>sub_module.param_name</code>。  </li></ul><p>3 . 为方便用户使用，PyTorch 实现了神经网络中绝大多数的 layer，这些 layer 都继承于 <code>nn.Module</code>，封装了可学习参数 <code>Parameter</code>，并实现了 <code>forward</code> 函数。这些自定义的 layer 对输入形状都有假设：输入的不是单个数据，而是一个 batch，输入只有一个数据，则必须调用 <code>tensor.unsqueeze(0)</code> 或 <code>tensor[None]</code> 将数据伪装成 batch_size=1 的 batch。  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;torch.nn&lt;/code&gt; 是专门为深度学习而设计的模块，它的核心数据结构是 &lt;code&gt;Module&lt;/code&gt;，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 &lt;code&gt;nn.Module&lt;/code&gt;，撰写自己的网络/层。 下面自定义一个全连接层。 &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nn.Module)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, in_features, out_features)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(Linear, self).__init__()  &lt;span class=&quot;comment&quot;&gt;# 等价于 nn.Module.__init__(self)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.w = nn.Parameter(t.randn(in_features, out_features))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.b = nn.Parameter(t.randn(out_features))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = x.mm(self.w)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; x + self.b.expand_as(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;layer = Linear(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output = layer(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(output)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; name, parameter &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; layer.named_parameters():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(name, parameter)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十一）</title>
    <link href="http://blog.keeplearning.group/2018/11/18/2018/11-18-pytorch11/"/>
    <id>http://blog.keeplearning.group/2018/11/18/2018/11-18-pytorch11/</id>
    <published>2018-11-18T04:28:00.000Z</published>
    <updated>2018-11-18T13:39:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">print(a.data)  <span class="comment"># 还是同一个 tensor</span></span><br><span class="line"></span><br><span class="line">print(a.data.requires_grad)  <span class="comment"># 但是已经独立于计算图之外了</span></span><br><span class="line"></span><br><span class="line">d = a.data.sigmoid_()  <span class="comment"># sigmoid_ 是一个 inplace 操作，会修改 a 自身的值</span></span><br><span class="line">print(a)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">print(a.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错</span></span><br><span class="line">tensor = a.detach()</span><br><span class="line">print(tensor.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计 tensor 的一些指标，不希望被记录</span></span><br><span class="line">mean = tensor.mean()</span><br><span class="line">std = tensor.std()</span><br><span class="line">maximum = tensor.max()</span><br><span class="line">print(mean, std, maximum)</span><br><span class="line"></span><br><span class="line">tensor[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># 下面会报错： RuntimeError: one of the variables needed for gradient</span></span><br><span class="line"><span class="comment">#              computation has been modified by an inplace operation.</span></span><br><span class="line"><span class="comment"># 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确</span></span><br><span class="line"><span class="comment"># c.sum().backward()</span></span><br></pre></td></tr></table></figure></p><a id="more"></a><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxc4jy2emjj20wu0edmy2.jpg" alt=""></p><p>2 . 在反向传播过程中非叶子节点的导数计算完之后即被清空，若想查看这些变量的梯度，有两种方法：  </p><ul><li>使用 <code>autograd.grad</code> 函数；</li><li>使用 <code>hook</code>。</li></ul><p>推荐使用 <code>hook</code> 方法，但是在实际应用中应尽量避免修改 grad 的值。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种方法：使用 grad 获取中间变量的梯度</span></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line">z = y.sum()</span><br><span class="line"><span class="comment"># z 对 y 的梯度，隐式调用 backward()</span></span><br><span class="line">print(t.autograd.grad(z, y))  <span class="comment"># (tensor([1., 1., 1.]),)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种方法：使用 hook</span></span><br><span class="line"><span class="comment"># hook 是一个函数，输入是梯度，无返回值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_hook</span><span class="params">(grad)</span>:</span></span><br><span class="line">    print(<span class="string">'y 的梯度：'</span>, grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line"><span class="comment"># 注册 hook</span></span><br><span class="line">hook_handle = y.register_hook(variable_hook)</span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除非你每次都要用 hook，否则用完之后记得移除 hook</span></span><br><span class="line">hook_handle.remove()</span><br></pre></td></tr></table></figure><p>3 . 看看 variable 中的 grad 属性和 backward 函数 grad_variables 参数的含义。  </p><ul><li>variable x 的梯度是目标函数 $f(x)$ 对 x 的梯度，$\frac{df(x)}{dx}=(\frac{df(x)}{dx_{0}},\frac{df(x)}{dx_{1}},…,\frac{df(x)}{dx_N})$，形状和 x 一致；</li><li>对于 <code>y.backward(grad_variables)</code> 中的 <code>grad_variables</code> 相当于链式求导法则 $\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\cdot \frac{\partial{y}}{\partial{x}}$ 中的 $\frac{\partial{z}}{\partial{y}}$，z 是目标函数，一般是一个标量，故而 $\frac{\partial{z}}{\partial{y}}$ 的形状与 variable y 的形状一致，<code>z.backward()</code> 在一定程度上等价于 <code>y.backward(grad_y)</code>。<code>z.backward()</code> 省略了 <code>grad_variables</code> 参数，因为 z 是一个标量，而 $\frac{\partial{z}}{\partial{z}}=1$。  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()  <span class="comment"># 从 z 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">y_gradient = t.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># dz/dy</span></span><br><span class="line">y.backward(y_gradient)  <span class="comment"># 从 y 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure><p>另外需要注意，只有对 variable 的操作才能使用 autograd，如果对 variable 的 data 直接进行操作，将无法使用反向传播，除了对参数初始化，一般我们不会修改 variable.data 的值。  </p><p><strong>总结</strong>  </p><p>PyTorch 中计算图的特点可总结如下：  </p><ul><li><code>autograd</code> 根据用户对 variable 的操作构建计算图，对变量的操作抽象为 <code>Function</code>；</li><li>对于那些不是任何函数的输出，由用户创建的节点称为叶子节点，叶子节点的 <code>grad_fn</code> 为 None，叶子节点中需要求导的 variable，具有 <code>AccumulateGrad</code> 标识，因其梯度是累加的；</li><li>variable 默认是不需要求导的，即 <code>requires_grad</code> 属性默认为 False，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都为 <code>True</code>；</li><li>variable 的 <code>volatitle</code> 属性默认为 <code>False</code>，如果某一个 variable 的 <code>volatitle</code> 属性被设置为 <code>True</code>，那么所有依赖它的节点的 <code>volatitle</code> 属性都为 <code>True</code>，<code>volatitle</code> 为 <code>True</code> 的节点不会求导，<code>volatitle</code> 的优先级比 <code>requires_grad</code> 高；</li><li>多次反向传播时，梯度是累加的，反向传播的中间缓存会被清空，为进行多次反向传播需指定 <code>retian_graph=True</code> 来保存这些缓存；</li><li>非叶子节点的梯度计算完之后即被清空，可以使用 <code>autograd.grad</code> 或 <code>hook</code> 技术获取非叶子节点值；</li><li>variable 的 grad 与 data 形状一致，应避免直接修改 variable.data，因为对 data 的直接操作无法利用 autograd 进行反向传播；</li><li>反向传播函数 <code>backward</code> 的参数 <code>grad_variables</code> 可以看成链式求导的中间结果，如果是标量，可以省略，默认为 1；</li><li>PyTorch 采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。   </li></ul><p>4 . 目前绝大多数函数都可以使用 <code>autograd</code> 实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？那就需要自己写一个 <code>Function</code>，实现它的前向传播和反向传播代码。  </p><p>此外实现了自己的 <code>Function</code> 之后，还可以使用 <code>gradcheck</code> 函数来检测实现是否正确，<code>gradcheck</code> 通过数值逼近来计算梯度，可能具有一定的误差，通过控制 <code>eps</code> 的大小可以控制容忍的误差。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyAdd</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, w, x, b)</span>:</span></span><br><span class="line">        ctx.save_for_backward(w, x)</span><br><span class="line">        output = w * x + b</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        w, x = ctx.saved_tensors</span><br><span class="line">        grad_w = grad_output * x</span><br><span class="line">        grad_x = grad_output * w</span><br><span class="line">        grad_b = grad_output * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> grad_w, grad_x, grad_b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始前向传播</span></span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line"><span class="comment"># 开始反向传播</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># x 不需要求导，中间过程还是会计算它的导数，但随后被清空</span></span><br><span class="line">print(x.grad, w.grad, b.grad)  <span class="comment"># (None, tensor([1.]), tensor([1.]))</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, x, )</span>:</span></span><br><span class="line">        output = <span class="number">1</span> / (<span class="number">1</span> + t.exp(-x))</span><br><span class="line">        ctx.save_for_backward(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        output, = ctx.saved_tensors</span><br><span class="line">        grad_x = output * (<span class="number">1</span> - output) * grad_output</span><br><span class="line">        <span class="keyword">return</span> grad_x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用数值逼近方式检验计算梯度的公式对不对</span></span><br><span class="line">test_input = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">test_input.requires_grad_()</span><br><span class="line">t.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  &lt;/p&gt;
&lt;p&gt;Input:&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c = a * b&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.data)  &lt;span class=&quot;comment&quot;&gt;# 还是同一个 tensor&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.data.requires_grad)  &lt;span class=&quot;comment&quot;&gt;# 但是已经独立于计算图之外了&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d = a.data.sigmoid_()  &lt;span class=&quot;comment&quot;&gt;# sigmoid_ 是一个 inplace 操作，会修改 a 自身的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(d.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tensor = a.detach()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(tensor.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 统计 tensor 的一些指标，不希望被记录&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mean = tensor.mean()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;std = tensor.std()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;maximum = tensor.max()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(mean, std, maximum)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tensor[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 下面会报错： RuntimeError: one of the variables needed for gradient&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#              computation has been modified by an inplace operation.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# c.sum().backward()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch10/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch10/</id>
    <published>2018-11-17T11:13:00.000Z</published>
    <updated>2018-11-17T14:25:57.582Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>torch.autograd</code> 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在创建 tensor 的时候指定 requires_grad</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">b = t.zeros(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">c = a.add(b)   <span class="comment"># 也可以写成 c = a + b</span></span><br><span class="line"></span><br><span class="line">d = c.sum()</span><br><span class="line">d.backward()   <span class="comment"># 反向传播</span></span><br><span class="line">print(d.requires_grad)   <span class="comment"># d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，</span></span><br><span class="line"><span class="comment"># 因此 c 的 requires_grad 属性会自动设置为 True</span></span><br><span class="line">print(a.requires_grad, b.requires_grad, c.requires_grad)   <span class="comment"># (True, True, True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否为叶子节点</span></span><br><span class="line">print(a.is_leaf, b.is_leaf, c.is_leaf)   <span class="comment"># (True, True, False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，</span></span><br><span class="line"><span class="comment"># 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了</span></span><br><span class="line">print(c.grad <span class="keyword">is</span> <span class="keyword">None</span>)    <span class="comment"># True</span></span><br></pre></td></tr></table></figure><p>2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  </p><p>$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  </p><a id="more"></a><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''手动求导函数'''</span></span><br><span class="line">    dx = <span class="number">2</span> * x * t.exp(x) + x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line">y.backward(t.ones(y.size()))</span><br><span class="line">print(x.grad)</span><br><span class="line">print(gradf(x))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxbcx53kogj20yc078gmn.jpg" alt="">  </p><p>3 . 每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个 variable 的梯度，这些函数的函数名通常以 Backward 结尾。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x  <span class="comment"># 等价于 y = w.mul(x)</span></span><br><span class="line">z = y + b  <span class="comment"># 等价于 z = y.add(b)</span></span><br><span class="line"></span><br><span class="line">print(x.requires_grad, b.requires_grad, w.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_fn 可以查看这个 variable 的反向传播函数，</span></span><br><span class="line"><span class="comment"># z 是 add 函数的输出，所以它的反向传播函数是 AddBackward</span></span><br><span class="line">print(z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># next_functions 保存 grad_fn 的输入，是一个 tuple，tuple 的元素也是 Function</span></span><br><span class="line"><span class="comment"># 第一个是 y，它是乘法（mul）的输出，所以对应的反向传播函数 y.grad_fn 是 MulBackward</span></span><br><span class="line"><span class="comment"># 第二个是 b，它是叶子节点，由用户创建，grad_fn 为 None，但是需要求导，其梯度是累加的</span></span><br><span class="line">print(z.grad_fn.next_functions)</span><br><span class="line">print(z.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>] == y.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个是 w，叶子节点，需要求导，梯度是累加的</span></span><br><span class="line"><span class="comment"># 第二个是 x，叶子节点，不需要求导，所以为 None</span></span><br><span class="line">print(y.grad_fn.next_functions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 叶子节点的 grad_fn 是 None</span></span><br><span class="line">print(w.grad_fn, x.grad_fn)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxbdwxg9bwj214406owfg.jpg" alt="">  </p><p>计算 $w$ 的梯度的时候，需要用到 $x$ 的数值（$\frac{\partial y}{\partial w}=x$），这些数值在前向过程中会保存成 buffer，在计算完梯度之后会自动清空，为了能够多次反向传播需要指定 <code>retain_graph</code> 来保留这些 buffer。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多次反向传播，梯度会累加，这也就是 w 中 AccumulateGrad 标识的含义</span></span><br><span class="line">z.backward()</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([2.])</span></span><br></pre></td></tr></table></figure><p>4 . PyTorch 使用的是动态图，它的计算图在每次前向传播时都是从头开始构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x.data[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line">x = <span class="number">-1</span> * t.ones(<span class="number">1</span>)  <span class="comment"># 写成 x = -1 * t.ones(1, requires_grad=True) 时，x 不计算梯度</span></span><br><span class="line">x = x.requires_grad_()</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([-1.])</span></span><br></pre></td></tr></table></figure><p>变量的 <code>requires_grad</code> 属性默认 <code>False</code>，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都是 <code>True</code>。  </p><p>5 . 有时候可能不希望 autograd 对 tensor 求导，因为求导需要缓存许多中间结构，增加额外的内存/显存开销，同时降低运行速度，那么我们可以关闭自动求导，譬如在模型训练完毕转而进行测试推断的时候。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> t.no_grad():  <span class="comment"># 也可以使用 t.set_grad_enable(False) 设置（无需 with），并且以下代码无缩进</span></span><br><span class="line">    x = t.ones(<span class="number">1</span>)</span><br><span class="line">    w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    y = x * w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y 虽然依赖于 w 和 x，虽然 w.requires_grad=True，但是 y.requires_grad=False</span></span><br><span class="line">    print(x.requires_grad, w.requires_grad, y.requires_grad)  <span class="comment"># (False, True, False)</span></span><br></pre></td></tr></table></figure><p>关闭自动求导后可以使用 <code>t.set_grad_enable(True)</code> 恢复设置。</p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;torch.autograd&lt;/code&gt; 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 在创建 tensor 的时候指定 requires_grad&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 或者&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;).requires_grad_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 或者&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a.requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;b = t.zeros(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;).requires_grad_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c = a.add(b)   &lt;span class=&quot;comment&quot;&gt;# 也可以写成 c = a + b&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d = c.sum()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d.backward()   &lt;span class=&quot;comment&quot;&gt;# 反向传播&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(d.requires_grad)   &lt;span class=&quot;comment&quot;&gt;# d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 因此 c 的 requires_grad 属性会自动设置为 True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.requires_grad, b.requires_grad, c.requires_grad)   &lt;span class=&quot;comment&quot;&gt;# (True, True, True)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 判断是否为叶子节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.is_leaf, b.is_leaf, c.is_leaf)   &lt;span class=&quot;comment&quot;&gt;# (True, True, False)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(c.grad &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;)    &lt;span class=&quot;comment&quot;&gt;# True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  &lt;/p&gt;
&lt;p&gt;$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  &lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（九）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch9/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch9/</id>
    <published>2018-11-17T09:03:00.000Z</published>
    <updated>2018-11-17T09:14:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节实现一个训练线性回归参数的例子，线性回归的损失函数为：$loss=\frac{1}{2} \sum_{i=1}^{N}(y_{i}-(wx_{i}+b))^2$，然后利用随机梯度下降法更新参数 $w$ 和 $b$ 来最小化损失函数，最终学得 $w$ 和 $b$ 的值。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">device = t.device(<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fake_data</span><span class="params">(batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    x = t.rand(batch_size, <span class="number">1</span>, device=device) * <span class="number">5</span></span><br><span class="line">    y = x * <span class="number">2</span> + <span class="number">3</span> + t.randn(batch_size, <span class="number">1</span>, device=device)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机初始化参数</span></span><br><span class="line">w = t.rand(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">b = t.zeros(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.02</span>  <span class="comment"># 设置学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    x, y = get_fake_data(batch_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward: 计算loss</span></span><br><span class="line">    y_pred = x.mm(w) + b.expand_as(y)</span><br><span class="line">    loss = <span class="number">0.5</span> * (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward: 手动计算梯度</span></span><br><span class="line">    dloss = <span class="number">1</span></span><br><span class="line">    dy_pred = dloss * (y_pred - y)</span><br><span class="line"></span><br><span class="line">    dw = x.t().mm(dy_pred)</span><br><span class="line">    db = dy_pred.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w.sub_(lr * dw)</span><br><span class="line">    b.sub_(lr * db)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ii % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 画图</span></span><br><span class="line">        x = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        x = x.float()</span><br><span class="line">        y = x.mm(w.cpu()) + b.cpu().expand_as(x)</span><br><span class="line">        plt.plot(x.cpu().numpy(), y.cpu().numpy())  <span class="comment"># predicted</span></span><br><span class="line"></span><br><span class="line">        x2, y2 = get_fake_data(batch_size=<span class="number">32</span>)</span><br><span class="line">        plt.scatter(x2.cpu().numpy(), y2.cpu().numpy())  <span class="comment"># true data</span></span><br><span class="line"></span><br><span class="line">        plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">        plt.ylim(<span class="number">0</span>, <span class="number">13</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'w: '</span>, w.item(), <span class="string">'b: '</span>, b.item())</span><br></pre></td></tr></table></figure></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节实现一个训练线性回归参数的例子，线性回归的损失函数为：$loss=\frac{1}{2} \sum_{i=1}^{N}(y_{i}-(wx_{i}+b))^2$，然后利用随机梯度下降法更新参数 $w$ 和 $b$ 来最小化损失函数，最终学得 $w$ 和 $b$ 的值。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（八）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch8/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch8/</id>
    <published>2018-11-17T01:49:00.000Z</published>
    <updated>2018-11-17T04:27:26.035Z</updated>
    
    <content type="html"><![CDATA[<p>1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。<br><a id="more"></a><br>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.storage())</span><br><span class="line"></span><br><span class="line">b = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个对象的 id 值可以看作它在内存中的地址</span></span><br><span class="line"><span class="comment"># storage 的内存地址一样，即是同一个 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># a 改变，b 也随之改变，因为它们共享 storage</span></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">c = a[<span class="number">2</span>:]</span><br><span class="line">print(c.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># data_ptr 返回 tensor 首元素的内存地址</span></span><br><span class="line"><span class="comment"># 可以看出相差 16，这是因为 2*8=16</span></span><br><span class="line"><span class="comment"># 相差两个元素，每个元素占 8 个字节（long）</span></span><br><span class="line">print(c.data_ptr(), a.data_ptr())</span><br><span class="line"></span><br><span class="line">c[<span class="number">0</span>] = <span class="number">-100</span>  <span class="comment"># c[0] 的内存地址对应 a[2]</span></span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">d = t.Tensor(c.storage().float())</span><br><span class="line">print(id(c.storage()) == id(d.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面 3 个 tensor 共享 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()) == id(c.storage))</span><br><span class="line"></span><br><span class="line">print(a.storage_offset(), c.storage_offset())</span><br><span class="line"></span><br><span class="line">e = b[::<span class="number">2</span>, ::<span class="number">2</span>]  <span class="comment"># 隔 2 行/列取一个元素</span></span><br><span class="line">print(id(e.storage()) == id(a.storage()))</span><br><span class="line"></span><br><span class="line">print(b.stride(), e.stride())</span><br><span class="line"></span><br><span class="line">print(e.is_contiguous())</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fxauj7ug2xj20wu0u0acf.jpg" alt="">  </p><p>可见绝大多数操作并不修改 tensor 的数据，而只是修改了 tensor 的头信息。这种做法更节省内存，同时提升了处理速度。此外有些操作会导致 tensor 不连续，这时需要调用 <code>tensor.contiguous</code> 方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享 storage。  </p><p>2 . tensor 可以随意地在 GPU/CPU 上传输，使用 <code>tensor.cuda(device_id)</code> 或者 <code>tensor.cpu()</code>，另外一个更通用的方法是 <code>tensor.to(device)</code>。  </p><ul><li>尽量使用 <code>tensor.to(device)</code>，将 <code>device</code> 设为一个可配置的参数，这样可以很轻松地使程序同时兼容 GPU 和 CPU；</li><li>数据在 GPU 之中传输的速度要远快于内存（CPU）到显存（GPU），所以尽量避免在内存和显存之间传输数据。</li></ul><p>3 . tensor 的保存和加载十分简单，使用 <code>torch.save</code> 和 <code>torch.load</code> 即可完成相应的功能。在 save/load 时可以指定使用的 pickle 模块，在 load 时还可以将 GPU tensor 映射到 CPU 或者其他 GPU 上。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> t.cuda.is_available():</span><br><span class="line">    a = a.cuda(<span class="number">1</span>)   <span class="comment"># 把 a 转为 GPU1 上的 tensor</span></span><br><span class="line">    t.save(a, <span class="string">'a.pth'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载 b，存储于 GPU1 上（因为保存时 tensor 就在 GPU1 上）</span></span><br><span class="line">    b = t.load(<span class="string">'a.pth'</span>)</span><br><span class="line">    <span class="comment"># 加载为 c，存储于 CPU 上</span></span><br><span class="line">    c = t.load(<span class="string">'a.pth'</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">    <span class="comment"># 加载为 d，存储于 GPU0 上</span></span><br><span class="line">    d = t.load(<span class="string">'a.pth'</span>, map_location=&#123;<span class="string">'cuda:1'</span>:<span class="string">'cuda:0'</span>&#125;)</span><br></pre></td></tr></table></figure><p>4 . 关于 tensor 还有几点需要注意：  </p><ul><li>大多数 <code>torch.function</code> 都有一个参数 <code>out</code>，这时候产生的结果将保存在 <code>out</code> 指定的 tensor 之中；</li><li><code>torch.set_num_threads</code> 可以设置 PyTorch 进行 CPU 多线程并行计算时候所占用的线程数，这个可以用来限制 PyTorch 所占用的 CPU 数目；</li><li><code>torch.set_printoptions</code> 可以用来设置打印 tensor 时的数值精度和格式。</li></ul><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">t.set_printoptions(precision=<span class="number">10</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxay2fzkx2j20sq04uaan.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>理解 Momentum</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-momentum/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-momentum/</id>
    <published>2018-11-16T13:39:00.000Z</published>
    <updated>2018-11-17T01:29:47.331Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>随机梯度下降（SGD）方法的一个缺点是其更新方向完全依赖于当前 batch 计算出的梯度，因而十分不稳定，Momentum 算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向，这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优的能力。数学表达式如下：  </p><p>$$v_{t}=\gamma v_{t-1}+\alpha \cdot \nabla_{\theta }J(\theta)$$  $$\theta=\theta -v_{t}$$  </p><p>Momentum 算法会观察历史梯度 $v_{t-1}$，若当前梯度的方向与历史梯度一致，表明当前样本不太可能为异常点，则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。<br><a id="more"></a></p><h2 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h2><p>在小球向下滚的过程中，我们希望小球能够提前知道在哪些方向坡面会上升，这样在遇到上升坡面之前，小球就开始减速，这方法就是 Nesterov Momentum，其在凸优化中有较强的理论保证收敛，并且，在实践中 Nesterov Momentum 也要比单纯的 Momentum 的效果好。数学表达式如下：  </p><p>$$v_{t}=\gamma v_{t-1} + \alpha \cdot \nabla_{\theta}J(\theta-\gamma v_{t-1})$$  $$\theta=\theta-v_{t}$$  </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Momentum&quot;&gt;&lt;a href=&quot;#Momentum&quot; class=&quot;headerlink&quot; title=&quot;Momentum&quot;&gt;&lt;/a&gt;Momentum&lt;/h2&gt;&lt;p&gt;随机梯度下降（SGD）方法的一个缺点是其更新方向完全依赖于当前 batch 计算出的梯度，因而十分不稳定，Momentum 算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向，这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优的能力。数学表达式如下：  &lt;/p&gt;
&lt;p&gt;$$v_{t}=\gamma v_{t-1}+\alpha \cdot \nabla_{\theta }J(\theta)$$  $$\theta=\theta -v_{t}$$  &lt;/p&gt;
&lt;p&gt;Momentum 算法会观察历史梯度 $v_{t-1}$，若当前梯度的方向与历史梯度一致，表明当前样本不太可能为异常点，则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Momentum" scheme="http://blog.keeplearning.group/tags/Momentum/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（七）</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch7/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch7/</id>
    <published>2018-11-16T08:01:00.000Z</published>
    <updated>2018-11-16T13:18:07.473Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">abs/sqrt/div/exp/fmod/log/pow…</td><td style="text-align:center">绝对值/平方根/除法/指数/求余/求幂…</td></tr><tr><td style="text-align:center">cos/sin/asin/atan2/cosh…</td><td style="text-align:center">相关三角函数</td></tr><tr><td style="text-align:center">ceil/round/floor/trunc</td><td style="text-align:center">上取整/四舍五入/下取整/只保留整数部分</td></tr><tr><td style="text-align:center">clamp(input, min, max)</td><td style="text-align:center">超过 min 和 max 部分截断</td></tr><tr><td style="text-align:center">sigmod/tanh..</td><td style="text-align:center">激活函数</td></tr></tbody></table><p>对于很多操作，例如 <code>div</code>、<code>mul</code>、<code>pow</code>、<code>fmod</code> 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 <code>a ** 2</code> 等价于 <code>torch.pow(a, 2)</code>，<code>a * 2</code> 等价于 <code>torch.mul(a, 2)</code>。<br><a id="more"></a><br>2 . 归并操作会使输出形状小于输入形状，并可以沿着某一维度进行执行操作，如加法 <code>sum</code>，既可以计算整个 tensor 的和，也可以计算 tensor 中每一行或每一列的和，常用的归并操作如下表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">mean/sum/median/mode</td><td style="text-align:center">均值/和/中位数/众数</td></tr><tr><td style="text-align:center">norm/dist</td><td style="text-align:center">范数/距离</td></tr><tr><td style="text-align:center">std/var</td><td style="text-align:center">标准差/方差</td></tr><tr><td style="text-align:center">cumsum/cumprod</td><td style="text-align:center">累加/累乘</td></tr></tbody></table><p>以上大多数函数都有一个参数 <code>dim</code>，用来指定这些操作是在哪个维度上执行的。  </p><p>假设输入的形状是 (m, n, k)：</p><ul><li>如果指定 <code>dim=0</code>，输出形状就是 (1, n ,k) 或者 (n, k)</li><li>如果指定 <code>dim=1</code>，输出形状就是 (m, 1, k) 或者 (m, k)</li><li>如果指定 <code>dim=2</code>，输出形状就是 (m, n, 1) 或者 (m, n)</li></ul><p>size 中是否有 “1”，取决于参数 <code>keepdim</code>,<code>keepdim=True</code> 会保留维度 1，注意，以上只是经验总结，并非所有的函数都符合这种形状变化方式。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">   </span><br><span class="line">b = t.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">True</span>))  <span class="comment"># 保留维度 1</span></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">False</span>))  <span class="comment"># 不保留维度 1</span></span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.cumsum(dim=<span class="number">1</span>))  <span class="comment"># 沿着行累加</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxa4du79t4j216408e3ze.jpg" alt=""></p><p>3 . 常用的比较函数表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">gt/lt/ge/le/eq/ne</td><td style="text-align:center">大于/小于/大于等于/小于等于/等于/不等</td></tr><tr><td style="text-align:center">topk</td><td style="text-align:center">最大的 k 个数</td></tr><tr><td style="text-align:center">sort</td><td style="text-align:center">排序</td></tr><tr><td style="text-align:center">max/min</td><td style="text-align:center">比较两个 tensor 最大最小值</td></tr></tbody></table><p>表中第一行的比较操作已经实现了运算符重载，因此可以使用 <code>a&gt;=b</code>、<code>a&gt;b</code>、<code>a!=b</code>、<code>a==b</code>，其返回结果是一个 ByteTensor，可用来选取元素。max/min 这两个操作比较特殊，以 <code>max</code> 为例说明如下：</p><ul><li><code>torch.max(tensor)</code>：返回 tensor 中最大的那个数；</li><li><code>torch.max(tensor, dim)</code>：指定维度上最大的数，同时返回 tensor 和下标；</li><li><code>torch.max(tensor1, tensor2)</code>：返回两个 tensor 相比较大的元素。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = t.linspace(<span class="number">15</span>, <span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">print(a &gt; b)</span><br><span class="line"></span><br><span class="line">print(a[a &gt; b])  <span class="comment"># a 中对应位置大于 b 的元素</span></span><br><span class="line"></span><br><span class="line">print(t.max(a))</span><br><span class="line"></span><br><span class="line">print(t.max(b, dim=<span class="number">1</span>))  <span class="comment"># 分别返回对应维度的最大值和最大值所在的下标</span></span><br><span class="line"></span><br><span class="line">print(t.max(a, b))</span><br><span class="line"></span><br><span class="line">print(t.clamp(a, min=<span class="number">10</span>))  <span class="comment"># 比较 a 和 10 较大的元素</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa5kkgcmbj215y0emq4l.jpg" alt=""></p><p>4 . 常用的线性代数函数表如下。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">trace</td><td style="text-align:center">矩阵的迹</td></tr><tr><td style="text-align:center">diag</td><td style="text-align:center">对角线元素</td></tr><tr><td style="text-align:center">triu/tril</td><td style="text-align:center">矩阵的上三角/下三角，可指定偏移量</td></tr><tr><td style="text-align:center">mm/bmm</td><td style="text-align:center">矩阵乘法，batch 的矩阵乘法</td></tr><tr><td style="text-align:center">addmm/addbmm/addmv/addr/badbmm…</td><td style="text-align:center">矩阵运算</td></tr><tr><td style="text-align:center">t</td><td style="text-align:center">转置</td></tr><tr><td style="text-align:center">dot/cross</td><td style="text-align:center">內积/外积</td></tr><tr><td style="text-align:center">inverse</td><td style="text-align:center">求逆矩阵</td></tr><tr><td style="text-align:center">svd</td><td style="text-align:center">奇异值分解</td></tr></tbody></table><p>需要注意的是，矩阵的转置会导致存储空间不连续，需要调用 <code>.contiguous</code> 方法将其转为连续。  </p><p>5 . Numpy 和 Tensor 可以相互转换，共享内存，但是当 Numpy 的数据类型和 Tensor 的数据类型不一样的时候，数据会被复制，不会共享内存。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(a.dtype)</span><br><span class="line"></span><br><span class="line">b = t.Tensor(a)   <span class="comment"># 此处进行拷贝，不共享内存</span></span><br><span class="line">print(b.dtype)</span><br><span class="line"></span><br><span class="line">c = t.from_numpy(a)  <span class="comment"># 注意 c 的类型（DoubleTensor）</span></span><br><span class="line">print(c.dtype)</span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)   <span class="comment"># b 与 a 不共享内存，a 改变但 b 不变</span></span><br><span class="line">print(c)   <span class="comment"># c 与 a 共享内存</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxa6jr9iznj20ue07ywf2.jpg" alt="">  </p><p>6 . 广播法则是科学计算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。  </p><p>Numpy 的广播法则定义如下：  </p><ul><li>让所有输入数组都向其中 shape 最长的数组看齐，shape 中不足的部分通过在前面加 1 补齐；</li><li>两个数组在某一维度的长度要么一致，要么其中一个为 1，否则不能计算；</li><li>当输入数组的某个维度的长度为 1 时，计算时沿此维度复制扩充成一样的形状。</li></ul><p>PyTorch 已经支持了自动广播法则，但是我们还是通过以下两个函数手动实现一下广播法则以加深理解吧。  </p><ul><li><code>unsqueeze</code> 或者 <code>view</code>，或者 <code>tensor[None]</code>，为数据某一维的形状补 1，实现法则 1；</li><li><code>expand</code> 或者 <code>expand_as</code>，重复数组，实现法则 3，该操作不会复制数组，所以不会占用额外的空间。</li></ul><p>注意：<code>repeat</code> 实现与 <code>expand</code> 相类似的功能，但是 <code>repeat</code> 会把相同数据复制多份，因此会占用额外的空间。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = t.zeros(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动广播法则</span></span><br><span class="line"><span class="comment"># 第一步： a 是 2 维，b 是 3 维，所以先在较小的 a 前面补 1，</span></span><br><span class="line"><span class="comment"># 即： a.unsqueeze(0)，a 的形状变为 (1, 3, 2)，b 的形状是 (2, 3, 1)</span></span><br><span class="line"><span class="comment"># 第二步： a 和 b 在第一维和第三维形状不一致，其中一个为 1，</span></span><br><span class="line"><span class="comment"># 可以利用广播法则，两者都扩展成 (2, 3, 2)</span></span><br><span class="line"></span><br><span class="line">print(a + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动广播法则</span></span><br><span class="line"><span class="comment"># 或者 a.view(1, 3, 2).expand(2, 3, 2) + b.expand(2, 3, 2)</span></span><br><span class="line">print(a[<span class="keyword">None</span>].expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>) + b.expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa7an8bd0j20ug0fkgm5.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;函数&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;abs/sqrt/div/exp/fmod/log/pow…&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;绝对值/平方根/除法/指数/求余/求幂…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;cos/sin/asin/atan2/cosh…&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;相关三角函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;ceil/round/floor/trunc&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;上取整/四舍五入/下取整/只保留整数部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;clamp(input, min, max)&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;超过 min 和 max 部分截断&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;sigmod/tanh..&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;激活函数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;对于很多操作，例如 &lt;code&gt;div&lt;/code&gt;、&lt;code&gt;mul&lt;/code&gt;、&lt;code&gt;pow&lt;/code&gt;、&lt;code&gt;fmod&lt;/code&gt; 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 &lt;code&gt;a ** 2&lt;/code&gt; 等价于 &lt;code&gt;torch.pow(a, 2)&lt;/code&gt;，&lt;code&gt;a * 2&lt;/code&gt; 等价于 &lt;code&gt;torch.mul(a, 2)&lt;/code&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（六）</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch6/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch6/</id>
    <published>2018-11-16T01:15:00.000Z</published>
    <updated>2018-11-16T07:15:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . Tensor 支持与 <code>numpy.ndarray</code> 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>])    <span class="comment"># 第 0 行，下标从 0 开始</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="number">0</span>])     <span class="comment"># 第 0 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>, <span class="number">2</span>])   <span class="comment"># 第 0 行第 2 个元素，等价于 a[0][2]</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>][<span class="number">-1</span>])   <span class="comment"># 第 0 行最后一个元素</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>])    <span class="comment"># 前两行</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>])   <span class="comment"># 前两行，第 0,1 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>:<span class="number">1</span>, :<span class="number">2</span>])  <span class="comment"># 第 0 行，前两列</span></span><br><span class="line">print(a[<span class="number">0</span>, :<span class="number">2</span>])   <span class="comment"># 注意两者的区别，形状不同</span></span><br></pre></td></tr></table></figure><a id="more"></a>  <p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx9neqhzwaj214f0ddjt6.jpg" alt="">  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># None 类似于 np.newaxis，为 a 新增了一个轴</span></span><br><span class="line"><span class="comment"># 等价于 a.view(1, a.shape[0], a.shape[1])</span></span><br><span class="line">print(a[<span class="keyword">None</span>].shape)  <span class="comment"># 等价于 a[None,:,:]</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :].shape)</span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :, <span class="keyword">None</span>, <span class="keyword">None</span>].shape)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320ly1fx9nscpu79j214h03kq33.jpg" alt=""></p><p>2 . 常用的选择函数。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>index_select(input, dim, index)</code></td><td style="text-align:center">在指定维度 dim 上选取，比如选取某些行、某些列</td></tr><tr><td style="text-align:center"><code>masked_select(input, mask)</code></td><td style="text-align:center">使用 ByteTensor 进行选取</td></tr><tr><td style="text-align:center"><code>non_zero(input)</code></td><td style="text-align:center">非 0 元素的下标</td></tr><tr><td style="text-align:center"><code>gather(input, dim, index)</code></td><td style="text-align:center">根据 index，在 dim 维度上选取数据，输出的 size 与 index 一样</td></tr></tbody></table><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a &gt; <span class="number">1</span>)  <span class="comment"># 返回一个 ByteTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于 a.masked_select(a&gt;1)</span></span><br><span class="line">print(a[a &gt; <span class="number">1</span>])  <span class="comment"># 选择结果与原 tensor 不共享内存空间</span></span><br><span class="line"></span><br><span class="line">print(a[t.LongTensor([<span class="number">0</span>, <span class="number">1</span>])])  <span class="comment"># 第 0 行和第 1 行</span></span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">Output:  </span><br><span class="line">  </span><br><span class="line">![](http://wx3.sinaimg.cn/mw690/<span class="number">79225320</span>gy1fx9osqbau0j214807lgmc.jpg)  </span><br><span class="line">  </span><br><span class="line">`gather` 是一个比较复杂的操作，对于一个 <span class="number">2</span> 维的 tensor，输出的每个元素如下：</span><br></pre></td></tr></table></figure><p>out[i][j] = input[index[i][j]][j]   # dim=0<br>out[i][j] = input[i][index[i][j]]   # dim=1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">三维 tensor 的 `gather` 操作同理。  </span><br><span class="line">  </span><br><span class="line">`gather(input, dim, index)` 中的 dim 表示的就是第几维度，在二维的例子中，如果 dim=0，那么它表示的就是你接下来的操作是对第一维度进行的，也就是行；如果 dim=1，那么它表示的就是你接下来的操作是对第二个维度进行的，也就是列。index 的大小和 input 的大小是一样的，它表示的是你所选择的维度上的操作。特别注意，index 必须是 LongTensor 类型。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line">import torch as t</span><br><span class="line"></span><br><span class="line">a = t.arange(0, 16).view(4, 4)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># 选取对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3]])</span><br><span class="line">print(a.gather(0, index))</span><br><span class="line"></span><br><span class="line"># 选取反对角线上的元素</span><br><span class="line">index = t.LongTensor([[3, 2, 1, 0]]).t()</span><br><span class="line">print(a.gather(1, index))</span><br><span class="line"></span><br><span class="line"># 选取两个对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()</span><br><span class="line">b = a.gather(1, index)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9purwv3xj214j0d8gmb.jpg" alt="">  </p><p>与 <code>gather</code> 相对应的逆操作是 <code>scatter_</code>，<code>gather</code> 把数据从 input 中按照 index 取出，而 <code>scatter_</code> 是把取出的数据再放回去（inplace 操作）。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把两个对角线的元素放回到指定位置</span></span><br><span class="line">c = t.zeros(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">b = b.float()  <span class="comment"># 将 b 转换成 FloatTensor</span></span><br><span class="line">c.scatter_(<span class="number">1</span>, index, b)</span><br><span class="line">print(c)</span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">对 tensor 的任何索引操作仍是一个 tensor，想要获取标准的 tensor 对象数值，需要调用 `tensor.item()`，这个方法只对包含一个元素的 tensor 适用。</span><br><span class="line">  </span><br><span class="line"><span class="number">3</span> . PyTorch 目前已支持绝大多数 numpy 的高级索引，高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的 Tensor 共享内存。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">27</span>).view(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">0</span>]])  <span class="comment"># x[1, 1, 2] 和 x[2, 2, 0]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])  <span class="comment"># x[2, 0, 1]，x[1, 0, 1], x[0, 0, 1]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">0</span>, <span class="number">2</span>], ...])   <span class="comment"># x[0] 和 x[2]</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9wkft7f2j21570kf0tu.jpg" alt=""></p><p>4 . Tensor 有不同的数据类型，每种类型分别对应有 CPU 和GPU 版本（HalfTensor 除外），默认的 tensor 都是 FloatTensor，可通过 <code>torch.set_default_tensor_type</code> 来修改默认 tensor 类型，如果默认类型为 GPU tensor，则所有操作都在 GPU 上进行，HalftTensor 是专门为 GPU 版本设计的，同样的元素个数，显存占用只有 FloatTensor 的一半，所以可以极大缓解 GPU 显存不足的问题，但由于其数值大小和精度有限，所以可能出现溢出等问题。  </p><p>各数据类型之间可以相互转换，<code>type(new_type)</code> 是通用的做法，同时还有 <code>float</code>、<code>long</code>、<code>half</code> 等快捷方法，CPU tensor 和 GPU tensor 之间的相互转换通过 <code>tensor.cuda</code> 和 <code>tensor.cpu</code> 方法来实现，此外还可以使用 <code>tensor.to(device)</code>。  </p><p>Tensor 还有一个 <code>new</code> 方法，用法与 <code>t.Tensor</code> 一样，会调用该 tensor 对应类型的构造函数，生成与当前 tensor 类型一致的 tensor，<code>torch.*_like(tensor)</code> 可以生成和 tensor 拥有同样属性（类型、形状、CPU?GPU）的新 tensor。<code>tensor.new_*(new_shape)</code> 新建一个不同形状的 tensor。  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . Tensor 支持与 &lt;code&gt;numpy.ndarray&lt;/code&gt; 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  &lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])    &lt;span class=&quot;comment&quot;&gt;# 第 0 行，下标从 0 开始&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])     &lt;span class=&quot;comment&quot;&gt;# 第 0 列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 第 0 行第 2 个元素，等价于 a[0][2]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 第 0 行最后一个元素&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])    &lt;span class=&quot;comment&quot;&gt;# 前两行&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 前两行，第 0,1 列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, :&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])  &lt;span class=&quot;comment&quot;&gt;# 第 0 行，前两列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, :&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 注意两者的区别，形状不同&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（五）</title>
    <link href="http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch5/"/>
    <id>http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch5/</id>
    <published>2018-11-15T11:57:00.000Z</published>
    <updated>2018-11-15T13:44:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 通过 <code>tensor.view</code> 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另一个也会跟着改变。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.view(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当某一维为 -1 时，会自动计算它的大小</span></span><br><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line">print(b.shape)</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 在实际应用中可能需要添加或减少某一维度，这时 <code>squeeze</code> 和 <code>unsqueeze</code> 两个函数就派上用场了。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(b)</span><br><span class="line">print(b.shape)</span><br><span class="line"></span><br><span class="line">print(b.unsqueeze(<span class="number">1</span>))  <span class="comment"># 在第 1 维（下标从 0 开始）上增加 "1"</span></span><br><span class="line"><span class="comment"># 等价于 b[:,None]</span></span><br><span class="line">print(b[:, <span class="keyword">None</span>].shape)</span><br><span class="line"></span><br><span class="line">print(b.unsqueeze(<span class="number">-2</span>))  <span class="comment"># -2 表示倒数第二个维度</span></span><br><span class="line"></span><br><span class="line">c = b.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(c)</span><br><span class="line">print(c.squeeze(<span class="number">0</span>))   <span class="comment"># 压缩第 0 维的 "1"</span></span><br><span class="line"></span><br><span class="line">print(c.squeeze())   <span class="comment"># 把所有维度为 "1" 的都压缩掉</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)   <span class="comment"># b 作为 view 之后的也跟着被修改</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx92fwjyk7j213s0icgmx.jpg" alt="">  </p><p>3 . <code>resize</code> 是另一种可用来调整 <code>size</code> 的方法，但是与 <code>view</code> 不同，它可以修改 tensor 的大小，如果新大小超过了原大小，会自动分配新的存储空间，而如果新大小小于原大小，则之前的数据依旧会被保存。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">b = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">b.resize_(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">b.resize_(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx92noka8jj214404udfy.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 通过 &lt;code&gt;tensor.view&lt;/code&gt; 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另一个也会跟着改变。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a = t.arange(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.view(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当某一维为 -1 时，会自动计算它的大小&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = a.view(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(b.shape)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（四）</title>
    <link href="http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch4/"/>
    <id>http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch4/</id>
    <published>2018-11-15T01:18:00.000Z</published>
    <updated>2018-11-15T07:29:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 从接口的角度来讲，对 tensor 的操作可分为两类：</p><ul><li><code>torch.function</code>，如 <code>torch.save</code> 等；</li><li>另一类是 <code>tensor.function</code>，如 <code>tensor.view</code> 等。</li></ul><p>为了方便使用，对 tensor 的大部分操作同时支持这两类接口，如 <code>torch.sum(torch.sum(a, b))</code> 与 <code>tensor.sum(a.sum(b))</code> 功能等价。</p><a id="more"></a><p>2 . 在 Pytorch 中新建 tensor 的方法具体有很多，如下表：  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>Tensor(*sizes)</code></td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center"><code>tensor(data)</code></td><td style="text-align:center">类似 <code>np.array</code> 的构造函数</td></tr><tr><td style="text-align:center"><code>ones(*sizes)</code></td><td style="text-align:center">全 1 Tensor</td></tr><tr><td style="text-align:center"><code>zeros(*sizes)</code></td><td style="text-align:center">全 0  Tensor</td></tr><tr><td style="text-align:center"><code>eye(*sizes)</code></td><td style="text-align:center">对角线为 1，其他为 0</td></tr><tr><td style="text-align:center"><code>arange(s, e, steps)</code></td><td style="text-align:center">从 s  到 e，步长为 step</td></tr><tr><td style="text-align:center"><code>linspace(s, e, steps)</code></td><td style="text-align:center">从 s 到 e，均匀切分成 steps 份</td></tr><tr><td style="text-align:center"><code>rand/randn(*sizes)</code></td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center"><code>normal(mean, std)/uniform(from, to)</code></td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center"><code>randperm(m)</code></td><td style="text-align:center">随机排列</td></tr></tbody></table><p>这些创建方法都可以在创建的时候指定数据类型 <code>dtype</code> 和存放 device(cpu/gpu)。　　</p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>)   <span class="comment"># 指定 tensor 的形状，其数值取决于内存空间的状态，print 的时候可能 overflow</span></span><br><span class="line"></span><br><span class="line">b = t.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])   <span class="comment"># 用 list 的数据创建 tensor</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">print(b.tolist())  <span class="comment"># 把 tensor 转为 list</span></span><br><span class="line"></span><br><span class="line">b_size = b.size()</span><br><span class="line">print(b_size)</span><br><span class="line"></span><br><span class="line">print(b.numel())   <span class="comment"># 返回 b 中元素总个数，等价于 b.nelement()</span></span><br><span class="line"></span><br><span class="line">c = t.Tensor(b_size)   <span class="comment"># 创建一个和 b 形状一样的 tensor</span></span><br><span class="line">d = t.Tensor((<span class="number">2</span>, <span class="number">3</span>))   <span class="comment"># 创建一个元素为 2 和 3 的 tensor</span></span><br><span class="line"></span><br><span class="line">print(c.shape)    <span class="comment"># 与 c.size() 等价</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fx8r4lflf0j214i06o74m.jpg" alt="">  </p><p><code>t.Tensor(*sizes)</code>　创建 tensor 时，系统不会马上分配空间，只会计算剩余的内存是否足够使用，使用到 tensor 时才会分配，而其他操作都是在创建完 tensor 之后马上进行空间分配。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">print(t.ones(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.zeros(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.arange(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(t.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.randn(<span class="number">2</span>, <span class="number">3</span>, device=t.device(<span class="string">'cpu'</span>)))</span><br><span class="line"></span><br><span class="line">print(t.randperm(<span class="number">5</span>))    <span class="comment"># 长度为 5 的随机排列</span></span><br><span class="line"></span><br><span class="line">print(t.eye(<span class="number">2</span>, <span class="number">3</span>, dtype=t.int))  <span class="comment"># 对角线为 1，不要求行数与列数一致</span></span><br><span class="line"></span><br><span class="line">scalar = t.tensor(<span class="number">3.14159</span>)</span><br><span class="line">print(<span class="string">'scalar: %s, shape of scalar: %s'</span> % (scalar, scalar.shape))</span><br><span class="line"></span><br><span class="line">vector = t.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(<span class="string">'vector: %s, shape of vector: %s'</span> % (vector, vector.shape))</span><br><span class="line"></span><br><span class="line">tensor = t.Tensor(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(tensor.shape)</span><br><span class="line"></span><br><span class="line">matrix = t.tensor([[<span class="number">0.1</span>, <span class="number">1.2</span>], [<span class="number">2.2</span>, <span class="number">3.1</span>], [<span class="number">4.9</span>, <span class="number">5.2</span>]])</span><br><span class="line">print(matrix)</span><br><span class="line">print(matrix.shape)</span><br><span class="line"></span><br><span class="line">ten = t.tensor([[<span class="number">0.11111</span>, <span class="number">0.22222</span>, <span class="number">0.33333</span>]], dtype=t.float64, device=t.device(<span class="string">'cpu'</span>))</span><br><span class="line">print(ten)</span><br><span class="line"></span><br><span class="line">empty_tensor = t.tensor([])</span><br><span class="line">print(empty_tensor.shape)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx8rusct9tj214h0kkgo1.jpg" alt="">  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 从接口的角度来讲，对 tensor 的操作可分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.function&lt;/code&gt;，如 &lt;code&gt;torch.save&lt;/code&gt; 等；&lt;/li&gt;
&lt;li&gt;另一类是 &lt;code&gt;tensor.function&lt;/code&gt;，如 &lt;code&gt;tensor.view&lt;/code&gt; 等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了方便使用，对 tensor 的大部分操作同时支持这两类接口，如 &lt;code&gt;torch.sum(torch.sum(a, b))&lt;/code&gt; 与 &lt;code&gt;tensor.sum(a.sum(b))&lt;/code&gt; 功能等价。&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（三）</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch3/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch3/</id>
    <published>2018-11-14T14:00:00.000Z</published>
    <updated>2018-11-15T07:27:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>对于常用的数据集，PyTorch 提供了封装好的接口供用户快速调用，这些主要保存在 <code>torchvision</code> 中。<code>torchvision</code> 实现了常用的图像数据加载功能，例如 Imagenet、CIFAR10、MNIST 等，以及常用的数据转换操作，这极大方便了数据加载，并且具有可重用性。<br><a id="more"></a><br>下面的程序训练网络 LeNet 对 CIFAR-10 数据集分类：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToPILImage</span><br><span class="line"></span><br><span class="line">show = ToPILImage()  <span class="comment"># 可以把 Tensor 转成 Image，方便可视化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义对数据的预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为 Tensor</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),  <span class="comment"># 归一化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">trainset = tv.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">'/home/abnerwang/tmp/data/'</span>,</span><br><span class="line">    train=<span class="keyword">True</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = t.utils.data.DataLoader(</span><br><span class="line">    trainset,</span><br><span class="line">    batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="keyword">True</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">testset = tv.datasets.CIFAR10(</span><br><span class="line">    <span class="string">'/home/abnerwang/tmp/data'</span>,</span><br><span class="line">    train=<span class="keyword">False</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line">testloader = t.utils.data.DataLoader(</span><br><span class="line">    testset,</span><br><span class="line">    batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="keyword">False</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># 定义优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU 训练网络</span></span><br><span class="line">device = t.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> t.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 输入数据</span></span><br><span class="line">        images, labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward</span></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印 log 信息</span></span><br><span class="line">        <span class="comment"># loss 是一个 scalar，需要使用 loss.item() 来获取数值，不能使用 loss[idx]</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:  <span class="comment"># 每 2000 个 batch 打印一下训练状态</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络在测试集上的效果</span></span><br><span class="line">correct = <span class="number">0</span>  <span class="comment"># 预测正确的图片数</span></span><br><span class="line">total = <span class="number">0</span>  <span class="comment"># 总共的图片数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于测试的时候不需要求导，可以暂时关闭 autograd，提高速度，节约内存</span></span><br><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = t.max(outputs, <span class="number">1</span>)   <span class="comment"># predicted 为每行概率最大值的索引，_ 为最大概率值</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'10000 张测试集中的准确率为： %d %%'</span> % (<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于常用的数据集，PyTorch 提供了封装好的接口供用户快速调用，这些主要保存在 &lt;code&gt;torchvision&lt;/code&gt; 中。&lt;code&gt;torchvision&lt;/code&gt; 实现了常用的图像数据加载功能，例如 Imagenet、CIFAR10、MNIST 等，以及常用的数据转换操作，这极大方便了数据加载，并且具有可重用性。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>理解 batch_size</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-batch_size/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-batch_size/</id>
    <published>2018-11-14T12:52:00.000Z</published>
    <updated>2018-11-14T13:20:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>梯度下降主要有以下三种：  </p><ul><li>batch gradient descent：即批梯度下降，其计算开销大，计算速度慢，不支持在线学习；</li><li>stochastic gradient descent：即随机梯度下降，收敛性能不太好，可能 hit 不到最优点；</li><li>mini-batch gradient descent：即小批量梯度下降，通常所说的 batch_size 指的就是这个批量的大小。</li></ul><a id="more"></a><p>batch_size = 1 时称为在线学习。</p><p>batch_size 的选择，首先决定的是下降方向，如果数据集较小，完全可以采用全数据集的形式。</p><p>增大 batch_size 的好处：</p><ul><li>内存的利用率提高了，大矩阵乘法的并行化效率提高了；</li><li>跑完一次 epoch（即全数据集）所需迭代次数减少，对于相同数据量的处理速度进一步加快；</li><li>在一定范围内，batch_size 越大，其确定的下降方向就越准确，引起训练震荡越小。</li></ul><p>盲目增大 batch_size 的坏处：</p><ul><li>当数据集太大时，内存撑不住；</li><li>跑完全数据集所需迭代次数减少了，但要达到相同的精度，时间开销大，参数的修正更加缓慢；</li><li>batch_size 增大到一定程度，其确定的下降方向已基本不再变化。</li></ul><p><strong>总结：</strong>  </p><p>1) batch_size 太小，而类别又比较多的时候，可能会导致 loss 函数震荡而不收敛，尤其是网络比较复杂的时候；</p><p>2) 随着 batch_size 的增大，处理相同数据量的速度越快；</p><p>3) 随着 batch_size 的增大，达到相同的精度所需的训练 epoch 数量越多；</p><p>4) 由于上述两种因素的矛盾，batch_size 增加到某个时候，达到时间上的最优；</p><p>5) 由于最终收敛精度会陷入不同的局部极值，因此，batch_size 增大到某些时候，达到最终收敛精度上的最优；</p><p>6) 过大的 batch_size 会使网络收敛到一些不好的局部最优点，同样太小的 batch_size 会使网络收敛太慢、不易收敛等；</p><p>7) 具体 batch_size 的选取和训练集的样本数目有关。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梯度下降主要有以下三种：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;batch gradient descent：即批梯度下降，其计算开销大，计算速度慢，不支持在线学习；&lt;/li&gt;
&lt;li&gt;stochastic gradient descent：即随机梯度下降，收敛性能不太好，可能 hit 不到最优点；&lt;/li&gt;
&lt;li&gt;mini-batch gradient descent：即小批量梯度下降，通常所说的 batch_size 指的就是这个批量的大小。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="batch_size" scheme="http://blog.keeplearning.group/tags/batch-size/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（二）</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch2/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch2/</id>
    <published>2018-11-14T10:26:00.000Z</published>
    <updated>2018-11-15T06:37:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 在 Tensor 上的所有操作，<code>autograd</code> 都能为它们自动提供微分，避免了手动计算导数的复杂过程，只需要设置 <code>tensor.requires_grad=True</code> 即可。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上一步等价于</span></span><br><span class="line"><span class="comment"># x = t.ones(2, 2)</span></span><br><span class="line"><span class="comment"># x.requires_grad = True</span></span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">y = x.sum()</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line">print(y.grad_fn)</span><br><span class="line"></span><br><span class="line">y.backward()   <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fx7rvm1w6sj214e05ojrw.jpg" alt=""></p><p><code>grad</code> 在反向传播过程中是累加的，每一次运行反向传播，梯度都会累加之前的梯度，因此反向传播之前需要把梯度清零。</p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下划线结束的函数是 inplace 操作，会修改自身的值</span></span><br><span class="line">x.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7s1w5mz3j213y05imx9.jpg" alt=""><br><a id="more"></a><br>2 . <code>torch.nn</code> 是专门为神经网络设计的模块化接口，<code>nn</code> 构建于 Autograd 之上，可用来定义和运行网络，<code>nn.Module</code> 是 <code>nn</code> 中最重要的类，可以把它看成是一个网络的封装，包含网络定义以及 <code>forward</code> 方法，调用 <code>forward(input)</code> 方法，可返回前向传播的结果，下面是 LeNet 的实现。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># nn.Module 子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="comment"># 下式等价于 nn.Module.__init__(self)</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 卷积层 1 表示输入图片为单通道，6 表示输出通道数，5 表示卷积核为 5*5</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 卷积层</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 仿射层/全连接层， y=Wx+b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 卷积 --&gt; 激活 --&gt; 池化</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># reshape, -1 表示自适应</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fx7swean9dj214e06jdgl.jpg" alt=""></p><p>只要在 <code>nn.Module</code> 的子类中定义了 <code>forward</code> 函数，<code>backward</code> 函数就会自动被实现（利用 <code>autograd</code>），网络的可学习参数通过 <code>net.parameters()</code> 返回，<code>net.named_parameters</code> 可同时返回可学习的参数和名称。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameters <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, <span class="string">':'</span>, parameters.size())</span><br></pre></td></tr></table></figure><p>Output:</p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7t8t7uitj21470a3gn3.jpg" alt=""></p><p><code>forward</code> 函数的输入和输出都是 Tensor。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out.size())</span><br></pre></td></tr></table></figure><p>Output:</p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx7tfza8rlj214l01amx0.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()   <span class="comment"># 所有参数的梯度清零</span></span><br><span class="line">out.backward(t.ones(<span class="number">1</span>, <span class="number">10</span>))    <span class="comment"># 反向传播</span></span><br></pre></td></tr></table></figure><p><code>torch.nn</code> 只支持 mini-batches，不支持一次只输入一个样本，即一次必须是一个 batch，但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code> 将 batch_size 设为 1。</p><p>3 . <code>nn</code> 实现了神经网络中大多数的损失函数，例如 <code>nn.MSELoss</code> 用来计算均方误差，<code>nn.CrossEntropyLoss</code> 用来计算交叉熵损失。</p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = t.arange(<span class="number">0</span>, <span class="number">10</span>).view(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target.float())</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><p>Output:</p><p><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx7uw9frqoj2149014aa1.jpg" alt=""></p><p><code>torch.LongTensor</code>   —-&gt;  <code>torch.FloatTensor</code>: <code>tensor.float()</code>  </p><p> <code>torch.FloatTensor</code>  —-&gt; <code>torch.LongTensor</code>: <code>tensor.long()</code>  </p><p> Input:  </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行 backward，观察调用之前和调用之后的 grad</span></span><br><span class="line">net.zero_grad()    <span class="comment"># 把 net 中所有可学习参数的梯度清零</span></span><br><span class="line">print(<span class="string">'反向传播之前 conv1.bias 的梯度: '</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line">loss.backward()</span><br><span class="line">print(<span class="string">'反向传播之后 conv1.bias 的梯度: '</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><p> Output:</p><p> <img src="http://wx2.sinaimg.cn/mw690/79225320ly1fx7v5aj09kj214c03vjrw.jpg" alt=""></p><p>4 . 在反向传播计算完所有参数的梯度之后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降（SGD）的更新策略手动实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.data * learning_rate)</span><br></pre></td></tr></table></figure><p><code>torch.optim</code> 中实现了深度学习中绝大多数的优化方法，例如 RMSProp、Adam、SGD 等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建一个优化器，指定要调整的参数和学习率</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练过程中</span></span><br><span class="line"><span class="comment"># 先梯度清零（与 net.zero_grad() 效果一样）</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target.float())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 在 Tensor 上的所有操作，&lt;code&gt;autograd&lt;/code&gt; 都能为它们自动提供微分，避免了手动计算导数的复杂过程，只需要设置 &lt;code&gt;tensor.requires_grad=True&lt;/code&gt; 即可。&lt;/p&gt;
&lt;p&gt;Input:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.ones(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 上一步等价于&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# x = t.ones(2, 2)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# x.requires_grad = True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y = x.sum()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(y.grad_fn)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y.backward()   &lt;span class=&quot;comment&quot;&gt;# 反向传播计算梯度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://wx1.sinaimg.cn/mw690/79225320gy1fx7rvm1w6sj214e05ojrw.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;grad&lt;/code&gt; 在反向传播过程中是累加的，每一次运行反向传播，梯度都会累加之前的梯度，因此反向传播之前需要把梯度清零。&lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;y.backward()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y.backward()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 以下划线结束的函数是 inplace 操作，会修改自身的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x.grad.data.zero_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y.backward()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://wx4.sinaimg.cn/mw690/79225320gy1fx7s1w5mz3j213y05imx9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（一）</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch1/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch1/</id>
    <published>2018-11-14T07:52:00.000Z</published>
    <updated>2018-11-15T06:37:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>Tensor</code> 可以认为是一个高维数组，可以使用 GPU 进行加速。   </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">x = t.Tensor(<span class="number">5</span>, <span class="number">3</span>)    <span class="comment"># 只分配了空间，未初始化</span></span><br><span class="line">x = t.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># 使用 [0, 1] 均匀分布随机初始化二维数组</span></span><br><span class="line">x = t.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># 查看 x 的形状</span></span><br><span class="line">print(x.size())</span><br><span class="line"><span class="comment"># 查看 x 中列的个数，两种写法等价</span></span><br><span class="line">print(x.size()[<span class="number">1</span>])</span><br><span class="line">print(x.size(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7j8w2u2qj20z9092wf3.jpg" alt="运行结果"><br><code>torch.Size</code> 是 tuple 对象的子类，因此它支持 tuple 的所有操作，如 <code>x.size()[0]</code> 等。<br><a id="more"></a></p><p>2 . 加法的三种写法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = t.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)   <span class="comment"># 加法的第一种写法</span></span><br><span class="line">print(t.add(x, y))   <span class="comment"># 加法的第二种写法</span></span><br><span class="line"><span class="comment"># 加法的第三种写法：指定加法结果的输出目标为 result</span></span><br><span class="line">result = t.Tensor(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">t.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'最初 y'</span>)</span><br><span class="line">print(y)</span><br><span class="line"><span class="comment"># 普通加法，不改变 y 的内容</span></span><br><span class="line">y.add(x)</span><br><span class="line">print(y)</span><br><span class="line"><span class="comment"># inplace 加法，改变 y 的内容</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>函数名后面带下划线 <code>_</code> 的函数会改变 Tensor 本身。</p><p>3 . Tensor 与 Numpy。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[:, <span class="number">1</span>]   <span class="comment"># 选取 x 的第一列所有内容，与 Numpy 相似</span></span><br></pre></td></tr></table></figure><p>Tensor 和 Numpy 的数组之间的互操作非常容易且快速，对于 Tensor 不支持的操作，可以先转换为 Numpy 数组处理，之后再转换回 Tensor。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensor <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = t.ones(<span class="number">5</span>)   <span class="comment"># 新建一个全 1 的 Tensor</span></span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy()   <span class="comment"># Tensor ----&gt; Numpy</span></span><br><span class="line">print(b)</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = t.from_numpy(a)   <span class="comment"># Numpy ----&gt; Tensor</span></span><br><span class="line">print(b)</span><br><span class="line"><span class="comment"># Tensor 与 Numpy 共享内存</span></span><br><span class="line">b.add_(<span class="number">1</span>)    <span class="comment"># 以 _ 结尾的函数会修改 Tensor 自身</span></span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7kkcurzdj214m05mjrm.jpg" alt=""><br>Tensor 和 Numpy 对象共享内存，如果其中一个变了，另外一个也会随之改变。</p><p>4 . 如果想获取某一个元素的值，可以使用 <code>scalar.item</code>，直接 <code>tensor[idx]</code> 得到的还是一个 tensor，一个 0-dim 的 tensor，一般称为 scalar。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scalar = b[<span class="number">0</span>]</span><br><span class="line">print(scalar)</span><br><span class="line">print(scalar.size())  <span class="comment"># 0-dim</span></span><br><span class="line">print(scalar.item())</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx2.sinaimg.cn/mw690/79225320ly1fx7m1ulcjhj213s02uaa3.jpg" alt="">  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 注意和 scalar 的区别</span></span><br><span class="line">tensor = t.tensor([<span class="number">2</span>])</span><br><span class="line">print(tensor)</span><br><span class="line">print(tensor.size())</span><br><span class="line"><span class="comment"># 只有一个元素的 tensor 也可以调用 tensor.item()</span></span><br><span class="line">print(tensor.item())</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx7misoww4j213x02yq2v.jpg" alt=""></p><p>5 . <code>t.tensor()</code> 总会进行数据拷贝，新 tensor 和原来的数据不共享内存，如果想要共享内存的话，建议使用 <code>torch.from_numpy()</code> 或者 <code>tensor.detach()</code> 来新建一个 tensor，二者共享内存。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor = t.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">old_tensor = tensor</span><br><span class="line">new_tensor = t.tensor(old_tensor)</span><br><span class="line">new_tensor[<span class="number">0</span>] = <span class="number">1111</span></span><br><span class="line">print(old_tensor)</span><br><span class="line">print(new_tensor)</span><br><span class="line">new_tensor = old_tensor.detach()</span><br><span class="line">new_tensor[<span class="number">0</span>] = <span class="number">1111</span></span><br><span class="line">print(old_tensor)</span><br><span class="line">print(new_tensor)</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx7mtib98oj214103ojrh.jpg" alt=""></p><p>6 . Tensor 可以通过 <code>.cuda</code> 方法转换为 GPU 的 Tensor，从而享受 GPU 带来的加速运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在不支持 CUDA 的机器上，下一步还是在 CPU 上运行</span></span><br><span class="line">device = t.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> t.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">x = x.to(device)</span><br><span class="line">y = y.to(device)</span><br><span class="line">z = x + y</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;Tensor&lt;/code&gt; 可以认为是一个高维数组，可以使用 GPU 进行加速。   &lt;/p&gt;
&lt;p&gt;Input:&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.Tensor(&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)    &lt;span class=&quot;comment&quot;&gt;# 只分配了空间，未初始化&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.Tensor([[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 使用 [0, 1] 均匀分布随机初始化二维数组&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.rand(&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 查看 x 的形状&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.size())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 查看 x 中列的个数，两种写法等价&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.size()[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.size(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Output:&lt;br&gt;&lt;img src=&quot;http://wx4.sinaimg.cn/mw690/79225320gy1fx7j8w2u2qj20z9092wf3.jpg&quot; alt=&quot;运行结果&quot;&gt;&lt;br&gt;&lt;code&gt;torch.Size&lt;/code&gt; 是 tuple 对象的子类，因此它支持 tuple 的所有操作，如 &lt;code&gt;x.size()[0]&lt;/code&gt; 等。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>自学编程那些事儿</title>
    <link href="http://blog.keeplearning.group/2017/07/31/2017/07-31-learn-program/"/>
    <id>http://blog.keeplearning.group/2017/07/31/2017/07-31-learn-program/</id>
    <published>2017-07-31T11:47:00.000Z</published>
    <updated>2018-11-07T08:29:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>我本人是从机械专业转过来学习编程的，在这个过程中我大概跟大多数半路出家的人一样，迷茫过，纠结过，努力过，也怅然若失过，同时也有在意想不到的拐角处收获到额外的惊喜。这一路不能说顺顺利利，但也算是跌跌撞撞地走过来了。虽然我的水平还远不至于到了能给别人指点一二的地步，不过我倒不介意把我认为最重要的几点浅薄的看法分享给大家。</p><a id="more"></a><h3 id="选择好一个方向很重要"><a href="#选择好一个方向很重要" class="headerlink" title="选择好一个方向很重要"></a>选择好一个方向很重要</h3><p>我相信大多数想入门编程并且未来想在 IT 领域中找到一份工作的人都听过几个热门的词，一会儿是人工智能（AI），一会儿是大数据，一会儿是 Android 开发 iOS 开发之类的，不然就是运维，或者前端、后端之类的。在最开始选择进入 IT 领域的时候，你必须花时间在这些五花八门的方向中找出一个你最感兴趣并且认为自己能够持之以恒地坚持学下去的方向。你可以向有经验的人了解这些方向都是干嘛的，找不到人的话也可以通过互联网求助，譬如知乎等等各大网络平台，搞清楚了这些方向是干嘛的，接下来就是好好规划自己的学习路线，第一步做什么，第二步做什么等等等等，现在网络资源这么发达，想找到自己的学习路线并不难，况且还有慕课网、coursera、edx 等在线教育平台可以利用。总之，利用一切你可以利用到的手段，选择一个你真正感兴趣的方向，即便为了钱而感兴趣也行，只要你认为碰到困难的时候你能坚持下去。  </p><h3 id="几个方向都精通只是锦上添花"><a href="#几个方向都精通只是锦上添花" class="headerlink" title="几个方向都精通只是锦上添花"></a>几个方向都精通只是锦上添花</h3><p>在寻找学习方向的时候，可能很多人会觉得自己好像对好几个方向都非常感兴趣，这个时候怎么办呢？我想对你说的是，虽然方向很多，但是在最开始学习的时候，你必须选择并且仅仅只能选择其中的一个，付出努力持之以恒地把它学好即可，其他的，无论你再有兴趣，也应该暂时地舍弃掉，不要去管它。我这么说主要是基于以下两点：</p><p>第一，你在真正找工作的时候，是凭借你的一技之长去投岗位的，请注意，一定是一技之长，每个公司列出那么多岗位给应聘者，并没有要求应聘者同时兼顾好几个方向的技能，相对于什么都知道一点但是什么都不够深入的人，他们一定是要那些在某一个方面有所长能够给他解决这一个方面的实际问题的人，至于其他方面，他们可以再去招聘那些在其他方面有所长的人，而不需要一个人兼顾这么多，大多数时候，一个人来做反而做不好；</p><p>第二，人每天的精力是有限的，无论你用它做了什么，每天用完即没有，我曾经对自己专注在一件事情上的时间做了一个粗略的统计，我指的是严格专注在一件事情之上，减去那些喝水、散步、走神的时间，一天顶多顶多只有 6 个小时，而且每周并不能做到每天都能有这么高的效率，通常工作日五天能有两天有这个效率就非常不错了，而且人会感觉非常疲劳。一天 24 小时，全身心铺在一件事情上的时间只能有 6 个小时，换而言之，只有 1/4 的有效工作时间，事实上 5 天的工作日只有顶多只有两天有这么高的效率，这个数字说出去是很吓人的，但是对不起，这真的就是事实，我不知道大家统计过没有，虽然每个人的精力略有差别，但是我相信这个数字的上下浮动不会太大。而随着你在某一个技术方向深入程度的增加，你一定会觉得你不会的东西需要弄明白的东西太多了（当然你也许会觉得有些东西我不需要弄得那么明白和熟练，用到的时候查一查会用就行，下文我会说明为什么这么想是有问题的），你的精力有限，想要弄明白这么多的东西，进一步做到让你的一技之长足够长，真的很难，这是我的切身体会，你真的不太可能让你的一技之长足够长的同时又能兼顾好其他方面的技术。</p><p>事实上，我见过在某一个技术方向足够熟练之后，由于个人的兴趣或者个人的需要在其他方面也做得比较好的人，但是，我还真没有见过同时一下子从很多个技术方向出发最后都能够做得非常非常好的人，我相信这种人即便有，也是在少数中的少数吧。先做好自己的一技之长，还有一个原因是，有很多技术方向是有共通点的，你做好了这个方向的一技之长，将来有需要精通其他方向的技术之后，触类旁通会相对容易一些。行业里所说的“全栈工程师”，这大多是环境要求，后来技术背景不断更换之后所带来的一个结果，而不是最初出发的一个目的。总而言之一句话，几个方向都精通只是锦上添花，但这并不要求是必须的，最初学习的时候，你最好只选择一个方向进行突破。  </p><h3 id="专注、持续、有效的投入"><a href="#专注、持续、有效的投入" class="headerlink" title="专注、持续、有效的投入"></a>专注、持续、有效的投入</h3><p>在最初选择技术方向的时候，我刚说了最好专注在一个方向上，事实上不仅如此，在你严格按照学习路线图进行学习的时候，你接触到某一方面的知识，譬如 Linux shell 脚本编程，那么，你也需要在短期内只专注 shell 怎么写，否则，一个基础没打牢固就跑去弄别的，最后的结果只能是捡了芝麻丢了西瓜，狗熊掰棒子而已。明确学习路线之后，需要一步步按照学习路线的规划，稳扎稳打地一个个把每一步要求的知识过关。</p><p>一个技术岗位的价值，首先取决于这个技术方向本身，如果你选择的技术方向是一个急需紧缺的类型，那么这个技术方向肯定价值会比较高（说的俗一点，就是钱多）；其次取决于选择这个技术岗位的人本身的不可替代性，也就是说，一个技术岗位的人能够把工作做得越出色，别人越难以取代，这个人的价值也就越大。那么，怎样变得越来越不可替代呢？或者，换句话说，怎样把工作做得比大多数人都优秀呢？那就需要在很多别人难以下功夫的地方多下功夫。譬如很多重要的东西，别人由于畏难而退却而你搞明白了，别人忽略的一些重要的细枝末节你掌握了，等等等等，就是静下心来把别人啃不好的硬骨头给啃下来，并且这些硬骨头还是能有效提高你能力的知识点。这也就是我上文说的需要把一些别人不在意的同时又很重要的一些东西弄得比较明白的原因，因为这是你与其他人的差别所在，你做的越好，你的不可替代性就会越强，从而你的价值就会越大。做到这些，就需要专注、持续、有效的投入了，所谓专注就是一心一意做好一件事情，do one thing and do one thing well，所谓持续就是不能三天打鱼两天晒网，大家估计都有体验，如果学习一门技术，刚入门然后断了一段时间，再次投入进去感觉很多东西特别陌生，捡起来重新开始花的时间代价会比较大，长此以往会陷入低水平重复的怪圈，所以我这里强调持续性，打铁要趁热，不要轻易被其他的事情所打断，所谓有效的投入指的是不能低水平重复，每一次的努力都要给自己带来实实在在的成长。</p><p>一万小时理论被认为是成为某个领域专家所花费时间的及格线，也是其必要条件，但如果只是低水平地做一些简单的重复工作的话，那再怎么样也成不了杰出专家，我们需要啃一些能有效地提高我们水平的硬骨头。顺便说一下一万小时，我算了一下，如果每天 6 个小时的投入的话，10000 小时的花费大概需要 1667 天，大概需要 4.5 年的时间，这还不包括周末和各种假日。所以，在一个领域内成为专家级的水平需要付出多少大概可想而知了，沉下心来戒骄戒躁好好努力才是王道。  </p><h3 id="系统的知识体系很重要"><a href="#系统的知识体系很重要" class="headerlink" title="系统的知识体系很重要"></a>系统的知识体系很重要</h3><p>做事情专业还是业余，成系统的知识在其中起着非常重要的作用。那么什么是专业呢？这不仅仅是面对已知的问题你能提供有效的解决方案，还有就是针对未知的问题，你能通过有效的规范的步骤，一步步地找出问题症结所在，一点点地让问题浮出水面，或者至少，你能提供一些有利于问题解决的有价值的参考出来。而业余的人，解决问题有时候就跟碰运气一样，有时候能得到解决，有时候却不怎么灵。在一个领域内比较专业的人，他的努力所带来的后果在很大程度上是可以预期的，究其原因，就是其背后有系统的知识体系作为支撑。拿算法来说吧，很多人觉得不重要，怎么说呢？重不重要的结论我先不给出来。我只想说，你认真地掌握了它，再来谈重不重要，得出的结论会更靠谱。实际上，在大多数编程场景中，很少让你写出具体的算法，但算法思想的应用还是比较多的，受过良好算法训练的人，拿来一些实现同样功能代码，他能很快分辨出代码是写的好还是烂，这就是专业与不专业的区别，而对于算法思想的领悟，又需要一定的计算机数学基础为铺垫，这背后就是一整套的系统知识体系。所以说，系统的知识体系，直接关系到你专业还是不专业，而你专业还是不专业，背后又关系到你是不是不可取代的，你的价值是多少。  </p><h3 id="到底要不要自学编程"><a href="#到底要不要自学编程" class="headerlink" title="到底要不要自学编程"></a>到底要不要自学编程</h3><p>经常听到有人问，我不是计算机科班出身的，可是我对编程很感兴趣，到底要不要学编程？或者，我现在年纪也不小了，二十四五岁的高龄了（我笑而不语），学编程还来得及吗？等等诸如此类的要不要学编程问题。我首先是不太愿意回答这样子的问题，原因是我觉得这些人中有相当一部分在提出这个问题的时候心里其实早就有答案了，就是觉得自己可能不太适合，而且岁数比较大等等各种能够说服自己放弃的理由，即便被我说服了，一个脑袋发热一头扎了进去，一碰到困难马上就觉得自己不合适了，想放弃了。对此，我只能说，你不仅仅只是不适合学习编程，你干其他的事情估计也不太可能干得好。我总认为，只要自己努力和积极向上，那么无论处于什么样糟糕的境地，都能找到自己的出路，没有这个自信人生简直都不要活了。有了这个自信的话，对于很多自己内心真正想从事的一些工作，只要是积极乐观的，又何妨一搏呢？梁漱溟先生不也是自学成才，当上了北大教授吗？一般人像他那样子才中学毕业，在这条路上估计早就自我放弃了，他不但通过自学成为了北大教授，而且还教出了像冯友兰、朱自清等一批优秀的学生。毛泽东主席在年轻的时候就写下了这样子的诗：自信人生两百年，会当水击三千里。当然了，不是一般人都有伟人一般的豪迈意气，但是对于我们每个人自己的人生，豪迈豪迈的勇气我们还是应该有的。总之，大胆去选择自己的梦想，只要是积极向上的，尽可努力去拼搏，人立于天地之间，无论现实怎么糟糕，都能找到自己的出路，反正，我就这么自信。  </p><p>最后，祝学习愉快。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我本人是从机械专业转过来学习编程的，在这个过程中我大概跟大多数半路出家的人一样，迷茫过，纠结过，努力过，也怅然若失过，同时也有在意想不到的拐角处收获到额外的惊喜。这一路不能说顺顺利利，但也算是跌跌撞撞地走过来了。虽然我的水平还远不至于到了能给别人指点一二的地步，不过我倒不介意把我认为最重要的几点浅薄的看法分享给大家。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>说说人工智能、大数据、医疗和教育（三）</title>
    <link href="http://blog.keeplearning.group/2017/07/23/2017/07-23-ai-bigdata/"/>
    <id>http://blog.keeplearning.group/2017/07/23/2017/07-23-ai-bigdata/</id>
    <published>2017-07-23T02:50:00.000Z</published>
    <updated>2018-11-07T08:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>在正式开始今天的主题之前，我想跟大家说的是，我们真的处在了一个非常非常好的时代，很多行业里的人士普遍认为，中国仍然有很好的长期发展的机遇，从 1990 年开始的 40 年间，中国会超越美国成为世界第一大经济体，并且，如果政治稳定的话，之后还会伴随着这个浪潮的惯性继续向前发展很长一段时间。而我们这帮 90 后们，算算时间点的话，恰恰会是这个高速发展时代的中坚力量。我们运气很好地碰在了一个高速发展的国家里，并且，又同样运气很好地碰在了人类历史上第四次技术革命发展的转折点 —— 从信息时代向智能时代转变的关键点上。《必然》一书的作者凯文凯利认为，以后几十年里人类生活离不开的产品还远远没有被发明出来。其实一个人一生中很难碰到一次技术革命的浪潮，而我们居然在一个青春年华的合适年纪里神奇地遇上了，这是我们的幸运。对于这一波浪潮而言，我们可以将其看成是每个人一生中只能玩一次的游戏，错过了就是错过了，再没机会，到底是眼巴巴看着还是投身其中闯荡一番？我相信很多人心中其实都有了答案。那么，时代的机遇究竟在哪里呢？这是今天的文章重点谈的内容。首先，既然我们是处于第四次技术革命的关键转折点上，我们不妨来回顾一下之前的三次技术革命里都发生了什么。</p><a id="more"></a><p>人类历史上的第一次工业革命发生在 18 世纪末的英国，它是以蒸汽机的发明为标志的，因此也叫做蒸汽机的革命，我们都知道蒸汽机的发明者是瓦特。但是实际上，在瓦特之前蒸汽机就有了，瓦特只是和当时著名的工厂主马修博尔顿（关于马修博尔顿见<a href="https://zh.wikipedia.org/wiki/%E9%A9%AC%E4%BF%AE%C2%B7%E5%8D%9A%E5%B0%94%E9%A1%BF" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E9%A9%AC%E4%BF%AE%C2%B7%E5%8D%9A%E5%B0%94%E9%A1%BF</a>） 一起合作，改良了传统的效率低下的纽科门蒸汽机，在此基础之上发明了一种万用蒸汽机。瓦特的发明奠定了第一次工业革命的基础，并且， 他还发展出马力的概念以及以他名字命名的功率的国际标准单位——瓦特。最初的纽科门蒸汽机只是供英国的一些矿井使用的，不但效率低下，而且适用性很差。而瓦特发明的万用蒸汽机可以适用于诸如纺织、陶瓷等等许多在之前大部分工作只能通过人力操作的行业。很多原来存在的产业，只要运用上蒸汽机，就会带来生产效率的极大提升。第一次工业革命给人类社会带来的积极影响，可以用马克思的一句话来说明：“资产阶级在其不到 100 年的阶级统治中所创造的生产力，比过去一切时代创造的全部生产力还要多、还要大”。当然我们这里讲的是积极的一方面。消极的一方面是，在纺织、瓷器等等蒸汽机能够渗透到的领域，一些传统的小作坊被更有效率、更标准化、生产出的产品质量更好的蒸汽机所取代，之前的经济结构被摧毁，很多人纷纷破产，从中产阶级一下子沦为了赤贫。拥有生产资料的工厂主们为了便于竞争，就雇佣一些低工资的童工，或者随意延长工人的劳动时间。也就在那个时代，英国发生了空前绝后工人运动，催生了马克思主义。技术的进步给当时的英国社会带来了很大的社会动荡，最初诅咒它的人比拥抱它的人多很多。最初享受到技术革命带来的好处的，只是瓦特和像博尔顿这样懂得运用蒸汽机技术的工厂主，那个时候的英国贫富分化非常严重。</p><p>那么，英国是怎么消除蒸汽机革命所带来的负面影响的呢？</p><p>那就是开拓全球殖民地，推行自由贸易，进行资本输出。实际上我们也可以看到，那个时候的很多次战争，就中国而言，总是动不动让中国开放这个港口那个港口什么的，这就是推行自由贸易的结果。实际上通过这种手段，也让第一次工业革命的成果得以全球化。    </p><p>第二次工业革命是电的革命，同蒸汽机的革命一样，电的使用在原来存在的很多领域中也造成了革命性的冲击，带来了生产效率的极大提升。并且，最初受益的也只是少部分懂得如何使用电并且能够拥有电的人，新的技术的应用导致了大量的失业潮和破产潮，那个时候的很多代表性的人物，如通用电气（GE）公司的创始人爱迪生、AT&amp;T的创始人亚历山大贝尔、福特公司的创始人亨利福特、奔驰汽车的创始人卡尔本茨（德国），不少到现在也依然是很多创业家心中的偶像。实际上，那个时候，在美国的贫富分化程度达到了北美殖民以来的最高点，不少激进的工人运动也发生在那个时期，石油大王洛克菲勒聚集的财富占了全美国的 1%，说个题外话，很多人对洛克菲勒的认识可能只是知道他很有钱并且做慈善出资建立了北京协和医院和协和医学院，当年协和医院的奠基的时候，洛克菲勒的儿子小洛克菲勒，坐了个把月的轮船，亲自来到了北京参加了这个奠基仪式，当然现在的北京协和医院已经跟洛克菲勒家族没啥关系了（早就充公了）。美国人抵消第二次工业革命的消极影响是通过开发西部广袤的还未被开发出来的处女地。而德国人就没那么幸运了，空前的社会矛盾导致他们把希特勒代表的纳粹主义分子推上了台（所以中国的制度好还是国外的制度好？真的不好讲）。实际上，美国19世纪中期南北战争北方战胜，扫除了发展资本主义的障碍——奴隶制，促使美国在第二次工业革命中采用新技术，用了大约 30 年的时间一举超过了英国，成为了世界第一大经济体。  </p><p>第三次技术革命是二战之后以计算机技术的兴起为基础发展起来的信息革命。实际上在过去 30 年的时间里，真正上来说受益于信息时代的只有美国和中国，这两个国家贡献了全球一半以上的 GDP 增长，对于美国来说不需要讲太多，诞生了一大批信息时代的明星公司：Google、Apple、Microsoft等等等等（了解一下 Google 的话可以看看我公众号里之前的一篇文章《Google 传奇》），对于这些公司的创始人来说，他们都在自己年富力强的时候幸运地赶上了信息革命的大潮，与我们现在很多的 90 后赶上智能革命的浪潮一样（偷笑中）。不过对于美国而言，信息革命技术的进步造成的社会问题也是显而易见的，美国人的失业率比较高，有很多游手好闲无所事事吃低保的人，因为他们的工作机会被新的技术进步所取代了。譬如特斯拉汽车公司，在它的汽车装配间里全部都是一个个软件技术操纵的机械手臂，几乎没有工人。为此特斯拉的门前总能招来一些抗议者，他们抗议特斯拉不给他们提供工作机会而用一个个的机械手臂去代替他们，不过这个大的趋势真的是不可逆转不以人的意志为转移。去年美国总统大选，特朗普一直在说要解决美国人的就业问题，因为很多很多人没有了工作机会。与此相比，同为贡献 GDP 增长最多的两个国家，中国则要幸运得多，这是因为中国的基础太薄弱了，从 1979 年改革开放起，中国用了 30 多年的时间，走完了西方国家 200 多年的工业化道路，并且同时进行了信息革命，与整个时代一起步入了信息时代的末期智能时代的初期。在这期间的人力资源的消耗是非常大的，所以在中国基本上没有看到像美国那样子的情况发生。不过在智能时代中国可就没那么幸运了，所以诸位真的要未雨绸缪啊。实际上现在就可以看出一些迹象了，大家都知道中国是世界工厂，譬如富士康，像 iPhone 手机之类的电子产品大都是在富士康生产和组装的，这些精密的电子产品对于组装工人的技术性要求还是比较高的，但是，有消息称，富士康正在加紧研制智能机器人，将这些智能机器人投放到生产产线上，取代现在的技工的工作。    </p><p>其实不仅仅只是这些取代，我昨天的文章里也谈过，像医生之类的工作，也有可能被机器大部分取代掉，并且昨天的文章里也说了详尽的理由。其实不止医生啊，像记者编辑，有些人可能不知道，实际上在美国，《纽约时报》之类的报纸，很多的文章都是机器写的，通过大量的文章训练出一个模板，然后把新闻信息输入进去，机器就把文章写出来了。未来不止医生、记者之类的行业，实际上将现有的很多很多行业，融于智能时代的思维来思考的话，很多很多的东西都将产生变化，这些变化会像历史上几次技术革命所带来的影响一样。  </p><p>从以上历史的回顾之中我们可以知道，新的技术进步所带来的生产力的提升，会让很多猝不及防的人因此丢了工作，刚开始受益的也只是很少的那一部分掌握了新技术并且懂得如何应用新技术的群体。与此同时我们也应该看到，新的技术进步在取代原有的工作的同时，一定会诞生一些新的职业，现在的很多职业，譬如程序员等，在以前是不存在的。那么为什么历史上新的职业旧时代的人却做不了呢？因为学习成本高并且难以适应，学习一门新的技术，不仅仅是学习这些东西本身而已，还有对于过去职业习惯所养成的一些思维定势的改变，而这，对于过去从事一项熟悉的工作几十年的人来说，是非常困难的。这就涉及到我昨天说的今天想讲的一个话题了 —— 教育。  </p><p>其实在新的时代背景下，大家都找到了未来教育形式的一个方向，即互联网教育。像 Coursera、edx、网易公开课等在线教育平台，把一些世界名校的课程放在了互联网上，让大家都可以便捷地享受到以前只有少数人才能得到的知识服务，那么，我有一个疑问就是，这些宝贵的东西放在互联网上，为什么得不到爆炸式的反馈效果呢？选择在线学习的人依然是非常非常少的，原因在哪儿呢？还有，智能时代以后取代的是现在很多人所从事的一些智能化的工作，譬如算算表单的会计呀，画画图纸的设计师呀，等等这些工作在未来一定程度上甚至全部都可以被机器所取代掉，由此看来现在的教育教授的很多东西在未来是没有用的，那么什么样子的教育可以在一定程度上减轻未来随之到来的失业潮呢？以后究竟需要什么样的教育呢？而为了应付未来教育上的需求，我们现在应该做好哪些事情呢？对于这些问题，我没有特别好的答案，只有一些粗浅的思考分享给大家，欢迎对这些问题有见解的人通过文末的方式来和我交流。我觉得，如果真正解决了我上面所提的那些问题，那么未来教育的问题，也肯定就解决了。  </p><p>为什么互联网上放了那么多好的教育资源，但是选择在线教育的人还是很少？我觉得其中有一方面的问题就是教育的形式的问题。我们从小习惯了课堂上互动式的教育，也习惯了同学之间知识的交流，如果能在在线教育的形式中融于课堂仪式感和交流互动的元素，譬如利用 VR 技术营造一种课堂的氛围等等，那么在线教育的形式一定能够被越来越多的人所接受。  </p><p>什么样子的教育可以减轻未来随之而来的失业潮？这个问题的答案我不知道，更诚实的一点的说法是，我有一点看法，但是由于过于粗浅经不起斟酌，所以也就不说出来了。不过，我觉得过去之所以会出现那么大的失业潮，部分原因是因为不能教授旧时代的人新时代的思维和新时代的技能，其中有一个很重要的原因之一是，那个时候的教育被局限的很厉害，只能通过大学里校园教育的方式获得，而对于未来，互联网触手可及，这个限制可以说不存在了，所以，我觉得可以用互联网教育的方式更快地传播新的技能以减轻失业潮造成的影响。  </p><p>未来需要什么样子的教育呢？机器智能所取代的是人现在所从事的很多智能化的工作，也就是说，现在人所从事的很多技能教育将来都不需要了。既然很多技能教育未来不需要，那么我认为其实未来教育教人的东西，应该是更让人能获得快乐和精神享受的东西，譬如诗词歌赋、绘画艺术、历史哲学等等，人的双手被解放了之后，很多工作完全不需要人来做了，所以人会更想追求精神层面的快乐。  </p><p>以上就是我对教育方面的一些粗浅的认识，欢迎持有观点的朋友通过下面的方式来和我交流。我一直觉得，人的生活一定要健康、幸福和快乐，并且我乐于用自己的所学给更多的人带来健康、幸福和快乐，所以其实从第二篇文章开始，我主要谈了未来智能社会里医疗和教育的话题，因为，医疗能带给人健康，而教育，能使人快乐。我们每天所写的并不仅仅只是一行行有趣的代码而已，而且这些东西，在未来，能让更多的人健康、幸福和快乐起来。我想在未来，在医疗和教育领域，我要么成为一个开拓者，要么加入别人成为追随者，因为这两样东西，我感觉是渗透在我的灵魂深处让我真正想去做的事业。希望在智能时代我们这帮 90 后们都能把握自己的机遇，找到自己一生的事业，无负于一个伟大的时代对我们最有价值的馈赠吧！  </p><p>谢谢耐心看完这三篇文章的人，对于这个话题，以这三篇文章的篇幅，到此告一段落。以后有机会我们可以接着聊。  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在正式开始今天的主题之前，我想跟大家说的是，我们真的处在了一个非常非常好的时代，很多行业里的人士普遍认为，中国仍然有很好的长期发展的机遇，从 1990 年开始的 40 年间，中国会超越美国成为世界第一大经济体，并且，如果政治稳定的话，之后还会伴随着这个浪潮的惯性继续向前发展很长一段时间。而我们这帮 90 后们，算算时间点的话，恰恰会是这个高速发展时代的中坚力量。我们运气很好地碰在了一个高速发展的国家里，并且，又同样运气很好地碰在了人类历史上第四次技术革命发展的转折点 —— 从信息时代向智能时代转变的关键点上。《必然》一书的作者凯文凯利认为，以后几十年里人类生活离不开的产品还远远没有被发明出来。其实一个人一生中很难碰到一次技术革命的浪潮，而我们居然在一个青春年华的合适年纪里神奇地遇上了，这是我们的幸运。对于这一波浪潮而言，我们可以将其看成是每个人一生中只能玩一次的游戏，错过了就是错过了，再没机会，到底是眼巴巴看着还是投身其中闯荡一番？我相信很多人心中其实都有了答案。那么，时代的机遇究竟在哪里呢？这是今天的文章重点谈的内容。首先，既然我们是处于第四次技术革命的关键转折点上，我们不妨来回顾一下之前的三次技术革命里都发生了什么。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>说说人工智能、大数据、医疗和教育（二）</title>
    <link href="http://blog.keeplearning.group/2017/07/22/2017/07-22-ai-bigdata/"/>
    <id>http://blog.keeplearning.group/2017/07/22/2017/07-22-ai-bigdata/</id>
    <published>2017-07-22T02:50:00.000Z</published>
    <updated>2018-11-07T08:32:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>在昨天的文章中我们详细说明了什么是机器智能，用数据驱动获取机器智能的方法，谈了机器智能的一个必要因素就是大数据，并且讲了大数据对于人类生活的预测指导作用，今天我们在此基础之上，主要来谈谈医疗的内容，基于一篇文章的篇幅所限，明天我们再来谈教育。  </p><p>当今医疗领域所面对的问题主要有三个方面：第一，看病贵；第二，医疗资源分配不均衡，好的医疗设备和技术精湛的名医大都集中在经济比较发达的地区，经济不发达地区的人面临的不仅仅是看不起病的问题，同时还有生了重病找不到好的医生的问题；第三，很多疾病，譬如癌症，人类仍然没有找到可靠的能够使其得到稳定治愈的方法。其实针对这三个方面的问题，大数据和机器智能的发展都能够提供一个理论上行得通的解决方案。下面我们一一来谈。</p><a id="more"></a><p>首先是看病贵的问题。那么为什么看病会如此昂贵呢？大致有以下三个方面的因素。  </p><p>第一，医学人才培养的成本很高，对于怎样成为一个顶尖的医学技术人才来说，就中国而言，维基百科中是这么描述的：  </p><blockquote><p>从大学第一个本科开始的。一般有五年制，七年制和八年制，分别授予医学学士，医学硕士和医学博士的学位。七年制和八年制统称为长学制。获得医学学士学位的毕业生，可以参加执业医师考试。五年制毕业的医学学士，可以继续攻读医学硕士及博士，两者同样需时三年。七年制毕业的医学硕士，可以继续攻读医学博士，如果攻读本校的医学博士，可以只参加转博考试，通过考试及面试后，再读3年，可以获得医学博士学位。  </p></blockquote><p>也就是说，从高中毕业算起，到获得本科学士学位再到参加执业医师考试获得从业资格，至少需要 5 ~ 6 年的时间，而这种技术水平也仅仅只是从医资格的起步水平，大多数情况下，这种技术水平的医生是很难看好病治好病的。获取博士学位，达到高级医师的水平，高中毕业后最短需要接受 8 年、最长需要接受 11 年的医学知识技能教育，这还不包括毕业获得从业资格之后经过大量临床经验和病例积累增长技术水平所用的时间。这么来看，一个优秀医师培养起来不仅仅是耗费了大量青春年华的宝贵时间，还有大量的金钱投入。事实上在美国以及其他国家也与此类似，美国有个笑话是这么说的：一个人从最初接受医学教育到获得外科医生的行医执照的时候，他的中学同学已经结婚生孩子并且事业略有所成了，而他才找到第一份 50 万美元年薪的工作。当然在美国第一份工作就能获得 50 万美元年薪的外科医师是很少的，事实上有不少人在这个过程中要么读不下去中途放弃了，要么读了个半吊子什么都没学好。年薪 50 万美元什么概念呢？我们可以和美国总统的工资比一下就知道到了，这相当于美国总统年薪的 1.25 倍（2016年美国总统的年薪是 40 万美元）。这是相当可观的一笔收入，在中国估计也是不相上下的。从培养一个高级医师的艰难过程来看的话，一个医生的年薪值这么多钱也是可以理解的。   </p><p>那么针对以上所说的医学人才培养成本高的问题，从机器智能和大数据的角度来看，有什么样的解决方法呢？事实上，我们在平时看病的时候，总喜欢找一些年长的医生，这背后的逻辑其实很简单，从业历史长，那么见的病例一定多，经验一定很丰富，最后做出诊断和治疗的精确性也会越高。也就是说，一个医生的医疗水平，是跟他见过的临床医疗病例的数量有正相关关系的。由此我们也可以想到，如果我们给定一个机器学习的模型，给它提供大量的病例数据以训练参数，那么它是不是也可以变得很聪明直至最后超过大部分医生的水平呢？答案当然是肯定的。事实上，在 2012 年的时候，当时有一个新闻不知道大家还记不记得，美国一个高中生，用 760 万例乳腺癌病例，训练出了一个可以精确定位乳腺癌癌细胞位置的算法，其判断准确率达到了 96% 以上，超过了外科医生的水平，这就是一个典型的例子。在 IBM，通过机器学习的方法训练出来了一个沃特森机器人，现在的语音识别技术赋予了这个机器人与人交流的能力，而这个机器人给人看病的水平，相当于一般中级医师，那么为什么这个机器人给人看病的水平还不能超过现在的大部分高级医师呢？因为人类积累的可供机器阅读的电子病例的数量还不够多，换句话说，机器比较笨，它需要大量大量的，比高级医师所见的多很多的数据，才能通过学习保证自己的水平超过高级医师。那么这些电子病例数据量的积累速度是怎样的呢？IBM 给出的数据是，每隔 73 天会翻一番，这种增长速度会保持到 2020 年。</p><p>因此，从这个方面来看的话，机器给人看病超过医师的水平，也只是时间早晚的问题，并且这个趋势一旦形成就基本上不可能逆转，没有任何一个医生可以与它相匹敌了，因为任何一个人的生命都是有限的，在有限的生命中，任何一个医生所见过的病例的数量都不可能比机器多，水平自然会没有机器高。这是未来的一个大趋势。既然在未来可以通过机器给人看病，并且诊断和治疗的精确度还很高，那么也许以后的哪一天，给人看病的话可以采用一种机器流水线的操作，机器 24 小时可以不停歇地运行，而且还不用花那么多的时间和金钱去培养职业的医学技术从业者，自然而然，看病的成本就降下来了。  </p><p>导致看病昂贵的第二个方面的因素是医疗体制造成的。医疗服务的提供方主要有两个：医院和医生的医疗诊所，而这两个服务提供方提供服务的方式，基本上就是一揽子合同，将诊断和治疗融为一体，将经验医疗和精确医疗融为一体，全部包办。这是什么意思呢？下面我来简单解释一下。  </p><p>医生给人看病的时候可以将病人症状的基本情况分为两类，第一类是一看就能够确诊是什么病，背后是什么机理引起的，这种类型我们称之为精确医疗；第二类是单单从表面症状上看，并不能确定背后的患病机理和患病类型，需要医生通过自己的临床经验，进行试探性地检查和探索，直到最后搞清楚所有的患病机理和患病类型，我们把这个过程称之为经验医疗。</p><p>在实际场景中，这两种形式的医疗都同时存在并且融合在同一个医疗服务的提供方之中的。然而我们回过头来看这两种类型的医疗时，他们虽然有着各自的不同特点，但是在最终的结合点上却有一点是共通的。对于精确医疗来说，一旦确诊之后，这种类型疾病大体可以按照流程划分为规范性的治疗步骤，只要按照这种规范性步骤操作，最后就一定能够得到理想的结果，这种规范性的步骤是通过大量有经验的权威性的医师总结出来的，对于这样的疾病类型，完全可以做到将其治疗的过程外包出去，一旦确诊之后，可以通过专门的医疗机构通过预先总结出来的权威性规范性的措施，一步步治疗达到最后的理想结果，这个过程完全可以不用夹杂在同一个医疗服务提供方之中。而对于经验医疗来说，医生通过一系列的试探性的措施和步骤，最后搞清楚各种状况之后，也就是一个各种精确医疗组合的问题了，对于这种情况而言，也可以通过之前说过的精确医疗的方式进行治疗。如果在实际场景中我们可以将疾病的诊断和治疗两个方面分开，对于其中的一些疾病，采用更低成本的人力配置，采用标准化和流水线化的操作，是完全可以很大程度上降低成本的。当然对于医疗体制方面的问题还有许多许多，譬如在疾病的治疗过程中完全是供决定需的关系，就是说，医生在具体的治疗过程按照项目收费，进行哪些项目完全是由医生决定的，不说决定，病人甚至都没法参与进去，这些都是医疗体制中存在的问题，这些体制问题能否找到行之有效的解决途径？讲一讲这些问题可能还需要好多篇文章的篇幅，对于这些问题，我们以后再说吧。  </p><p>影响看病贵的最后一个因素，是药物研发的问题。实际上，对于很多药物而言，其研发大约需要 20 年的时间里投入 20 亿美元的资金，而对于这种药物的专利，在其研发过程的早期就申请了，因为不申请的话就被别人申请走了，从最早期的专利申请，到药物走过几期临床试验最终面世，大约需要十几年的时间，而专利的保护年限只有 20 年，所以等药物真正面世之后，可能只有几年的时间把早期的研发成本收回来，而到了专利到期的那天，一夜之间，药物的价格大约会下降 80% 以上。所以基于这些因素的考虑，一款新药上市之后价格是非常昂贵的。但是对于很多疾病来说，我们是可以通过早期的一些身体数据的异常，利用大数据分析的方法，推测患某种疾病的可能性，从而做到对很多疾病防患于未然的。在疾病的预防上，人们做的还远远不够，现在的医学的发展也更加重视得了病之后怎么治的问题，而对于很多疾病的早期跟踪和预防问题，显然做的还很不够。  </p><p>上文我们针对医疗领域看病贵的问题，分别谈了三个方面的因素，并且针对每个不同的因素，聊了一下在大数据和机器智能时代有哪些可以优化的措施。现在我们接着来谈下一个问题，也就是医疗资源分配不均衡的问题。  </p><p>我们经常说，互联网能够拉近人与人的距离，在地理位置上相隔几千公里的甚至无论多远的两个人，都可以通过互联网的方式将彼此联系起来，而在医疗资源之中，最重要的资源就是医生资源，我们是否可以通过互联网的手段将一些医术高超的医生和患者跨越地理位置的距离而将其联系起来呢？答案当然是肯定的。不过为了达到在线医疗的预期效果，我们需要建立一种标准格式的在线病历系统，患者保留对自己病历的所有权，并且通过患者自己的授权，这个病历能够被网上任何一个由他自己指定的医生查阅到，同时搭建一个患者与患者之间、医生与医生之间（会诊）、患者与医生之间的交流平台，并且能够根据患者借助于医疗设备做完检查之后，电子病历系统实时更新的变化，用大数据的方法，针对每个患者不同的病情，推荐与之最匹配的医生。这些是完全可以实现的，一旦这些工作落到实处了，那么医疗资源分配不均衡的情况，也可以在某种程度上得到有效的缓解。其实我们回过头来说，如果到了机器智能给人看病超过人类医生的那一天，医疗资源分配不均衡的情况就已经得到解决了，在那些医疗技术水平落后的地区，放一些机器就行了，机器能与人对话，并且随着将来可穿戴设备的普及，人体各项体征数据的采集基本上只要带一个可穿戴设备就行了。  </p><p>对于最后一个方面，很多疾病难以治愈的问题，其实我们可以通过换一种思维，即大数据的思维来解决疾病治疗的问题。拿癌症来说，癌症之所以难以治愈，主要原因是它与诸如感冒之类的疾病致病机理完全不一样，主要体现在以下三个方面：  </p><ul><li>癌症是由于人类自身的细胞在细胞复制的过程中产生基因错误而导致的，它不比感冒这种由于细菌感染所引起的疾病，对于后者，我们可以用青霉素等药物，破坏病毒细胞的细胞壁，杀死病毒细胞，从而可以得到有效的治愈。但是对于癌细胞而言，它来源于人体自身，是没有细胞壁的，所以不能通过这种方式来把癌细胞杀死。实际上，同一种癌症其背后的致病机理可能是由不同的基因错误引起的，所以有些抗癌药物对于有些患者有效，而对于另外一些患者无效，就是因为虽然是同一种癌症，其背后的基因错误是不一样的。很多时候，医生在给癌症患者进行药物治疗之前，需要进行基因比对，以确定这类药物是否对该患者有效，就是这个道理。  </li><li>癌症是由于自身细胞复制的过程中产生基因错误引起的，那么既然人体细胞在复制的过程中能够产生一次错误，自然更容易产生第二次错误（也就是说，坏了的东西其实更容易损坏），这就是我们可能听说过，以前有一位患者癌症治好了，但是突然在某一天复发之后，原来的药物不见效，很快就去世了。就是这个原因，第二次的基因错误和第一次不一样。</li><li>癌细胞既然是由于基因错误引起的，那么这种基因错误的癌细胞，它自身在复制的过程中，也更容易产生错误，也就是说，癌细胞变异进而引起其他癌症的风险很大。</li></ul><p>基于以上三个方面的原因，所以癌症这种疾病变得极难治愈。目前我所了解到的针对癌症治疗的科学进展，主要有两个：其一是所谓的“饿死癌细胞”，这个观点是清华大学的颜宁教授提出来的，其背后的机理是，在提供其他营养物质维持正常细胞代谢的同时，阻断葡萄糖运输特异，饿死癌细胞。至于这种阻断的方式是什么？目前好像还不知道，科学家正在研究中。不过这是一个方向。其二是换一种思维，即采用大数据的思维来解决癌症治疗的问题。下面我具体来谈谈这种方法。  </p><p>癌症是由于基因错误引起的，那么其实我们可以通过研制治疗这种基因错误的药物，来治疗癌症。但是，遗憾的是，目前已知的导致肿瘤的基因错误在万这个数量级上，已知的癌症在百这个数量级上，如果考虑所有可能的基因复制错误和各种癌症的组合，种类将是几百万到上千万种，对于人类来说，这个数字是非常庞大的，不过，与此相对应的，这个数字对于计算机而言，却是一个很小的数字，如果能够利用大数据技术，在这些几百万到上千万种组合中找到真正引起癌症的组合，并且针对这些组合研制相对应的治疗药物，那么治疗癌症的梦想就可以实现了。实际上，国际上有很多大公司都在做相应的努力，譬如 Google 旗下的 Calico，以及美国的 Grail 公司等。  </p><p>以上就是我对于医疗领域了解到的一些知识和自己的一些想法，希望分享给大家能够有所帮助。明天，我们来聊一聊智能时代背景下的教育问题。  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在昨天的文章中我们详细说明了什么是机器智能，用数据驱动获取机器智能的方法，谈了机器智能的一个必要因素就是大数据，并且讲了大数据对于人类生活的预测指导作用，今天我们在此基础之上，主要来谈谈医疗的内容，基于一篇文章的篇幅所限，明天我们再来谈教育。  &lt;/p&gt;
&lt;p&gt;当今医疗领域所面对的问题主要有三个方面：第一，看病贵；第二，医疗资源分配不均衡，好的医疗设备和技术精湛的名医大都集中在经济比较发达的地区，经济不发达地区的人面临的不仅仅是看不起病的问题，同时还有生了重病找不到好的医生的问题；第三，很多疾病，譬如癌症，人类仍然没有找到可靠的能够使其得到稳定治愈的方法。其实针对这三个方面的问题，大数据和机器智能的发展都能够提供一个理论上行得通的解决方案。下面我们一一来谈。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>说说人工智能、大数据、医疗和教育（一）</title>
    <link href="http://blog.keeplearning.group/2017/07/21/2017/07-21-ai-bigdata/"/>
    <id>http://blog.keeplearning.group/2017/07/21/2017/07-21-ai-bigdata/</id>
    <published>2017-07-21T02:50:00.000Z</published>
    <updated>2018-11-07T08:33:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>什么是人工智能？维基百科上是这么说的：  </p><blockquote><p>人工智能（英语：Artificial Intelligence, AI）亦称机器智能，是指由人工制造出来的系统所表现出来的智能。通常人工智能是指通过普通电脑实现的智能。该词同时也指研究这样的智能系统是否能够实现，以及如何实现的科学领域。  </p></blockquote><p>简而言之，就是让人制造的机器具有从事智能活动的能力。实际上，在人工智能发展的早期阶段，科学家就是通过让机器模拟人脑产生智能的方式使其具备从事简单智能活动的能力，人的智能活动很多时候依赖于人类的意识，然而冷冰冰的机器是不能也不会产生意识的，所以实际上这种方法在后来就遇到了很多难以逾越的障碍，几乎没有做出什么突破性的成果。人类和机器之间一个重要的区别就是人类是有意识的，人类能够感知自我的存在和行为，而机器不行。很多媒体报道什么家用机器人跳水自杀之类的新闻显然是哗众取宠和危言耸听，再加上各种科幻电影的渲染，在一些不知所以然的群体中制造了莫名其妙又毫无必要的恐慌，担心机器哪天意识觉醒之后会成为人类的威胁，相比于担忧这些压根儿就不存在的事情，人类更应该考虑的是将来自己的工作机会被机器取代了该怎么办？关于这一点我会在后文提到。</p><p>言归正传，这种让机器模拟人脑产生智能的方式使其具有智能的方法，我们一般称之为传统的人工智能方法。实际上，现在人工智能领域所取得的一系列重大突破性成果，如语音识别、图像识别、机器翻译等，都是采用数据驱动而不是通过模拟人类产生智能的方式取得的。在怎么发展机器智能这条路上，人类大概挣扎了十多年的时间才找到了数据驱动发展机器智能的方法，这个过程就好比人类学会飞翔一样，最初通过模拟鸟类的飞翔给自己的双手绑上插了羽毛的翅膀，然后从高处一跃而下，当然结果可想而知了，后来空气动力学的发展让人类明白了可以利用其原理造出飞机，才真正实现了飞翔的梦想。飞机不需要像鸟儿一样有一双长满羽毛的翅膀才能飞起来，飞机有自己飞起来的方式，与此类似，机器产生智能的方式也不需要向人类一样，可以采用完全不同于人类思维的数据驱动的方式让机器具有智能。那么，什么是数据驱动的方式呢？</p><a id="more"></a><p>所谓数据驱动的方法，首先需要大量的具有代表性的数据，接着需要能够处理这些数据的超级运算能力，然后就是利用一些简单的数学模型组合在一起，去契合这些数据，以训练出模型的参数，这种训练模型参数的过程，就是现在很火的机器学习。从这里可以看到，数据驱动方法的成功，取决于两个必要的因素，一是大量的爆炸式的数据积累，二是超级运算能力。恰好互联网技术，特别是移动互联网的发展，导致数据量喷井式地爆发，产生了大量的可用于机器学习的数据，而摩尔定律（即集成电路的性能会每隔18个月翻一番）又为超级计算的出现准备了条件。数据量和运算能力都指数级增长，所以，在现在这个时代，人工智能领域才出现了很多突破性的进展，并且随着数据量的积累，将来在诸多领域难以解决的问题都可以转化为数据问题用机器智能的方法得到解决。在后文中我将会提到，癌症的治疗问题也可以是一个数据问题。  </p><p>人工智能的关键是利用大量的数据训练出模型的参数，这个过程称为机器学习。对于机器学习，我一直在找一个大多数人都能理解的方式来加以说明，我想我们可以把它看作是解非齐次线性方程组（即等号右边的常数部分不全为 0 的方程组）的逆向过程，先说明一下，对线性代数不熟悉的话可以跳过这部分，只需记住机器学习就是由已知的部分解集合找方程组的过程即可。所谓逆向过程，即已知一个有无穷多解的非齐次线性方程组的部分解集合，求满足这个解集合的非齐次线性方程组的系数矩阵的过程。在这里，解集合就是数据，一个非齐次线性方程组有无穷多解的话，要求其系数矩阵的秩等于增广矩阵的秩同时小于未知数的个数，这就是一个基本的简单模型，我们由这个基本的简单模型出发，一步步去拟合解集合中的数据，直到找到最终的方程组，对于这个最终的方程组来说，目前存在于解集合中的数据，全是它的解。这个过程就是机器学习。   </p><p>细心的人可能已经发现了，我这里所说的是部分解集合，而不是全部（实际上除了用基础解系的方式能够把全部解表示出来的话，没有其他方法能把解集合的全部都表示出来）。这是因为，在实际场景中，我们虽然可以得到大量的数据，但是，永远不可能得到完完全全的所有数据完备集，因为谁也不知道这个完备集的范围是多大，而我们最终训练出来的模型，跟数据量的大小是又很大关系的，数据量不足的情况下，我们的模型误差就会比较大。但是只要我们用以训练的数据量足够大，不断大，那么，我们最终训练出来的模型的精确度跟真实模型的误差就会越小，这不是随口乱说的，背后是有着严密的数学保障的，这个数学保障就是俄国数学家切比雪夫提出的切比雪夫大数定律，这个定律背后表达的思想是：当样本数足够多时，一个随机变量和它的期望值之间的误差可以任意小。那么由此看来，我们最终得到的非齐次线性方程组有以下两种情况：  </p><ul><li>与最真实的模型同解，甚至就是最终的模型，这是最理想的情况；</li><li>满足部分解集合的模型与最真实的模型只是有公共解，那个部分解集合里面的解就是公共解。</li></ul><p>对于第二种情况，是不是意味着工作都白做了，完全没用了呢？当然不是。下面的举个例子说明一下这种情形。  </p><p>我们天体运动的模型是开普勒和牛顿所确定的日心说椭圆形模型，开普勒提出了开普勒三定律，准确地描述了行星运动的规律，牛顿在这个基础上修正了椭圆的焦点，将其移动到了太阳系的重心。开普勒是怎样得到这个模型的呢？实际上，开普勒首先很幸运地观察到了行星运动的椭圆形轨迹，这相当于一个最初的简单模型，并且在此基础之上，用了大半辈子的时光，通过从他的老师那里继承到的和观察到的大量的数据，一步步将模型拟合起来，人工地完成了这个机器学习的过程。其实在此之前，最早提出日心说的是哥白尼，但是哥白尼的日心说模型很不准确，与实际的误差很大，原因是哥白尼是以圆为基础而不是以椭圆为基础建立模型的。早在哥白尼之前，托勒密以圆为基础的地心说模型准确的契合了一百多年以来行星运动的观察数据，这个模型是由 40 ~ 60 个相互嵌套的大小圆组成的，并且托勒密根据自己的模型绘制了一张表，准确地预测了将来的某个时候某个星球所在的位置。托勒密之所以把行星的轨迹看作是圆，是因为受到了毕达哥拉斯的影响，毕达哥拉斯认为，圆是这个世界上最美的图形，所以托勒密认为行星运动的轨迹应该是圆。在哥白尼的时代，他所提出的日心说模型还没有托勒密的地心说模型准确度高呢。托勒密的地心说模型准确地指导了欧洲人的农时1500年，1500年后，托勒密模型对太阳运动的累积误差才多出了 10 天。  </p><p>由上面的例子我们可以看到，其实在实际生活中，最接近真实情况的那个模型往往很难得到（开普勒是运气很好地恰好观察到了椭圆形轨迹），但是我们可以用一系列简单模型的组合来逼近那个最真实的模型（托勒密就是用圆），并且切比雪夫大数定律告诉我们，随着数据量越来越多，我们的简单模型会越来越接近最真实的情况，可以无限接近。实际上这也是我们现在机器学习研究机器智能的方法，大家都知道前段时间 3:0 战胜柯洁 Google 大脑，其背后也就是简单的人工神经网络在几万台服务器上的复杂实现。  </p><p>讲完了数据驱动的人工智能方法，还有一个问题就是，怎么判断机器具有了智能呢？其实这个方法很早很早就有人提出来了，这个人就是大名鼎鼎的阿兰图灵，这个方法被称之为图灵测试。图灵测试是这样子的：拿一块帘子挡住具有智能的机器和一个人，另一个人坐在帘子的对面，这个人和帘子的另一面交谈，如果这个人无法判断对面跟他交谈的是人还是机器的话，那么我们就说这个机器具有了智能。  </p><p>现在，我们已经知道了数据驱动人工智能的方法一个关键性的影响因素是数据量，数据必须要大，必须是 big data ，即大数据。通常来说，人们一看到名字，就知道大数据的其中一个特点，就是数据的体量必须要大，这个没什么可说的，上文中已经说了，这是很关键的。除此之外，大数据还有两个特点：多维度和完备性。什么叫做大数据的多维度呢？拿微信来说，当一个人发一条状态的时候，或者发出一条聊天消息的时候，微信的后台是能够知道这个人的年龄、性别、地域的，年龄、性别、地域和这条状态或记录联系在一起，就成为了一个多维度的数据，而不是最初的仅仅只是一条孤零零的状态或者消息。那么这些多维度的数据又有什么用呢？为什么要求大数据具有多维度呢？这是因为，多维度的数据可以利用数据的交叉性和强相关性的特点得出很多单单从一种角度得不到的信息。举个例子吧，譬如娱乐圈某某明星摊上大事了（出轨了之类的），我们可以在微信上通过大数据分析的方法得到一些对于这个事件的结论：某某年龄段的女生、男生对这个事件的态度啊，某个地域的人对这个事件的态度啊种种，一目了然，通常不同的年龄段的人对这种事情的态度是不一样的，我们通过大数据的方法可以分析出来，不同地域的人态度也可能不一样，譬如该明星所在地的人，可能对这个事件的态度与其他地方的人就不一样了。什么是大数据的完备性呢？就是数据要尽可能地覆盖面要广，这一点在以上也略作了分析，大量重复的数据是毫无意义的。  </p><p>其实，大数据和机器智能不仅仅可以用来做各种社会问题调查（譬如上文所说的对某明星摊上大事的态度），还有一个最重要的特点，也是人工智能领域非常重要的特点，就是预测！什么意思呢？举个例子吧。如果我们掌握了合肥市过去很多年每条道路的交通运行情况数据，我们就可以在过去这些数据的基础上，对每天每条道路的拥堵情况做出准确的预测，帮助人们合理地安排出行时间和选择出行道路，事实上，已经有很多公司在做这方面的研究了。想象一下可以互相通信自动驾驶汽车，它们对整个城市的道路情况了如指掌，可以帮助你选择合理地出行路径以节约大部分的时间。对于自动驾驶汽车的问题，我跟身边人聊起这些的时候，总有人说之前特斯拉出的那场事故，事实上，特斯拉的那场事故已经过去快两年了，依照人工智能的进化速度，自动驾驶早就已经不是当初那个量级了，人很容易犯的一个错误就是用过去的观点看待现在的问题。自动驾驶的应用，现在只剩下政策和保险的问题了。  </p><p>我们知道机器智能的发展依赖于大数据，实际上在大数据领域有两个不同的方向，一个是大数据工程，另一个是大数据分析，大数据工程做的工作是大数据的设计、部署、获取和维护，大数据分析则属于数据挖掘的内容，本质上来说就是机器学习，在大量的数据中提取有用的信息。说到这儿，其实有一个信息度量的问题，数据和信息是不一样的概念，数据量大并不意味着信息量大，譬如在大量重复的数据中，数据量是很大，但是信息量却是很少的，那么怎么样度量信息呢？我们生在了一个好的时代，早就有人替我们解决了信息度量的问题了，这个人是谁呢？也是一个大名鼎鼎的人物 —— 克劳德香农博士。一个杂乱无章的系统是充满很多不确定性的，怎么样消除这些不确定性呢？就是引入信息。香农博士在概率论的基础之上发展了信息论，引入了信息熵的概念，引入的信息量越大，不确定性越小，香农博士的信息论是现代数字通信技术的基础，通信领域的最高奖叫做香农奖，就是以香农博士的名字命名的。如果你对信息论有兴趣，可以看看吴军老师的《数学之美》这本书。  </p><p>大数据和机器智能的发展，在未来所涉及到的影响面会非常之广，在这一点明天的文章再来谈吧。明天的文章会写一下我自己对于未来世界的思考，主要集中在医疗和教育方面。大数据和机器智能的发展可以让很多疾病的发现变得可以预测，特别是癌症，很多癌症在早期是很难检测出来的，但是我们可以用大数据的方法提前发现和预防，另外，就是以后的教育，智能社会机器会剥夺人很多的从事技能劳动的机会，现在的技能教育以后会有什么变化？以后的教育是什么样子的？对于医疗来说，为什么看病这么贵？有哪些方法可以降低医疗成本，我们明天的文章再来谈。  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;什么是人工智能？维基百科上是这么说的：  &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;人工智能（英语：Artificial Intelligence, AI）亦称机器智能，是指由人工制造出来的系统所表现出来的智能。通常人工智能是指通过普通电脑实现的智能。该词同时也指研究这样的智能系统是否能够实现，以及如何实现的科学领域。  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;简而言之，就是让人制造的机器具有从事智能活动的能力。实际上，在人工智能发展的早期阶段，科学家就是通过让机器模拟人脑产生智能的方式使其具备从事简单智能活动的能力，人的智能活动很多时候依赖于人类的意识，然而冷冰冰的机器是不能也不会产生意识的，所以实际上这种方法在后来就遇到了很多难以逾越的障碍，几乎没有做出什么突破性的成果。人类和机器之间一个重要的区别就是人类是有意识的，人类能够感知自我的存在和行为，而机器不行。很多媒体报道什么家用机器人跳水自杀之类的新闻显然是哗众取宠和危言耸听，再加上各种科幻电影的渲染，在一些不知所以然的群体中制造了莫名其妙又毫无必要的恐慌，担心机器哪天意识觉醒之后会成为人类的威胁，相比于担忧这些压根儿就不存在的事情，人类更应该考虑的是将来自己的工作机会被机器取代了该怎么办？关于这一点我会在后文提到。&lt;/p&gt;
&lt;p&gt;言归正传，这种让机器模拟人脑产生智能的方式使其具有智能的方法，我们一般称之为传统的人工智能方法。实际上，现在人工智能领域所取得的一系列重大突破性成果，如语音识别、图像识别、机器翻译等，都是采用数据驱动而不是通过模拟人类产生智能的方式取得的。在怎么发展机器智能这条路上，人类大概挣扎了十多年的时间才找到了数据驱动发展机器智能的方法，这个过程就好比人类学会飞翔一样，最初通过模拟鸟类的飞翔给自己的双手绑上插了羽毛的翅膀，然后从高处一跃而下，当然结果可想而知了，后来空气动力学的发展让人类明白了可以利用其原理造出飞机，才真正实现了飞翔的梦想。飞机不需要像鸟儿一样有一双长满羽毛的翅膀才能飞起来，飞机有自己飞起来的方式，与此类似，机器产生智能的方式也不需要向人类一样，可以采用完全不同于人类思维的数据驱动的方式让机器具有智能。那么，什么是数据驱动的方式呢？&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>谈谈《我的前半生》—— 顺便聊聊我的爱情观</title>
    <link href="http://blog.keeplearning.group/2017/07/18/2017/07-18-half-life/"/>
    <id>http://blog.keeplearning.group/2017/07/18/2017/07-18-half-life/</id>
    <published>2017-07-18T02:50:00.000Z</published>
    <updated>2018-11-07T08:37:44.000Z</updated>
    
    <content type="html"><![CDATA[<p>出来混总是要还的，欠下的文章总还是要写的。一直想写一篇关于《我的前半生》这部电视剧的文章，无奈事情太多，所以拖到了今天。这部剧我没有全部看完，很多地方都是快进或者跳着看的（实在太忙），不过在现在这个时代，就算是跳着看、快进看的也能八九不离十地了解一部电视剧的剧情，这也大概就是我们这个时代各种五花八门的网络剧神奇的地方吧（偷笑中）。想到可以就着这个剧蹭蹭热点，聊聊自己的爱情观，以借此说明一下自己虽在一所男女比例10：1的恐怖学校里待了那么多年，但肯定真的不是基佬并且到现在还没有被掰弯这个事实，不禁会心一笑：嗯，就写它了。  </p><a id="more"></a><p>在这之前我也曾屡次表达过我对追求爱情的看法，主要有以下几点吧： </p><ul><li><p>选择大于努力。你必须要选择一个懂得欣赏你的人，有些人即便你再喜欢，那也不是你的菜，不要觉得自己多么努力地奉献爱心多么暖男那对方就一定会和自己在一起，这么想那就太天真了。第一，这个世界上总是好人比坏人多的，我始终相信这一点，所以，其实对方去选择一个好人的话选择的余地还是蛮大的，不一定会是你，当然我这么说不是讲你不需要成为一个好人，相反你还必须是一个好人，不过要成为一个有魅力的好人，这就不简单了；第二，就生活中而言，那些曾经主动帮助过你的人，总比那些你帮助过的人，更愿意帮助你，世人喜欢雪中送炭的很少，不过都爱锦上添花，同样的道理，一个欣赏你的人，比那些仅仅只是你单方面欣赏的人，更容易跟你在一起，也更容易懂你。选择一个你很欣赏很在乎但是却不怎么在意你的人，可能你再怎么努力对人好，都不见得有什么好的效果，这一点需要明确。我觉得无论男生还是女生，都是如此吧。所以放弃不切实际的幻想还是挺重要的。而且选择一个什么样的人跟自己在一起，直接决定了你以后的生活会是什么样子的，所以，选择还需要慎重。</p></li><li><p>选一个聪明人很重要。这不是我的观点，而是吴军博士的，吴军博士说，这个世界上聪明会欣赏聪明人，而且只有聪明人会欣赏聪明人，这是一个相当有智慧的人告诉他的，他观察周围的人二十余年，证明这句话是对的。对于这个观点，我是非常认同的，不过，我认为，吴军博士这里说的聪明不是指的我们通常意义上的智商高，而是有智慧。什么是智慧？马云说世界上聪明的人很多，但是智慧的人很少，那么什么是智慧呢？就是知道不要什么？举个简单的例子，如果问大多数人，你们究竟要哪些东西？我相信大多数都是能说出来的，嗯，不错，都是聪明人，但是如果问大家，你们究竟不要哪些东西呢？我相信很多很多人，一时真不一定能说出个一二三四来。知道自己不要什么，这也是智慧的一种。如果认为自己是一个智慧的人的话，那就选择一个智慧的人吧，因为只有智慧的人之间才了解彼此才懂得彼此欣赏。</p></li><li><p>选一个『真正的家人』跟自己在一起。至于什么是真正的家人？我在我的另一篇文章《和久浩介该不该跑路》里说了，所以这里就不再赘述。</p></li><li><p>要学会跟自己相处。自己跟自己处出快乐，才能带给别人快乐。试想，一个人如果自己跟自己待在一起都觉得无聊的话，能被谁喜欢上呢？谁会去喜欢一个生活很无趣的人呢？生活才是人生的主题，一个人工作和学习无论多忙，但是千万别忘了你要会生活，有趣的生活才能给人带来幸福的人生。就我自己而言，我平时非常喜欢摄影，几乎到哪都想找个创意咔咔咔几张的那种，不过很少拍人或自拍，更多的是拍景，男生老闲着没事自拍总觉得这人是不是有问题，还有就是景比人更能拍出意境，我觉得。虽然我的摄影技术有很大的进步空间（偷笑中），不过我乐此不疲，这也确实给我带来了很多快乐；我还喜欢K歌，我自我感觉我歌唱得还挺好（熟悉我的可以轻喷），自己一个人或者喊上几个朋友去 KTV 嗨上几首歌也是很愉快的事情；我喜欢漫跑，就喜欢一段长跑之后那种酣畅淋漓和浑身酸爽的感觉，这里顺带说一句和主题无关的话，熟悉我的人可能觉得我挺瘦的，这其实有两个原因，第一是在我们学校读书真的很累（实话实说）；第二个原因就是我们的食堂，我一直觉得我们的食堂是这样子的食堂：西区的不好吃，东区的太难吃，横批：猪都懒得吃。还有一个爱好就是阅读，凡是感兴趣的都想拿出来读一读，最近看亚马逊打折，买了八册的《莎士比亚全集》。熟悉我的人都知道，我的周末一般是什么学习都不干的，要么去打球，要么好好陪家人和朋友吹吹牛谈谈心，其实我挺能说的，一个人可以在那里滔滔不绝讲个把小时的那种，要么静下心来看看书（非技术类）。主要的学习业务，我其实都在工作日干，而且一心一意做，手机开勿扰模式开好久的那种，我发现我这么做了之后，效率反而提上去不少，周末不干活，也没出什么了不起的后果，因为自己已经下定决心周末什么也不做了，平时反而把很多事情都处理得比较好了。  </p></li></ul><p>除了以上四点之外，针对《我的前半生》里的故事，我还想说的是，另外一点也非常非常重要，我们还应该懂得怎么给予爱。  </p><p>故事的一开始，主人公罗子君以一种尖酸刻薄、疑神疑鬼的阔太形象出现，实话说，第一次看到这样子的女子确实让人很讨厌。但是凡事必有因，一个如此形象的人出现在面前，背后肯定有其形成的原因。不少人把这个原因归咎为罗子君自己，我认为对，她自己指定是有责任的，后来看到她妈妈和妹妹的形象之后，更加肯定了这一点，她自身以及家庭的原因的确影响很大。但是，我还想说的是，一段婚姻的失败，出现了问题，肯定是婚姻夫妻双方都有责任，对于那个男的来说，除了出轨之外（出轨当然是应该义正言辞谴责并且十恶不赦的），他难道就没有其他的问题了吗？他的妻子变成了如此一副讨人厌的模样，他自己就没有责任么？他是怎么爱他老婆的？而后爱坏了之后又是怎样把她一脚踢开的？这难道不是问题吗？  </p><p>我一直觉得，两个人在一起，是为了让彼此成为最好的彼此，就是说，咦，认识你之前我都没有发现，我居然有能力做好这么多的事情，原来我的生活还可以这么积极向上有幸福感呀。这，才对了。</p><p>从这一点上来看，罗子君那个老公责任大了去了，他首先是不会爱人，他爱的方式有问题呀，让老婆一个人闷在家里，一年到头出差，陪伴家人的时间少之又少，后来他们打官司的时候出示的证据表明，这个男人常年到头是不在家的，跟自己的老婆极少有精神世界的交流，除了物质上，精神上的给予极其匮乏。连个基本的陪伴都给予不了，只是觉得我挣好钱就行了，这是爱么？最后精神交流都出了问题，反而不知自我反省，还出轨了，爱上了别的女人。他是把一个人爱坏了之后，粗暴地又把这个人踢了出去。</p><p>我一直觉得，感情关系，首先是不能随便选择，与此相对应的，既然选择了，也不能轻易放弃呀。什么是爱？爱就是哪一天我发现对方不好了，我首先想到的是我自己肯定是有责任的，我们在一起生活彼此影响，也许是我忽略了你的生活和你的成长，但是我可以有耐心陪着你慢慢变好呀（当然实在变不好就没办了），爱人之间需要陪伴，无论多么忙碌，生活的平衡感很重要，如果没有了生活的平衡感，缺少了陪伴，家人便会很难感到幸福，如果家里人都不幸福，自己奋斗的目的又是在哪里呢？什么是责任感和安全感？那就是虽然在外面有诱惑，虽然跟外面的诱惑比起来你显得没那么好，但是，请放心，即便你没那么好，我也有足够的耐心陪着你慢慢地变好，这才是责任心和安全感呀。这不是对男生一方说的，我觉得这一条对男生和女生都适用。反正我就是这样子的人，在我心里，纵然再有诱惑，我和你这么多年的感情永远是第一位的。  </p><p>那么怎么爱呢？我有时候听身边的女生说，我喜欢吃香蕉，你却给我一车苹果，你还觉得自己应该让我很感动？这不是有问题吗？不少人还觉得这句话很有道理。我觉得这是一个非常危险的想法。为什么呢？挑食对身体不好，真的，亲。苹果也是对身体很好的呀。如果说对苹果过敏，那过敏至少也是一种病吧？那么我的看法是什么呢？我觉得，你喜欢香蕉，对身体好，没关系，我肯定会给你，但是，我同样会给你苹果，而且会想办法让你接受苹果跟接受香蕉一样开心。当然有的人就是没有成长的心态，就是只爱香蕉不爱苹果，那说明你找的人有问题，你的选择出问题了。</p><p>不少女生希望被自己的男朋友或者老公宠爱着，但是聪明的人一定要想到这一点，如果哪一天你真被她宠坏了，可能被抛弃的也是你。看看罗子君，这不就是一个鲜活的例子么？所以，什么是负责任的爱情呢？就是共同成长，彼此包容，爱你的人不仅仅只是包容你，还要带你一起成长的，让你感受到进步的快乐，让你活得更幸福。这种爱才是负责任的爱，这才是懂得怎么给予爱。  </p><p>其实我对相亲还有一点看法。我个人认为并不是所有的人都适合走相亲这条路的。原因是什么呢？我们可以首先来看一下相亲看什么？第一，长相，肯定要帅或者要漂亮，在相亲的时候，长相是非常重要的，长相不是属于特别帅或者特别漂亮的那种，可能一下就给 pass 掉了，这是实话，当然了，肯把实话说出来的人不多；第二，看品质，怎么看呢？其实只要你不是那种一看就特别不靠谱的人，仅仅只是靠见一面说两句话，这个东西真看不出什么来，可以讲从这个方面来说，大多数人都一样；第三，看能力，怎么看呢？通过标签来看，相亲的时候要看能力的话，只能看标签，譬如你是某某重点大学毕业的呀，学历高呀，收入好呀，各种。所以，如果是以上三个方面都很好的人，无论男生女生，都可以通过相亲来认识未来的伴侣，但是，我相信上面三个条件都很好的话，大多数应该不缺男朋友或者女朋友吧。如果上述三点都不好的话，特别是第一点，那我劝你为了避免以后的麻烦，还是趁早在身边找一个日久生情靠谱的吧，相亲这条路不适合你，倒腾来倒腾去只能是瞎费功夫。很多东西，怎么权衡，看重什么，其实很重要，不可能十全十美，你得看重那些对你而言最重要的东西。  </p><p>最后，祝大家都能找到理想的伴侣吧。  </p><hr><p>另外，打个小广告，请大家帮个忙把这篇文章转发到朋友圈一下，我在这个公众号的开篇第一号的文章《我为什么写公众号》中就表达过，写这个公众号的目的，就是为了一起认识世界，探索世界，并且有志于找一些志同道合的朋友加入进来。我在研究大数据医疗相关的一些东西，并且有志于做一些这方面相关的事情，但是医疗知识匮乏，急需认识一些对医疗体系和医学有深刻了解的朋友，如果认识医疗行业相关的朋友的话，或者你自己就是这方面的人才的话，欢迎通过此公众号与我联系，没准将来可以一起干一票（^_^）：  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;出来混总是要还的，欠下的文章总还是要写的。一直想写一篇关于《我的前半生》这部电视剧的文章，无奈事情太多，所以拖到了今天。这部剧我没有全部看完，很多地方都是快进或者跳着看的（实在太忙），不过在现在这个时代，就算是跳着看、快进看的也能八九不离十地了解一部电视剧的剧情，这也大概就是我们这个时代各种五花八门的网络剧神奇的地方吧（偷笑中）。想到可以就着这个剧蹭蹭热点，聊聊自己的爱情观，以借此说明一下自己虽在一所男女比例10：1的恐怖学校里待了那么多年，但肯定真的不是基佬并且到现在还没有被掰弯这个事实，不禁会心一笑：嗯，就写它了。  &lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
</feed>
