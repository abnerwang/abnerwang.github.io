<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qiyexuxu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.keeplearning.group/"/>
  <updated>2018-11-20T13:19:11.791Z</updated>
  <id>http://blog.keeplearning.group/</id>
  
  <author>
    <name>David Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>PyTorch 学习笔记（十五）</title>
    <link href="http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch15/"/>
    <id>http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch15/</id>
    <published>2018-11-20T10:34:00.000Z</published>
    <updated>2018-11-20T13:19:11.791Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>nn</code> 中的大多数 layer，在 <code>nn.functional</code> 中都有一个与之相对应的函数，<code>nn.functional</code> 中的函数和 <code>nn.Module</code> 的主要区别在于，用 <code>nn.Module</code> 实现的 layers 是一个特殊的类，都是由 <code>class layer(nn.Module)</code> 定义，会自动提取可学习的参数，而 <code>nn.functional</code> 中的函数更像是纯函数，由 <code>def function(input)</code> 定义。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">output1 = model(input)</span><br><span class="line">output2 = nn.functional.linear(input, model.weight, model.bias)</span><br><span class="line">print(output1 == output2)</span><br><span class="line"></span><br><span class="line">b = nn.functional.relu(input)</span><br><span class="line">b2 = nn.ReLU()(input)</span><br><span class="line">print(b == b2)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxeq3vk9axj20wy05a74n.jpg" alt="">  </p><p>如果模型有可学习的参数，最好用 <code>nn.Module</code>，否则既可以用 <code>nn.Module</code> 也可以使用 <code>nn.functional</code>，二者在性能上没有太大差异。但 dropout 操作虽然没有可学习的参数，但还是建议使用 <code>nn.Dropout</code> 而不是 <code>nn.functional.dropout</code>，因为 dropout 在训练和测试两个阶段的行为有所差异，使用 <code>nn.Module</code> 对象能够通过 <code>model.eval</code> 操作加以区分。  </p><p>在模型中搭配使用 <code>nn.Module</code> 和 <code>nn.functional</code>：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.pool(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.pool(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数 <code>__init__</code> 中。<br><a id="more"></a><br>2 . 在深度学习中参数的初始化非常重要，良好的初始化能让模型更快收敛，并达到更高水平，而糟糕的初始化则可能使模型迅速瘫痪。<code>nn.Module</code> 模块的参数都采取了较为合理的初始化策略，因此一般不需要我们考虑。而当我们使用 Parameter 时，自定义初始化尤其重要，PyTorch 中 <code>nn.init</code> 模块就是专门为初始化而设计的，如果某种初始化策略 <code>nn.init</code> 不提供，用户也可以自己直接初始化。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用 nn.init 初始化</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line">init.xavier_normal_(linear.weight)  <span class="comment"># 等价于 linear.weight.data.normal_(0, std)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接初始化</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xavier 初始化的计算公式</span></span><br><span class="line">std = math.sqrt(<span class="number">2</span>)/math.sqrt(<span class="number">7.</span>)</span><br><span class="line">linear.weight.data.normal_(<span class="number">0</span>, std)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对模型的所有参数进行初始化</span></span><br><span class="line"><span class="keyword">for</span> name, params <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> name.find(<span class="string">'linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="comment"># init linear</span></span><br><span class="line">        params[<span class="number">0</span>] <span class="comment"># weight</span></span><br><span class="line">        params[<span class="number">1</span>] <span class="comment"># bias</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'norm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>3 . <code>nn.Module</code> 基类的构造函数：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self._parameters = OrderedDict()</span><br><span class="line">    self._modules = OrderedDict()</span><br><span class="line">    self._buffers = OrderedDict()</span><br><span class="line">    self._backward_hooks = OrderedDict()</span><br><span class="line">    self._forward_hooks = OrderedDict()</span><br><span class="line">    self.training = <span class="keyword">True</span></span><br></pre></td></tr></table></figure><ul><li><code>_parameters</code>：保存用户直接设置的 <code>Parameter</code>;</li><li><code>_modules</code>：指定的子 module 会保存于此；</li><li><code>_buffers</code>：缓存，如 batchnorm 使用 momentum 机制，每次前向传播需用到上一次前向传播的结果；</li><li><code>_backward_hooks</code> 与 <code>_forward_hooks</code>：钩子技术，用来提取中间变量，类似 variable 的 hook；</li><li><code>training</code>：BatchNorm 与 Dropout 层在训练阶段和测试阶段会分别采取不同的策略，通过判断 training 的值来决定前向传播策略。  </li></ul><p>上述几个属性中，<code>_parameters</code>、<code>_modules</code> 和 <code>_buffers</code> 这三个字典中的值，都可以通过 <code>self.key</code> 方式获得，效果等价于 <code>self._parameters[key]</code>。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 等价于 self.register_parameter('param1', nn.Patameter(t.rand(3, 3)))</span></span><br><span class="line">        self.param1 = nn.Parameter(t.rand(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.submodel1 = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        x = self.param1.mm(input)</span><br><span class="line">        x = self.submodel1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(<span class="string">'-------- Net 结构 --------'</span>)</span><br><span class="line">print(net)</span><br><span class="line">print(<span class="string">'-------- Net 结构 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的子 module --------'</span>)</span><br><span class="line">print(net._modules)</span><br><span class="line">print(<span class="string">'-------- Net 中的子 module --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的自定义参数 --------'</span>)</span><br><span class="line">print(net._parameters)</span><br><span class="line">print(<span class="string">'-------- Net 中的自定义参数 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中 para1 的参数 --------'</span>)</span><br><span class="line">print(net.param1)  <span class="comment"># 等价于 net._parameters['param1']</span></span><br><span class="line">print(<span class="string">'-------- Net 中 para1 的参数 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的参数大小 --------'</span>)</span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, parameter.size())</span><br><span class="line">print(<span class="string">'-------- Net 中的参数大小 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的子 module 及其名称 --------'</span>)</span><br><span class="line"><span class="keyword">for</span> name, submodel <span class="keyword">in</span> net.named_modules():</span><br><span class="line">    print(name, submodel)</span><br><span class="line">print(<span class="string">'-------- Net 中的子 module 及其名称 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">input = t.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">output = bn(input)</span><br><span class="line">print(bn._buffers)</span><br></pre></td></tr></table></figure><p><code>nn.Module</code> 在实际使用中可能层层嵌套，一个 module 包含若干个子 module，每一个子 module 又包含更多的子 module，<code>children</code> 方法可以查看直接子 module，<code>module</code> 函数可以查看所有的子 module（包含当前 module）。与之相对应的还有函数 <code>named_children</code> 和 <code>named_modules</code>，其能够在返回 module 列表的同时返回它们的名字。  </p><p>dropout 在训练和测试阶段采取不同策略举例：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input = t.arange(<span class="number">0</span>, <span class="number">12</span>).view(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">model = nn.Dropout()</span><br><span class="line"><span class="comment"># 在训练阶段，会有一半左右的数被随机置为 0</span></span><br><span class="line">print(model(input))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试阶段，dropout 什么都不做</span></span><br><span class="line">model.training = <span class="keyword">False</span></span><br><span class="line">model(input)</span><br></pre></td></tr></table></figure><p>如果一个模型具有多个 dropout 层，不需要为每个 dropout 层指定 training 属性，更为推荐的做法是调用 <code>model.train()</code> 函数，它会将当前 module 及其子 module 中的所有 training 属性都设置为 True，相应的，<code>model.eval()</code> 函数会把 <code>training</code> 属性都设为 False。  </p><p><code>register_forward_hook</code> 与 <code>register_backward_hook</code>，这两个函数的功能类似于 <code>variable</code> 函数的 <code>register_hook</code>，可在 module 前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：<code>hook(module, input, output) -&gt; None</code>，而反向传播则具有如下形式：<code>hook(module, grad_input, grad_output) -&gt; Tensor or None</code>。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在 forward 函数中，但如果在 forward 函数中专门加上这些处理，可能会使处理逻辑比较复杂，这时候使用钩子技术就更合适一些。  </p><p>下面考虑一种场景，有一个预训练好的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，但又不希望修改其原有的模型定义文件，这时就可以利用钩子函数。下面给出实现的伪代码。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = VGG()</span><br><span class="line">features = t.Tensor()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span><span class="params">(module, input, output)</span>:</span></span><br><span class="line">    <span class="string">'''把这层的输出拷贝到 features 中'''</span></span><br><span class="line">    features.copy_(output.data)</span><br><span class="line">    </span><br><span class="line">handle = model.layer8.register_forward_hook(hook)</span><br><span class="line">_ = model(input)</span><br><span class="line"><span class="comment"># 用完 hook 后删除</span></span><br><span class="line">handle.remove()</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;nn&lt;/code&gt; 中的大多数 layer，在 &lt;code&gt;nn.functional&lt;/code&gt; 中都有一个与之相对应的函数，&lt;code&gt;nn.functional&lt;/code&gt; 中的函数和 &lt;code&gt;nn.Module&lt;/code&gt; 的主要区别在于，用 &lt;code&gt;nn.Module&lt;/code&gt; 实现的 layers 是一个特殊的类，都是由 &lt;code&gt;class layer(nn.Module)&lt;/code&gt; 定义，会自动提取可学习的参数，而 &lt;code&gt;nn.functional&lt;/code&gt; 中的函数更像是纯函数，由 &lt;code&gt;def function(input)&lt;/code&gt; 定义。  &lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;model = nn.Linear(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output1 = model(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output2 = nn.functional.linear(input, model.weight, model.bias)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(output1 == output2)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = nn.functional.relu(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b2 = nn.ReLU()(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(b == b2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:&lt;br&gt;&lt;img src=&quot;http://wx1.sinaimg.cn/mw690/79225320gy1fxeq3vk9axj20wy05a74n.jpg&quot; alt=&quot;&quot;&gt;  &lt;/p&gt;
&lt;p&gt;如果模型有可学习的参数，最好用 &lt;code&gt;nn.Module&lt;/code&gt;，否则既可以用 &lt;code&gt;nn.Module&lt;/code&gt; 也可以使用 &lt;code&gt;nn.functional&lt;/code&gt;，二者在性能上没有太大差异。但 dropout 操作虽然没有可学习的参数，但还是建议使用 &lt;code&gt;nn.Dropout&lt;/code&gt; 而不是 &lt;code&gt;nn.functional.dropout&lt;/code&gt;，因为 dropout 在训练和测试两个阶段的行为有所差异，使用 &lt;code&gt;nn.Module&lt;/code&gt; 对象能够通过 &lt;code&gt;model.eval&lt;/code&gt; 操作加以区分。  &lt;/p&gt;
&lt;p&gt;在模型中搭配使用 &lt;code&gt;nn.Module&lt;/code&gt; 和 &lt;code&gt;nn.functional&lt;/code&gt;：  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; functional &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; F&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nn.Module)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(Net, self).__init__()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv1 = nn.Conv2d(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv2 = nn.Conv2d(&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc1 = nn.Linear(&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;120&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc2 = nn.Linear(&lt;span class=&quot;number&quot;&gt;120&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;84&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc3 = nn.Linear(&lt;span class=&quot;number&quot;&gt;84&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.pool(F.relu(self.conv1(x)), &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.pool(F.relu(self.conv2(x)), &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = x.view(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.relu(self.fc1(x))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.relu(self.fc2(x))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = self.fc3(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; x&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数 &lt;code&gt;__init__&lt;/code&gt; 中。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十四）</title>
    <link href="http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch14/"/>
    <id>http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch14/</id>
    <published>2018-11-20T06:47:00.000Z</published>
    <updated>2018-11-20T10:16:55.559Z</updated>
    
    <content type="html"><![CDATA[<p>1 . PyTorch 实现了如今最常用的三种循环神经网络（RNN）：RNN(vanilla RNN)、LSTM 和 GRU，此外还有对应的三种 RNNCell，RNN 和 RNNCell 层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上 RNN 层的一种后端实现方式就是调用 RNNCell 来实现的。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 输入：batch_size=3，序列长度都为 2，序列中每个元素占 4 维</span></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># lstm 输入向量 4 维，隐藏元 3，1 层</span></span><br><span class="line">lstm = nn.LSTM(<span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 初始状态：1 层，batch_size=3，3 个隐藏元</span></span><br><span class="line">h0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">c0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out, hn = lstm(input, (h0, c0))</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 一个 LSTMCell 对应的层数只能是一层</span></span><br><span class="line">lstm = nn.LSTMCell(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">hx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">cx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out = []</span><br><span class="line"><span class="keyword">for</span> i_ <span class="keyword">in</span> input:</span><br><span class="line">    hx, cx = lstm(i_, (hx, cx))</span><br><span class="line">    out.append(hx)</span><br><span class="line">t.stack(out)</span><br></pre></td></tr></table></figure><p>词向量在自然语言中应用十分普及，PyTorch 同样提供了 Embedding 层。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有 4 个词，每个词用 5 维的向量表示</span></span><br><span class="line">embedding = nn.Embedding(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 可以用训练好的词向量初始化 embedding</span></span><br><span class="line">embedding.weight.data = t.arange(<span class="number">0</span>, <span class="number">20</span>).view(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = t.arange(<span class="number">3</span>, <span class="number">0</span>, <span class="number">-1</span>).long()</span><br><span class="line">output = embedding(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 在深度学习中要用到各种各样的损失函数（loss function），这些损失函数可以看成是一种特殊的 layer，PyTorch 也将这些损失函数实现为 <code>nn.Module</code> 的子类，然而在实际应用中通常将这些 loss function 专门提取出来，和主模型互相独立。  </p><p>交叉熵损失（CrossEntropyLoss)：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch_size=3，计算对应每个类别的分数（只有两个类别）</span></span><br><span class="line">score = t.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 三个样本分别属于 1， 0， 1 类，label 必须是 LongTensor</span></span><br><span class="line">label = t.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]).long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss 与普通的 layer 无差异</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(score, label)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><p>3 . PyTorch 将深度学习中常用的优化方法全部封装在 <code>torch.optim</code> 中，能够方便地扩展成自定义的优化方法，所有的优化方法都是继承基类 <code>optim.Optimizer</code>，并实现了自己的优化步骤，下面以随机梯度下降（SGD）说明：  </p><ul><li>优化方法的基本使用方法；</li><li>如何对模型的不同部分设置不同的学习率；</li><li>如何调整学习率。  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义一个 LeNet 网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(params=net.parameters(), lr=<span class="number">1</span>)</span><br><span class="line">optimizer.zero_grad()  <span class="comment"># 梯度清零，等价于 net.zero_grad()</span></span><br><span class="line"></span><br><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = net(input)</span><br><span class="line">output.backward(output)  <span class="comment"># fake backward</span></span><br><span class="line"></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p>为不同的子网络设置不同的学习率，在 finetune 中经常用到，如果对某个参数不指定学习率，就使用最外层的默认学习率。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">1e-2</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure><p>只为两个全连接层设置较大的学习率，其余层的学习率较小。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">special_layers = nn.ModuleList([net.classifier[<span class="number">0</span>], net.classifier[<span class="number">3</span>]])</span><br><span class="line">special_layers_params = list(map(id, special_layers.parameters()))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> special_layers_params, net.parameters())</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: base_params&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: special_layers.parameters(), <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">], lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure><p><code>id</code>：用于获取对象的内存地址。<br><code>map(function, iterable,...)</code>: 第一个参数 <code>function</code> 以参数序列中的每一个元素调用 <code>function</code> 函数，返回包含每次 <code>function</code> 函数返回值的新列表。<br><code>filter(function, iterable)</code>：用于过滤掉不符合条件的元素，返回由符合条件的元素组成的新列表。<code>iterable</code> 是可迭代对象。  </p><p>对于如何调整学习率，主要有两种做法，一种是修改 <code>optimizer.param_groups</code> 中对应的学习率，另一种是更简单也是较为推荐的做法 —— 新建优化器，但是后者对于使用动量的优化器（如 Adam），会丢失动量状态信息，可能会造成损失函数的收敛出现震荡等情况。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：调整学习率，手动 decay，保存动量</span></span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">'lr'</span>] *= <span class="number">0.1</span>   <span class="comment"># 学习率为之前的 0.1 倍</span></span><br><span class="line">print(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：调整学习率，新建一个 optimizer</span></span><br><span class="line">old_lr = <span class="number">0.1</span></span><br><span class="line">optimizer1 = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>: old_lr * <span class="number">0.1</span>&#125;</span><br><span class="line">    ], lr=<span class="number">1e-5</span>)</span><br><span class="line">print(optimizer1)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . PyTorch 实现了如今最常用的三种循环神经网络（RNN）：RNN(vanilla RNN)、LSTM 和 GRU，此外还有对应的三种 RNNCell，RNN 和 RNNCell 层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上 RNN 层的一种后端实现方式就是调用 RNNCell 来实现的。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;t.manual_seed(&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 输入：batch_size=3，序列长度都为 2，序列中每个元素占 4 维&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# lstm 输入向量 4 维，隐藏元 3，1 层&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lstm = nn.LSTM(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 初始状态：1 层，batch_size=3，3 个隐藏元&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;h0 = t.randn(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c0 = t.randn(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;out, hn = lstm(input, (h0, c0))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(out)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;t.manual_seed(&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 一个 LSTMCell 对应的层数只能是一层&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lstm = nn.LSTMCell(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hx = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cx = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;out = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i_ &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; input:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    hx, cx = lstm(i_, (hx, cx))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out.append(hx)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;t.stack(out)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;词向量在自然语言中应用十分普及，PyTorch 同样提供了 Embedding 层。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 有 4 个词，每个词用 5 维的向量表示&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;embedding = nn.Embedding(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 可以用训练好的词向量初始化 embedding&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;embedding.weight.data = t.arange(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;20&lt;/span&gt;).view(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.arange(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;).long()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output = embedding(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(output)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十三）</title>
    <link href="http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch13/"/>
    <id>http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch13/</id>
    <published>2018-11-20T00:56:00.000Z</published>
    <updated>2018-11-20T04:43:24.967Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 图像的卷积操作。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line"></span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">to_pil = ToPILImage()</span><br><span class="line">lena = Image.open(<span class="string">'path to your image'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入是一个 batch，batch_size=1</span></span><br><span class="line">input = to_tensor(lena).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锐化卷积核</span></span><br><span class="line">kernel = t.ones(<span class="number">3</span>, <span class="number">3</span>)/<span class="number">-9</span></span><br><span class="line">kernel[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = conv(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 池化层可以看作是一种特殊的卷积层，用来下采样，但池化层没有可学习的参数，其 weight 是固定的。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool = nn.AvgPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(list(pool.parameters()))  <span class="comment"># []</span></span><br><span class="line"></span><br><span class="line">out = pool(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>3 . 除了卷积层和池化层，深度学习中还将常用到以下几层：  </p><ul><li>Linear：全连接层；</li><li>BatchNorm：批规范化层，分为 1D、2D 和 3D。除了标准的 BatchNorm 之外，还有在风格迁移中常用到的 InstanceNorm 层；</li><li>Dropout：dropout 层用来防止过拟合，同样分为 1D、2D 和 3D。  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入 batch_size = 2，维度３</span></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">h = linear(input)</span><br><span class="line">print(h)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 channel，初始化标准差为 4，均值为 0</span></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">4</span>)</span><br><span class="line">bn.weight.data = t.ones(<span class="number">4</span>) * <span class="number">4</span></span><br><span class="line">bn.bias.data = t.zeros(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">bn_out = bn(h)</span><br><span class="line"><span class="comment"># 注意输出的均值和方差</span></span><br><span class="line"><span class="comment"># 方差是标准差的平方，计算无偏方差分母会减 1</span></span><br><span class="line"><span class="comment"># 使用 unbiased=False，分母不减 1</span></span><br><span class="line">print(bn_out.mean(<span class="number">0</span>), bn_out.var(<span class="number">0</span>, unbiased=<span class="keyword">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个元素以 0.5 的概率舍弃</span></span><br><span class="line">dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">o = dropout(bn_out)</span><br><span class="line">print(o)  <span class="comment"># 有一半的数变为０</span></span><br></pre></td></tr></table></figure><p>4 . PyTorch 实现了常见的激活函数，这些激活函数可作为独立的 layer 使用，这里介绍一下常用的激活函数 ReLU，其数学表达式为 $ReLU(x)=max(0, x)$。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(input)</span><br><span class="line">output = relu(input)  <span class="comment"># 等价于 input.clamp(min=0)</span></span><br><span class="line">print(output)   <span class="comment"># 小于 0  的被截断为 0</span></span><br></pre></td></tr></table></figure><p>ReLU 函数有个 inplace 参数，如果设置为 True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算 ReLU 的反向传播时，只需根据输出就能够推算反向传播的梯度。但是只有少数的 autograd 操作支持 inplace 操作（如 <code>tensor.sigmoid_()</code>)，除非你明确地知道自己在做什么，否则一般不要使用 inplace 操作。</p><p>5 . 在以上例子中，基本上都是将每一层的输出直接作为下一层的输入，这种网络称为前馈传播网络，对于此类网络如果每次都写复杂的 forward 函数会有些麻烦，有两种简化方式，<code>ModuleList</code> 和 <code>Sequential</code>，其中 <code>Sequential</code> 是一个特殊的 Module，它包含几个子 Module，前向传播时会将输入一层接一层地传递下去，<code>ModuleList</code> 也是一个极其特殊的 Module，可以包含几个子 Module，可以像 list 一样使用它，但不能直接把输入传递给 ModuleList。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequential 的三种写法</span></span><br><span class="line">net1 = nn.Sequential()</span><br><span class="line">net1.add_module(<span class="string">'conv'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">'batchnorm'</span>, nn.BatchNorm2d(<span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">'activation_layer'</span>, nn.ReLU())</span><br><span class="line"></span><br><span class="line">net2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">3</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">net3 = nn.Sequential(OrderedDict[</span><br><span class="line">                         (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">                         (<span class="string">'bn1'</span>, nn.BatchNorm2d(<span class="number">3</span>)),</span><br><span class="line">                         (<span class="string">'relu1'</span>, nn.ReLU())</span><br><span class="line">                     ])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'net1:'</span>, net1)</span><br><span class="line">print(<span class="string">'net2:'</span>, net2)</span><br><span class="line">print(<span class="string">'net3:'</span>, net3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可根据名字或序号取出子 Module</span></span><br><span class="line">print(net1.conv, net2[<span class="number">0</span>], net3.conv1)</span><br><span class="line"></span><br><span class="line">input = t.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">output = net1(input)</span><br><span class="line">output = net2(input)</span><br><span class="line">output = net3(input)</span><br><span class="line">output = net3.relu1(net1.batchnorm(net1.conv(input)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">modellist = nn.ModuleList([nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU(), nn.Linear(<span class="number">4</span>, <span class="number">2</span>)])</span><br><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> modellist:</span><br><span class="line">    input = model(input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会报错，因为 modellist 没有实现 forward 方法</span></span><br><span class="line"><span class="comment"># output = modellist(input)</span></span><br></pre></td></tr></table></figure><p><code>ModuleList</code> 是 <code>Module</code> 的子类，当在 <code>Module</code> 中使用它的时候，就能自动识别为子 module，而 python 自带的 list 则不行。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line">        self.list = [nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU()]</span><br><span class="line">        self.module_list = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), nn.ReLU()])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModule()</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(name, param)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxec1gj8swj20u00xt0wx.jpg" alt="">  </p><p>可见，list 中的子 Module 并不能被主 Module 所识别，而 ModuleList 中的子 Module  能够被主 Module 所识别，这意味着如果用 list 保存子 Module，将无法调整其参数，因其未加入到主 Module 的参数中。  </p><p>在实际应用中，如果在构造函数 <code>__init__</code> 中用到 list、tuple、dict 等对象时，一定要思考是否应该用 ModuleList 或 ParameterList 代替。</p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 图像的卷积操作。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; PIL &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Image&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torchvision.transforms &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; ToTensor, ToPILImage&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_tensor = ToTensor()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_pil = ToPILImage()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lena = Image.open(&lt;span class=&quot;string&quot;&gt;&#39;path to your image&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 输入是一个 batch，batch_size=1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = to_tensor(lena).unsqueeze(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 锐化卷积核&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kernel = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)/&lt;span class=&quot;number&quot;&gt;-9&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kernel[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conv = nn.Conv2d(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, (&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;), &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conv.weight.data = kernel.view(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;out = conv(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_pil(out.data.squeeze(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十二）</title>
    <link href="http://blog.keeplearning.group/2018/11/19/2018/11-19-pytorch12/"/>
    <id>http://blog.keeplearning.group/2018/11/19/2018/11-19-pytorch12/</id>
    <published>2018-11-19T06:08:00.000Z</published>
    <updated>2018-11-19T11:19:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>torch.nn</code> 是专门为深度学习而设计的模块，它的核心数据结构是 <code>Module</code>，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 <code>nn.Module</code>，撰写自己的网络/层。 下面自定义一个全连接层。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer = Linear(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">output = layer(input)</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> layer.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br></pre></td></tr></table></figure><a id="more"></a> <p>有几点需要注意：  </p><ul><li>自定义层必须继承 <code>nn.Module</code>，并且在其构造函数中需调用 <code>nn.Module</code> 的构造函数；</li><li>在构造函数 <code>__init__</code> 中必须自己定义可学习的参数，并封装成 <code>nn.Parameter</code>，<code>Parameter</code> 是一种特殊的 Tensor，但其默认需要求导（requires_grad=True);</li><li><code>forward</code> 函数实现前向传播过程，其输入可以是一个或多个 tensor；</li><li>无需写反向传播函数，<code>nn.Module</code> 能够利用 autograd 自动实现反向传播；</li><li><code>Module</code> 中的可学习参数可以通过 <code>named_parameters()</code> 或者 <code>parameters()</code> 返回迭代器，前者会给每个 parameter 都附上名字，使其更具有辨识度。</li></ul><p>2 . <code>Module</code> 能够自动检测到自己的 <code>Parameter</code>，并将其作为学习参数，除了 <code>Parameter</code> 之外，<code>Module</code> 还包含子 <code>Module</code>，主 <code>Module</code> 能够递归查找子 <code>Module</code> 中的 <code>Parameter</code>，下面实现一个多层感知机。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, hidden_features, out_features)</span>:</span></span><br><span class="line">        super(Perceptron, self).__init__()</span><br><span class="line">        self.layer1 = Linear(in_features, hidden_features)</span><br><span class="line">        self.layer2 = Linear(hidden_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = t.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> self.layer2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perceptron = Perceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> perceptron.named_parameters():</span><br><span class="line">    print(name, parameter.size())</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxdg714iv6j20s704c3yx.jpg" alt=""></p><p><code>Module</code> 中 parameter 的命名规范：</p><ul><li>对于类似 <code>self.param_name = nn.Parameter(t.randn(3, 4))</code>，命名为 <code>param_name</code>;</li><li>对于子 <code>Module</code> 中的 <code>parameter</code>，会其名字之前加上当前 <code>Module</code> 的名字。如对于 <code>self.sub_module = SubModel()</code>，<code>SubModel</code>中有个 <code>parameter</code> 的名字叫做 <code>param_name</code>，那么二者拼接而成的 <code>parameter name</code> 就是 <code>sub_module.param_name</code>。  </li></ul><p>3 . 为方便用户使用，PyTorch 实现了神经网络中绝大多数的 layer，这些 layer 都继承于 <code>nn.Module</code>，封装了可学习参数 <code>Parameter</code>，并实现了 <code>forward</code> 函数。这些自定义的 layer 对输入形状都有假设：输入的不是单个数据，而是一个 batch，输入只有一个数据，则必须调用 <code>tensor.unsqueeze(0)</code> 或 <code>tensor[None]</code> 将数据伪装成 batch_size=1 的 batch。  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;torch.nn&lt;/code&gt; 是专门为深度学习而设计的模块，它的核心数据结构是 &lt;code&gt;Module&lt;/code&gt;，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 &lt;code&gt;nn.Module&lt;/code&gt;，撰写自己的网络/层。 下面自定义一个全连接层。 &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nn.Module)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, in_features, out_features)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(Linear, self).__init__()  &lt;span class=&quot;comment&quot;&gt;# 等价于 nn.Module.__init__(self)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.w = nn.Parameter(t.randn(in_features, out_features))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.b = nn.Parameter(t.randn(out_features))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = x.mm(self.w)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; x + self.b.expand_as(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;layer = Linear(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output = layer(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(output)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; name, parameter &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; layer.named_parameters():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(name, parameter)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十一）</title>
    <link href="http://blog.keeplearning.group/2018/11/18/2018/11-18-pytorch11/"/>
    <id>http://blog.keeplearning.group/2018/11/18/2018/11-18-pytorch11/</id>
    <published>2018-11-18T04:28:00.000Z</published>
    <updated>2018-11-18T13:39:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">print(a.data)  <span class="comment"># 还是同一个 tensor</span></span><br><span class="line"></span><br><span class="line">print(a.data.requires_grad)  <span class="comment"># 但是已经独立于计算图之外了</span></span><br><span class="line"></span><br><span class="line">d = a.data.sigmoid_()  <span class="comment"># sigmoid_ 是一个 inplace 操作，会修改 a 自身的值</span></span><br><span class="line">print(a)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">print(a.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错</span></span><br><span class="line">tensor = a.detach()</span><br><span class="line">print(tensor.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计 tensor 的一些指标，不希望被记录</span></span><br><span class="line">mean = tensor.mean()</span><br><span class="line">std = tensor.std()</span><br><span class="line">maximum = tensor.max()</span><br><span class="line">print(mean, std, maximum)</span><br><span class="line"></span><br><span class="line">tensor[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># 下面会报错： RuntimeError: one of the variables needed for gradient</span></span><br><span class="line"><span class="comment">#              computation has been modified by an inplace operation.</span></span><br><span class="line"><span class="comment"># 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确</span></span><br><span class="line"><span class="comment"># c.sum().backward()</span></span><br></pre></td></tr></table></figure></p><a id="more"></a><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxc4jy2emjj20wu0edmy2.jpg" alt=""></p><p>2 . 在反向传播过程中非叶子节点的导数计算完之后即被清空，若想查看这些变量的梯度，有两种方法：  </p><ul><li>使用 <code>autograd.grad</code> 函数；</li><li>使用 <code>hook</code>。</li></ul><p>推荐使用 <code>hook</code> 方法，但是在实际应用中应尽量避免修改 grad 的值。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种方法：使用 grad 获取中间变量的梯度</span></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line">z = y.sum()</span><br><span class="line"><span class="comment"># z 对 y 的梯度，隐式调用 backward()</span></span><br><span class="line">print(t.autograd.grad(z, y))  <span class="comment"># (tensor([1., 1., 1.]),)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种方法：使用 hook</span></span><br><span class="line"><span class="comment"># hook 是一个函数，输入是梯度，无返回值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_hook</span><span class="params">(grad)</span>:</span></span><br><span class="line">    print(<span class="string">'y 的梯度：'</span>, grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line"><span class="comment"># 注册 hook</span></span><br><span class="line">hook_handle = y.register_hook(variable_hook)</span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除非你每次都要用 hook，否则用完之后记得移除 hook</span></span><br><span class="line">hook_handle.remove()</span><br></pre></td></tr></table></figure><p>3 . 看看 variable 中的 grad 属性和 backward 函数 grad_variables 参数的含义。  </p><ul><li>variable x 的梯度是目标函数 $f(x)$ 对 x 的梯度，$\frac{df(x)}{dx}=(\frac{df(x)}{dx_{0}},\frac{df(x)}{dx_{1}},…,\frac{df(x)}{dx_N})$，形状和 x 一致；</li><li>对于 <code>y.backward(grad_variables)</code> 中的 <code>grad_variables</code> 相当于链式求导法则 $\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\cdot \frac{\partial{y}}{\partial{x}}$ 中的 $\frac{\partial{z}}{\partial{y}}$，z 是目标函数，一般是一个标量，故而 $\frac{\partial{z}}{\partial{y}}$ 的形状与 variable y 的形状一致，<code>z.backward()</code> 在一定程度上等价于 <code>y.backward(grad_y)</code>。<code>z.backward()</code> 省略了 <code>grad_variables</code> 参数，因为 z 是一个标量，而 $\frac{\partial{z}}{\partial{z}}=1$。  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()  <span class="comment"># 从 z 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">y_gradient = t.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># dz/dy</span></span><br><span class="line">y.backward(y_gradient)  <span class="comment"># 从 y 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure><p>另外需要注意，只有对 variable 的操作才能使用 autograd，如果对 variable 的 data 直接进行操作，将无法使用反向传播，除了对参数初始化，一般我们不会修改 variable.data 的值。  </p><p><strong>总结</strong>  </p><p>PyTorch 中计算图的特点可总结如下：  </p><ul><li><code>autograd</code> 根据用户对 variable 的操作构建计算图，对变量的操作抽象为 <code>Function</code>；</li><li>对于那些不是任何函数的输出，由用户创建的节点称为叶子节点，叶子节点的 <code>grad_fn</code> 为 None，叶子节点中需要求导的 variable，具有 <code>AccumulateGrad</code> 标识，因其梯度是累加的；</li><li>variable 默认是不需要求导的，即 <code>requires_grad</code> 属性默认为 False，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都为 <code>True</code>；</li><li>variable 的 <code>volatitle</code> 属性默认为 <code>False</code>，如果某一个 variable 的 <code>volatitle</code> 属性被设置为 <code>True</code>，那么所有依赖它的节点的 <code>volatitle</code> 属性都为 <code>True</code>，<code>volatitle</code> 为 <code>True</code> 的节点不会求导，<code>volatitle</code> 的优先级比 <code>requires_grad</code> 高；</li><li>多次反向传播时，梯度是累加的，反向传播的中间缓存会被清空，为进行多次反向传播需指定 <code>retian_graph=True</code> 来保存这些缓存；</li><li>非叶子节点的梯度计算完之后即被清空，可以使用 <code>autograd.grad</code> 或 <code>hook</code> 技术获取非叶子节点值；</li><li>variable 的 grad 与 data 形状一致，应避免直接修改 variable.data，因为对 data 的直接操作无法利用 autograd 进行反向传播；</li><li>反向传播函数 <code>backward</code> 的参数 <code>grad_variables</code> 可以看成链式求导的中间结果，如果是标量，可以省略，默认为 1；</li><li>PyTorch 采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。   </li></ul><p>4 . 目前绝大多数函数都可以使用 <code>autograd</code> 实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？那就需要自己写一个 <code>Function</code>，实现它的前向传播和反向传播代码。  </p><p>此外实现了自己的 <code>Function</code> 之后，还可以使用 <code>gradcheck</code> 函数来检测实现是否正确，<code>gradcheck</code> 通过数值逼近来计算梯度，可能具有一定的误差，通过控制 <code>eps</code> 的大小可以控制容忍的误差。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyAdd</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, w, x, b)</span>:</span></span><br><span class="line">        ctx.save_for_backward(w, x)</span><br><span class="line">        output = w * x + b</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        w, x = ctx.saved_tensors</span><br><span class="line">        grad_w = grad_output * x</span><br><span class="line">        grad_x = grad_output * w</span><br><span class="line">        grad_b = grad_output * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> grad_w, grad_x, grad_b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始前向传播</span></span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line"><span class="comment"># 开始反向传播</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># x 不需要求导，中间过程还是会计算它的导数，但随后被清空</span></span><br><span class="line">print(x.grad, w.grad, b.grad)  <span class="comment"># (None, tensor([1.]), tensor([1.]))</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, x, )</span>:</span></span><br><span class="line">        output = <span class="number">1</span> / (<span class="number">1</span> + t.exp(-x))</span><br><span class="line">        ctx.save_for_backward(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        output, = ctx.saved_tensors</span><br><span class="line">        grad_x = output * (<span class="number">1</span> - output) * grad_output</span><br><span class="line">        <span class="keyword">return</span> grad_x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用数值逼近方式检验计算梯度的公式对不对</span></span><br><span class="line">test_input = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">test_input.requires_grad_()</span><br><span class="line">t.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  &lt;/p&gt;
&lt;p&gt;Input:&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c = a * b&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.data)  &lt;span class=&quot;comment&quot;&gt;# 还是同一个 tensor&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.data.requires_grad)  &lt;span class=&quot;comment&quot;&gt;# 但是已经独立于计算图之外了&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d = a.data.sigmoid_()  &lt;span class=&quot;comment&quot;&gt;# sigmoid_ 是一个 inplace 操作，会修改 a 自身的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(d.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tensor = a.detach()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(tensor.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 统计 tensor 的一些指标，不希望被记录&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mean = tensor.mean()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;std = tensor.std()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;maximum = tensor.max()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(mean, std, maximum)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tensor[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 下面会报错： RuntimeError: one of the variables needed for gradient&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#              computation has been modified by an inplace operation.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# c.sum().backward()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch10/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch10/</id>
    <published>2018-11-17T11:13:00.000Z</published>
    <updated>2018-11-17T14:25:57.582Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>torch.autograd</code> 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在创建 tensor 的时候指定 requires_grad</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">b = t.zeros(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">c = a.add(b)   <span class="comment"># 也可以写成 c = a + b</span></span><br><span class="line"></span><br><span class="line">d = c.sum()</span><br><span class="line">d.backward()   <span class="comment"># 反向传播</span></span><br><span class="line">print(d.requires_grad)   <span class="comment"># d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，</span></span><br><span class="line"><span class="comment"># 因此 c 的 requires_grad 属性会自动设置为 True</span></span><br><span class="line">print(a.requires_grad, b.requires_grad, c.requires_grad)   <span class="comment"># (True, True, True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否为叶子节点</span></span><br><span class="line">print(a.is_leaf, b.is_leaf, c.is_leaf)   <span class="comment"># (True, True, False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，</span></span><br><span class="line"><span class="comment"># 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了</span></span><br><span class="line">print(c.grad <span class="keyword">is</span> <span class="keyword">None</span>)    <span class="comment"># True</span></span><br></pre></td></tr></table></figure><p>2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  </p><p>$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  </p><a id="more"></a><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''手动求导函数'''</span></span><br><span class="line">    dx = <span class="number">2</span> * x * t.exp(x) + x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line">y.backward(t.ones(y.size()))</span><br><span class="line">print(x.grad)</span><br><span class="line">print(gradf(x))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxbcx53kogj20yc078gmn.jpg" alt="">  </p><p>3 . 每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个 variable 的梯度，这些函数的函数名通常以 Backward 结尾。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x  <span class="comment"># 等价于 y = w.mul(x)</span></span><br><span class="line">z = y + b  <span class="comment"># 等价于 z = y.add(b)</span></span><br><span class="line"></span><br><span class="line">print(x.requires_grad, b.requires_grad, w.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_fn 可以查看这个 variable 的反向传播函数，</span></span><br><span class="line"><span class="comment"># z 是 add 函数的输出，所以它的反向传播函数是 AddBackward</span></span><br><span class="line">print(z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># next_functions 保存 grad_fn 的输入，是一个 tuple，tuple 的元素也是 Function</span></span><br><span class="line"><span class="comment"># 第一个是 y，它是乘法（mul）的输出，所以对应的反向传播函数 y.grad_fn 是 MulBackward</span></span><br><span class="line"><span class="comment"># 第二个是 b，它是叶子节点，由用户创建，grad_fn 为 None，但是需要求导，其梯度是累加的</span></span><br><span class="line">print(z.grad_fn.next_functions)</span><br><span class="line">print(z.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>] == y.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个是 w，叶子节点，需要求导，梯度是累加的</span></span><br><span class="line"><span class="comment"># 第二个是 x，叶子节点，不需要求导，所以为 None</span></span><br><span class="line">print(y.grad_fn.next_functions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 叶子节点的 grad_fn 是 None</span></span><br><span class="line">print(w.grad_fn, x.grad_fn)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxbdwxg9bwj214406owfg.jpg" alt="">  </p><p>计算 $w$ 的梯度的时候，需要用到 $x$ 的数值（$\frac{\partial y}{\partial w}=x$），这些数值在前向过程中会保存成 buffer，在计算完梯度之后会自动清空，为了能够多次反向传播需要指定 <code>retain_graph</code> 来保留这些 buffer。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多次反向传播，梯度会累加，这也就是 w 中 AccumulateGrad 标识的含义</span></span><br><span class="line">z.backward()</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([2.])</span></span><br></pre></td></tr></table></figure><p>4 . PyTorch 使用的是动态图，它的计算图在每次前向传播时都是从头开始构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x.data[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line">x = <span class="number">-1</span> * t.ones(<span class="number">1</span>)  <span class="comment"># 写成 x = -1 * t.ones(1, requires_grad=True) 时，x 不计算梯度</span></span><br><span class="line">x = x.requires_grad_()</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([-1.])</span></span><br></pre></td></tr></table></figure><p>变量的 <code>requires_grad</code> 属性默认 <code>False</code>，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都是 <code>True</code>。  </p><p>5 . 有时候可能不希望 autograd 对 tensor 求导，因为求导需要缓存许多中间结构，增加额外的内存/显存开销，同时降低运行速度，那么我们可以关闭自动求导，譬如在模型训练完毕转而进行测试推断的时候。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> t.no_grad():  <span class="comment"># 也可以使用 t.set_grad_enable(False) 设置（无需 with），并且以下代码无缩进</span></span><br><span class="line">    x = t.ones(<span class="number">1</span>)</span><br><span class="line">    w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    y = x * w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y 虽然依赖于 w 和 x，虽然 w.requires_grad=True，但是 y.requires_grad=False</span></span><br><span class="line">    print(x.requires_grad, w.requires_grad, y.requires_grad)  <span class="comment"># (False, True, False)</span></span><br></pre></td></tr></table></figure><p>关闭自动求导后可以使用 <code>t.set_grad_enable(True)</code> 恢复设置。</p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;torch.autograd&lt;/code&gt; 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 在创建 tensor 的时候指定 requires_grad&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 或者&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;).requires_grad_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 或者&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a.requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;b = t.zeros(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;).requires_grad_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c = a.add(b)   &lt;span class=&quot;comment&quot;&gt;# 也可以写成 c = a + b&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d = c.sum()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d.backward()   &lt;span class=&quot;comment&quot;&gt;# 反向传播&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(d.requires_grad)   &lt;span class=&quot;comment&quot;&gt;# d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 因此 c 的 requires_grad 属性会自动设置为 True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.requires_grad, b.requires_grad, c.requires_grad)   &lt;span class=&quot;comment&quot;&gt;# (True, True, True)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 判断是否为叶子节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.is_leaf, b.is_leaf, c.is_leaf)   &lt;span class=&quot;comment&quot;&gt;# (True, True, False)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(c.grad &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;)    &lt;span class=&quot;comment&quot;&gt;# True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  &lt;/p&gt;
&lt;p&gt;$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  &lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（九）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch9/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch9/</id>
    <published>2018-11-17T09:03:00.000Z</published>
    <updated>2018-11-17T09:14:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节实现一个训练线性回归参数的例子，线性回归的损失函数为：$loss=\frac{1}{2} \sum_{i=1}^{N}(y_{i}-(wx_{i}+b))^2$，然后利用随机梯度下降法更新参数 $w$ 和 $b$ 来最小化损失函数，最终学得 $w$ 和 $b$ 的值。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">device = t.device(<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fake_data</span><span class="params">(batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    x = t.rand(batch_size, <span class="number">1</span>, device=device) * <span class="number">5</span></span><br><span class="line">    y = x * <span class="number">2</span> + <span class="number">3</span> + t.randn(batch_size, <span class="number">1</span>, device=device)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机初始化参数</span></span><br><span class="line">w = t.rand(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">b = t.zeros(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.02</span>  <span class="comment"># 设置学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    x, y = get_fake_data(batch_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward: 计算loss</span></span><br><span class="line">    y_pred = x.mm(w) + b.expand_as(y)</span><br><span class="line">    loss = <span class="number">0.5</span> * (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward: 手动计算梯度</span></span><br><span class="line">    dloss = <span class="number">1</span></span><br><span class="line">    dy_pred = dloss * (y_pred - y)</span><br><span class="line"></span><br><span class="line">    dw = x.t().mm(dy_pred)</span><br><span class="line">    db = dy_pred.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w.sub_(lr * dw)</span><br><span class="line">    b.sub_(lr * db)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ii % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 画图</span></span><br><span class="line">        x = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        x = x.float()</span><br><span class="line">        y = x.mm(w.cpu()) + b.cpu().expand_as(x)</span><br><span class="line">        plt.plot(x.cpu().numpy(), y.cpu().numpy())  <span class="comment"># predicted</span></span><br><span class="line"></span><br><span class="line">        x2, y2 = get_fake_data(batch_size=<span class="number">32</span>)</span><br><span class="line">        plt.scatter(x2.cpu().numpy(), y2.cpu().numpy())  <span class="comment"># true data</span></span><br><span class="line"></span><br><span class="line">        plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">        plt.ylim(<span class="number">0</span>, <span class="number">13</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'w: '</span>, w.item(), <span class="string">'b: '</span>, b.item())</span><br></pre></td></tr></table></figure></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节实现一个训练线性回归参数的例子，线性回归的损失函数为：$loss=\frac{1}{2} \sum_{i=1}^{N}(y_{i}-(wx_{i}+b))^2$，然后利用随机梯度下降法更新参数 $w$ 和 $b$ 来最小化损失函数，最终学得 $w$ 和 $b$ 的值。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（八）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch8/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch8/</id>
    <published>2018-11-17T01:49:00.000Z</published>
    <updated>2018-11-17T04:27:26.035Z</updated>
    
    <content type="html"><![CDATA[<p>1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。<br><a id="more"></a><br>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.storage())</span><br><span class="line"></span><br><span class="line">b = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个对象的 id 值可以看作它在内存中的地址</span></span><br><span class="line"><span class="comment"># storage 的内存地址一样，即是同一个 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># a 改变，b 也随之改变，因为它们共享 storage</span></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">c = a[<span class="number">2</span>:]</span><br><span class="line">print(c.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># data_ptr 返回 tensor 首元素的内存地址</span></span><br><span class="line"><span class="comment"># 可以看出相差 16，这是因为 2*8=16</span></span><br><span class="line"><span class="comment"># 相差两个元素，每个元素占 8 个字节（long）</span></span><br><span class="line">print(c.data_ptr(), a.data_ptr())</span><br><span class="line"></span><br><span class="line">c[<span class="number">0</span>] = <span class="number">-100</span>  <span class="comment"># c[0] 的内存地址对应 a[2]</span></span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">d = t.Tensor(c.storage().float())</span><br><span class="line">print(id(c.storage()) == id(d.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面 3 个 tensor 共享 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()) == id(c.storage))</span><br><span class="line"></span><br><span class="line">print(a.storage_offset(), c.storage_offset())</span><br><span class="line"></span><br><span class="line">e = b[::<span class="number">2</span>, ::<span class="number">2</span>]  <span class="comment"># 隔 2 行/列取一个元素</span></span><br><span class="line">print(id(e.storage()) == id(a.storage()))</span><br><span class="line"></span><br><span class="line">print(b.stride(), e.stride())</span><br><span class="line"></span><br><span class="line">print(e.is_contiguous())</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fxauj7ug2xj20wu0u0acf.jpg" alt="">  </p><p>可见绝大多数操作并不修改 tensor 的数据，而只是修改了 tensor 的头信息。这种做法更节省内存，同时提升了处理速度。此外有些操作会导致 tensor 不连续，这时需要调用 <code>tensor.contiguous</code> 方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享 storage。  </p><p>2 . tensor 可以随意地在 GPU/CPU 上传输，使用 <code>tensor.cuda(device_id)</code> 或者 <code>tensor.cpu()</code>，另外一个更通用的方法是 <code>tensor.to(device)</code>。  </p><ul><li>尽量使用 <code>tensor.to(device)</code>，将 <code>device</code> 设为一个可配置的参数，这样可以很轻松地使程序同时兼容 GPU 和 CPU；</li><li>数据在 GPU 之中传输的速度要远快于内存（CPU）到显存（GPU），所以尽量避免在内存和显存之间传输数据。</li></ul><p>3 . tensor 的保存和加载十分简单，使用 <code>torch.save</code> 和 <code>torch.load</code> 即可完成相应的功能。在 save/load 时可以指定使用的 pickle 模块，在 load 时还可以将 GPU tensor 映射到 CPU 或者其他 GPU 上。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> t.cuda.is_available():</span><br><span class="line">    a = a.cuda(<span class="number">1</span>)   <span class="comment"># 把 a 转为 GPU1 上的 tensor</span></span><br><span class="line">    t.save(a, <span class="string">'a.pth'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载 b，存储于 GPU1 上（因为保存时 tensor 就在 GPU1 上）</span></span><br><span class="line">    b = t.load(<span class="string">'a.pth'</span>)</span><br><span class="line">    <span class="comment"># 加载为 c，存储于 CPU 上</span></span><br><span class="line">    c = t.load(<span class="string">'a.pth'</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">    <span class="comment"># 加载为 d，存储于 GPU0 上</span></span><br><span class="line">    d = t.load(<span class="string">'a.pth'</span>, map_location=&#123;<span class="string">'cuda:1'</span>:<span class="string">'cuda:0'</span>&#125;)</span><br></pre></td></tr></table></figure><p>4 . 关于 tensor 还有几点需要注意：  </p><ul><li>大多数 <code>torch.function</code> 都有一个参数 <code>out</code>，这时候产生的结果将保存在 <code>out</code> 指定的 tensor 之中；</li><li><code>torch.set_num_threads</code> 可以设置 PyTorch 进行 CPU 多线程并行计算时候所占用的线程数，这个可以用来限制 PyTorch 所占用的 CPU 数目；</li><li><code>torch.set_printoptions</code> 可以用来设置打印 tensor 时的数值精度和格式。</li></ul><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">t.set_printoptions(precision=<span class="number">10</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxay2fzkx2j20sq04uaan.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>理解 Momentum</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-momentum/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-momentum/</id>
    <published>2018-11-16T13:39:00.000Z</published>
    <updated>2018-11-17T01:29:47.331Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>随机梯度下降（SGD）方法的一个缺点是其更新方向完全依赖于当前 batch 计算出的梯度，因而十分不稳定，Momentum 算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向，这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优的能力。数学表达式如下：  </p><p>$$v_{t}=\gamma v_{t-1}+\alpha \cdot \nabla_{\theta }J(\theta)$$  $$\theta=\theta -v_{t}$$  </p><p>Momentum 算法会观察历史梯度 $v_{t-1}$，若当前梯度的方向与历史梯度一致，表明当前样本不太可能为异常点，则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。<br><a id="more"></a></p><h2 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h2><p>在小球向下滚的过程中，我们希望小球能够提前知道在哪些方向坡面会上升，这样在遇到上升坡面之前，小球就开始减速，这方法就是 Nesterov Momentum，其在凸优化中有较强的理论保证收敛，并且，在实践中 Nesterov Momentum 也要比单纯的 Momentum 的效果好。数学表达式如下：  </p><p>$$v_{t}=\gamma v_{t-1} + \alpha \cdot \nabla_{\theta}J(\theta-\gamma v_{t-1})$$  $$\theta=\theta-v_{t}$$  </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Momentum&quot;&gt;&lt;a href=&quot;#Momentum&quot; class=&quot;headerlink&quot; title=&quot;Momentum&quot;&gt;&lt;/a&gt;Momentum&lt;/h2&gt;&lt;p&gt;随机梯度下降（SGD）方法的一个缺点是其更新方向完全依赖于当前 batch 计算出的梯度，因而十分不稳定，Momentum 算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向，这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优的能力。数学表达式如下：  &lt;/p&gt;
&lt;p&gt;$$v_{t}=\gamma v_{t-1}+\alpha \cdot \nabla_{\theta }J(\theta)$$  $$\theta=\theta -v_{t}$$  &lt;/p&gt;
&lt;p&gt;Momentum 算法会观察历史梯度 $v_{t-1}$，若当前梯度的方向与历史梯度一致，表明当前样本不太可能为异常点，则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Momentum" scheme="http://blog.keeplearning.group/tags/Momentum/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（七）</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch7/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch7/</id>
    <published>2018-11-16T08:01:00.000Z</published>
    <updated>2018-11-16T13:18:07.473Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">abs/sqrt/div/exp/fmod/log/pow…</td><td style="text-align:center">绝对值/平方根/除法/指数/求余/求幂…</td></tr><tr><td style="text-align:center">cos/sin/asin/atan2/cosh…</td><td style="text-align:center">相关三角函数</td></tr><tr><td style="text-align:center">ceil/round/floor/trunc</td><td style="text-align:center">上取整/四舍五入/下取整/只保留整数部分</td></tr><tr><td style="text-align:center">clamp(input, min, max)</td><td style="text-align:center">超过 min 和 max 部分截断</td></tr><tr><td style="text-align:center">sigmod/tanh..</td><td style="text-align:center">激活函数</td></tr></tbody></table><p>对于很多操作，例如 <code>div</code>、<code>mul</code>、<code>pow</code>、<code>fmod</code> 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 <code>a ** 2</code> 等价于 <code>torch.pow(a, 2)</code>，<code>a * 2</code> 等价于 <code>torch.mul(a, 2)</code>。<br><a id="more"></a><br>2 . 归并操作会使输出形状小于输入形状，并可以沿着某一维度进行执行操作，如加法 <code>sum</code>，既可以计算整个 tensor 的和，也可以计算 tensor 中每一行或每一列的和，常用的归并操作如下表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">mean/sum/median/mode</td><td style="text-align:center">均值/和/中位数/众数</td></tr><tr><td style="text-align:center">norm/dist</td><td style="text-align:center">范数/距离</td></tr><tr><td style="text-align:center">std/var</td><td style="text-align:center">标准差/方差</td></tr><tr><td style="text-align:center">cumsum/cumprod</td><td style="text-align:center">累加/累乘</td></tr></tbody></table><p>以上大多数函数都有一个参数 <code>dim</code>，用来指定这些操作是在哪个维度上执行的。  </p><p>假设输入的形状是 (m, n, k)：</p><ul><li>如果指定 <code>dim=0</code>，输出形状就是 (1, n ,k) 或者 (n, k)</li><li>如果指定 <code>dim=1</code>，输出形状就是 (m, 1, k) 或者 (m, k)</li><li>如果指定 <code>dim=2</code>，输出形状就是 (m, n, 1) 或者 (m, n)</li></ul><p>size 中是否有 “1”，取决于参数 <code>keepdim</code>,<code>keepdim=True</code> 会保留维度 1，注意，以上只是经验总结，并非所有的函数都符合这种形状变化方式。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">   </span><br><span class="line">b = t.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">True</span>))  <span class="comment"># 保留维度 1</span></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">False</span>))  <span class="comment"># 不保留维度 1</span></span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.cumsum(dim=<span class="number">1</span>))  <span class="comment"># 沿着行累加</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxa4du79t4j216408e3ze.jpg" alt=""></p><p>3 . 常用的比较函数表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">gt/lt/ge/le/eq/ne</td><td style="text-align:center">大于/小于/大于等于/小于等于/等于/不等</td></tr><tr><td style="text-align:center">topk</td><td style="text-align:center">最大的 k 个数</td></tr><tr><td style="text-align:center">sort</td><td style="text-align:center">排序</td></tr><tr><td style="text-align:center">max/min</td><td style="text-align:center">比较两个 tensor 最大最小值</td></tr></tbody></table><p>表中第一行的比较操作已经实现了运算符重载，因此可以使用 <code>a&gt;=b</code>、<code>a&gt;b</code>、<code>a!=b</code>、<code>a==b</code>，其返回结果是一个 ByteTensor，可用来选取元素。max/min 这两个操作比较特殊，以 <code>max</code> 为例说明如下：</p><ul><li><code>torch.max(tensor)</code>：返回 tensor 中最大的那个数；</li><li><code>torch.max(tensor, dim)</code>：指定维度上最大的数，同时返回 tensor 和下标；</li><li><code>torch.max(tensor1, tensor2)</code>：返回两个 tensor 相比较大的元素。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = t.linspace(<span class="number">15</span>, <span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">print(a &gt; b)</span><br><span class="line"></span><br><span class="line">print(a[a &gt; b])  <span class="comment"># a 中对应位置大于 b 的元素</span></span><br><span class="line"></span><br><span class="line">print(t.max(a))</span><br><span class="line"></span><br><span class="line">print(t.max(b, dim=<span class="number">1</span>))  <span class="comment"># 分别返回对应维度的最大值和最大值所在的下标</span></span><br><span class="line"></span><br><span class="line">print(t.max(a, b))</span><br><span class="line"></span><br><span class="line">print(t.clamp(a, min=<span class="number">10</span>))  <span class="comment"># 比较 a 和 10 较大的元素</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa5kkgcmbj215y0emq4l.jpg" alt=""></p><p>4 . 常用的线性代数函数表如下。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">trace</td><td style="text-align:center">矩阵的迹</td></tr><tr><td style="text-align:center">diag</td><td style="text-align:center">对角线元素</td></tr><tr><td style="text-align:center">triu/tril</td><td style="text-align:center">矩阵的上三角/下三角，可指定偏移量</td></tr><tr><td style="text-align:center">mm/bmm</td><td style="text-align:center">矩阵乘法，batch 的矩阵乘法</td></tr><tr><td style="text-align:center">addmm/addbmm/addmv/addr/badbmm…</td><td style="text-align:center">矩阵运算</td></tr><tr><td style="text-align:center">t</td><td style="text-align:center">转置</td></tr><tr><td style="text-align:center">dot/cross</td><td style="text-align:center">內积/外积</td></tr><tr><td style="text-align:center">inverse</td><td style="text-align:center">求逆矩阵</td></tr><tr><td style="text-align:center">svd</td><td style="text-align:center">奇异值分解</td></tr></tbody></table><p>需要注意的是，矩阵的转置会导致存储空间不连续，需要调用 <code>.contiguous</code> 方法将其转为连续。  </p><p>5 . Numpy 和 Tensor 可以相互转换，共享内存，但是当 Numpy 的数据类型和 Tensor 的数据类型不一样的时候，数据会被复制，不会共享内存。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(a.dtype)</span><br><span class="line"></span><br><span class="line">b = t.Tensor(a)   <span class="comment"># 此处进行拷贝，不共享内存</span></span><br><span class="line">print(b.dtype)</span><br><span class="line"></span><br><span class="line">c = t.from_numpy(a)  <span class="comment"># 注意 c 的类型（DoubleTensor）</span></span><br><span class="line">print(c.dtype)</span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)   <span class="comment"># b 与 a 不共享内存，a 改变但 b 不变</span></span><br><span class="line">print(c)   <span class="comment"># c 与 a 共享内存</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxa6jr9iznj20ue07ywf2.jpg" alt="">  </p><p>6 . 广播法则是科学计算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。  </p><p>Numpy 的广播法则定义如下：  </p><ul><li>让所有输入数组都向其中 shape 最长的数组看齐，shape 中不足的部分通过在前面加 1 补齐；</li><li>两个数组在某一维度的长度要么一致，要么其中一个为 1，否则不能计算；</li><li>当输入数组的某个维度的长度为 1 时，计算时沿此维度复制扩充成一样的形状。</li></ul><p>PyTorch 已经支持了自动广播法则，但是我们还是通过以下两个函数手动实现一下广播法则以加深理解吧。  </p><ul><li><code>unsqueeze</code> 或者 <code>view</code>，或者 <code>tensor[None]</code>，为数据某一维的形状补 1，实现法则 1；</li><li><code>expand</code> 或者 <code>expand_as</code>，重复数组，实现法则 3，该操作不会复制数组，所以不会占用额外的空间。</li></ul><p>注意：<code>repeat</code> 实现与 <code>expand</code> 相类似的功能，但是 <code>repeat</code> 会把相同数据复制多份，因此会占用额外的空间。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = t.zeros(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动广播法则</span></span><br><span class="line"><span class="comment"># 第一步： a 是 2 维，b 是 3 维，所以先在较小的 a 前面补 1，</span></span><br><span class="line"><span class="comment"># 即： a.unsqueeze(0)，a 的形状变为 (1, 3, 2)，b 的形状是 (2, 3, 1)</span></span><br><span class="line"><span class="comment"># 第二步： a 和 b 在第一维和第三维形状不一致，其中一个为 1，</span></span><br><span class="line"><span class="comment"># 可以利用广播法则，两者都扩展成 (2, 3, 2)</span></span><br><span class="line"></span><br><span class="line">print(a + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动广播法则</span></span><br><span class="line"><span class="comment"># 或者 a.view(1, 3, 2).expand(2, 3, 2) + b.expand(2, 3, 2)</span></span><br><span class="line">print(a[<span class="keyword">None</span>].expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>) + b.expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa7an8bd0j20ug0fkgm5.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;函数&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;abs/sqrt/div/exp/fmod/log/pow…&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;绝对值/平方根/除法/指数/求余/求幂…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;cos/sin/asin/atan2/cosh…&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;相关三角函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;ceil/round/floor/trunc&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;上取整/四舍五入/下取整/只保留整数部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;clamp(input, min, max)&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;超过 min 和 max 部分截断&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;sigmod/tanh..&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;激活函数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;对于很多操作，例如 &lt;code&gt;div&lt;/code&gt;、&lt;code&gt;mul&lt;/code&gt;、&lt;code&gt;pow&lt;/code&gt;、&lt;code&gt;fmod&lt;/code&gt; 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 &lt;code&gt;a ** 2&lt;/code&gt; 等价于 &lt;code&gt;torch.pow(a, 2)&lt;/code&gt;，&lt;code&gt;a * 2&lt;/code&gt; 等价于 &lt;code&gt;torch.mul(a, 2)&lt;/code&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（六）</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch6/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch6/</id>
    <published>2018-11-16T01:15:00.000Z</published>
    <updated>2018-11-16T07:15:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . Tensor 支持与 <code>numpy.ndarray</code> 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>])    <span class="comment"># 第 0 行，下标从 0 开始</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="number">0</span>])     <span class="comment"># 第 0 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>, <span class="number">2</span>])   <span class="comment"># 第 0 行第 2 个元素，等价于 a[0][2]</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>][<span class="number">-1</span>])   <span class="comment"># 第 0 行最后一个元素</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>])    <span class="comment"># 前两行</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>])   <span class="comment"># 前两行，第 0,1 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>:<span class="number">1</span>, :<span class="number">2</span>])  <span class="comment"># 第 0 行，前两列</span></span><br><span class="line">print(a[<span class="number">0</span>, :<span class="number">2</span>])   <span class="comment"># 注意两者的区别，形状不同</span></span><br></pre></td></tr></table></figure><a id="more"></a>  <p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx9neqhzwaj214f0ddjt6.jpg" alt="">  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># None 类似于 np.newaxis，为 a 新增了一个轴</span></span><br><span class="line"><span class="comment"># 等价于 a.view(1, a.shape[0], a.shape[1])</span></span><br><span class="line">print(a[<span class="keyword">None</span>].shape)  <span class="comment"># 等价于 a[None,:,:]</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :].shape)</span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :, <span class="keyword">None</span>, <span class="keyword">None</span>].shape)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320ly1fx9nscpu79j214h03kq33.jpg" alt=""></p><p>2 . 常用的选择函数。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>index_select(input, dim, index)</code></td><td style="text-align:center">在指定维度 dim 上选取，比如选取某些行、某些列</td></tr><tr><td style="text-align:center"><code>masked_select(input, mask)</code></td><td style="text-align:center">使用 ByteTensor 进行选取</td></tr><tr><td style="text-align:center"><code>non_zero(input)</code></td><td style="text-align:center">非 0 元素的下标</td></tr><tr><td style="text-align:center"><code>gather(input, dim, index)</code></td><td style="text-align:center">根据 index，在 dim 维度上选取数据，输出的 size 与 index 一样</td></tr></tbody></table><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a &gt; <span class="number">1</span>)  <span class="comment"># 返回一个 ByteTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于 a.masked_select(a&gt;1)</span></span><br><span class="line">print(a[a &gt; <span class="number">1</span>])  <span class="comment"># 选择结果与原 tensor 不共享内存空间</span></span><br><span class="line"></span><br><span class="line">print(a[t.LongTensor([<span class="number">0</span>, <span class="number">1</span>])])  <span class="comment"># 第 0 行和第 1 行</span></span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">Output:  </span><br><span class="line">  </span><br><span class="line">![](http://wx3.sinaimg.cn/mw690/<span class="number">79225320</span>gy1fx9osqbau0j214807lgmc.jpg)  </span><br><span class="line">  </span><br><span class="line">`gather` 是一个比较复杂的操作，对于一个 <span class="number">2</span> 维的 tensor，输出的每个元素如下：</span><br></pre></td></tr></table></figure><p>out[i][j] = input[index[i][j]][j]   # dim=0<br>out[i][j] = input[i][index[i][j]]   # dim=1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">三维 tensor 的 `gather` 操作同理。  </span><br><span class="line">  </span><br><span class="line">`gather(input, dim, index)` 中的 dim 表示的就是第几维度，在二维的例子中，如果 dim=0，那么它表示的就是你接下来的操作是对第一维度进行的，也就是行；如果 dim=1，那么它表示的就是你接下来的操作是对第二个维度进行的，也就是列。index 的大小和 input 的大小是一样的，它表示的是你所选择的维度上的操作。特别注意，index 必须是 LongTensor 类型。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line">import torch as t</span><br><span class="line"></span><br><span class="line">a = t.arange(0, 16).view(4, 4)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># 选取对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3]])</span><br><span class="line">print(a.gather(0, index))</span><br><span class="line"></span><br><span class="line"># 选取反对角线上的元素</span><br><span class="line">index = t.LongTensor([[3, 2, 1, 0]]).t()</span><br><span class="line">print(a.gather(1, index))</span><br><span class="line"></span><br><span class="line"># 选取两个对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()</span><br><span class="line">b = a.gather(1, index)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9purwv3xj214j0d8gmb.jpg" alt="">  </p><p>与 <code>gather</code> 相对应的逆操作是 <code>scatter_</code>，<code>gather</code> 把数据从 input 中按照 index 取出，而 <code>scatter_</code> 是把取出的数据再放回去（inplace 操作）。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把两个对角线的元素放回到指定位置</span></span><br><span class="line">c = t.zeros(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">b = b.float()  <span class="comment"># 将 b 转换成 FloatTensor</span></span><br><span class="line">c.scatter_(<span class="number">1</span>, index, b)</span><br><span class="line">print(c)</span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">对 tensor 的任何索引操作仍是一个 tensor，想要获取标准的 tensor 对象数值，需要调用 `tensor.item()`，这个方法只对包含一个元素的 tensor 适用。</span><br><span class="line">  </span><br><span class="line"><span class="number">3</span> . PyTorch 目前已支持绝大多数 numpy 的高级索引，高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的 Tensor 共享内存。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">27</span>).view(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">0</span>]])  <span class="comment"># x[1, 1, 2] 和 x[2, 2, 0]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])  <span class="comment"># x[2, 0, 1]，x[1, 0, 1], x[0, 0, 1]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">0</span>, <span class="number">2</span>], ...])   <span class="comment"># x[0] 和 x[2]</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9wkft7f2j21570kf0tu.jpg" alt=""></p><p>4 . Tensor 有不同的数据类型，每种类型分别对应有 CPU 和GPU 版本（HalfTensor 除外），默认的 tensor 都是 FloatTensor，可通过 <code>torch.set_default_tensor_type</code> 来修改默认 tensor 类型，如果默认类型为 GPU tensor，则所有操作都在 GPU 上进行，HalftTensor 是专门为 GPU 版本设计的，同样的元素个数，显存占用只有 FloatTensor 的一半，所以可以极大缓解 GPU 显存不足的问题，但由于其数值大小和精度有限，所以可能出现溢出等问题。  </p><p>各数据类型之间可以相互转换，<code>type(new_type)</code> 是通用的做法，同时还有 <code>float</code>、<code>long</code>、<code>half</code> 等快捷方法，CPU tensor 和 GPU tensor 之间的相互转换通过 <code>tensor.cuda</code> 和 <code>tensor.cpu</code> 方法来实现，此外还可以使用 <code>tensor.to(device)</code>。  </p><p>Tensor 还有一个 <code>new</code> 方法，用法与 <code>t.Tensor</code> 一样，会调用该 tensor 对应类型的构造函数，生成与当前 tensor 类型一致的 tensor，<code>torch.*_like(tensor)</code> 可以生成和 tensor 拥有同样属性（类型、形状、CPU?GPU）的新 tensor。<code>tensor.new_*(new_shape)</code> 新建一个不同形状的 tensor。  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . Tensor 支持与 &lt;code&gt;numpy.ndarray&lt;/code&gt; 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  &lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])    &lt;span class=&quot;comment&quot;&gt;# 第 0 行，下标从 0 开始&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])     &lt;span class=&quot;comment&quot;&gt;# 第 0 列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 第 0 行第 2 个元素，等价于 a[0][2]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 第 0 行最后一个元素&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])    &lt;span class=&quot;comment&quot;&gt;# 前两行&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 前两行，第 0,1 列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, :&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])  &lt;span class=&quot;comment&quot;&gt;# 第 0 行，前两列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, :&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 注意两者的区别，形状不同&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（五）</title>
    <link href="http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch5/"/>
    <id>http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch5/</id>
    <published>2018-11-15T11:57:00.000Z</published>
    <updated>2018-11-15T13:44:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 通过 <code>tensor.view</code> 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另一个也会跟着改变。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.view(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当某一维为 -1 时，会自动计算它的大小</span></span><br><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line">print(b.shape)</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 在实际应用中可能需要添加或减少某一维度，这时 <code>squeeze</code> 和 <code>unsqueeze</code> 两个函数就派上用场了。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(b)</span><br><span class="line">print(b.shape)</span><br><span class="line"></span><br><span class="line">print(b.unsqueeze(<span class="number">1</span>))  <span class="comment"># 在第 1 维（下标从 0 开始）上增加 "1"</span></span><br><span class="line"><span class="comment"># 等价于 b[:,None]</span></span><br><span class="line">print(b[:, <span class="keyword">None</span>].shape)</span><br><span class="line"></span><br><span class="line">print(b.unsqueeze(<span class="number">-2</span>))  <span class="comment"># -2 表示倒数第二个维度</span></span><br><span class="line"></span><br><span class="line">c = b.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(c)</span><br><span class="line">print(c.squeeze(<span class="number">0</span>))   <span class="comment"># 压缩第 0 维的 "1"</span></span><br><span class="line"></span><br><span class="line">print(c.squeeze())   <span class="comment"># 把所有维度为 "1" 的都压缩掉</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)   <span class="comment"># b 作为 view 之后的也跟着被修改</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx92fwjyk7j213s0icgmx.jpg" alt="">  </p><p>3 . <code>resize</code> 是另一种可用来调整 <code>size</code> 的方法，但是与 <code>view</code> 不同，它可以修改 tensor 的大小，如果新大小超过了原大小，会自动分配新的存储空间，而如果新大小小于原大小，则之前的数据依旧会被保存。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">b = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">b.resize_(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">b.resize_(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx92noka8jj214404udfy.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 通过 &lt;code&gt;tensor.view&lt;/code&gt; 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另一个也会跟着改变。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a = t.arange(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.view(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当某一维为 -1 时，会自动计算它的大小&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = a.view(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(b.shape)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（四）</title>
    <link href="http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch4/"/>
    <id>http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch4/</id>
    <published>2018-11-15T01:18:00.000Z</published>
    <updated>2018-11-15T07:29:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 从接口的角度来讲，对 tensor 的操作可分为两类：</p><ul><li><code>torch.function</code>，如 <code>torch.save</code> 等；</li><li>另一类是 <code>tensor.function</code>，如 <code>tensor.view</code> 等。</li></ul><p>为了方便使用，对 tensor 的大部分操作同时支持这两类接口，如 <code>torch.sum(torch.sum(a, b))</code> 与 <code>tensor.sum(a.sum(b))</code> 功能等价。</p><a id="more"></a><p>2 . 在 Pytorch 中新建 tensor 的方法具体有很多，如下表：  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>Tensor(*sizes)</code></td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center"><code>tensor(data)</code></td><td style="text-align:center">类似 <code>np.array</code> 的构造函数</td></tr><tr><td style="text-align:center"><code>ones(*sizes)</code></td><td style="text-align:center">全 1 Tensor</td></tr><tr><td style="text-align:center"><code>zeros(*sizes)</code></td><td style="text-align:center">全 0  Tensor</td></tr><tr><td style="text-align:center"><code>eye(*sizes)</code></td><td style="text-align:center">对角线为 1，其他为 0</td></tr><tr><td style="text-align:center"><code>arange(s, e, steps)</code></td><td style="text-align:center">从 s  到 e，步长为 step</td></tr><tr><td style="text-align:center"><code>linspace(s, e, steps)</code></td><td style="text-align:center">从 s 到 e，均匀切分成 steps 份</td></tr><tr><td style="text-align:center"><code>rand/randn(*sizes)</code></td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center"><code>normal(mean, std)/uniform(from, to)</code></td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center"><code>randperm(m)</code></td><td style="text-align:center">随机排列</td></tr></tbody></table><p>这些创建方法都可以在创建的时候指定数据类型 <code>dtype</code> 和存放 device(cpu/gpu)。　　</p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>)   <span class="comment"># 指定 tensor 的形状，其数值取决于内存空间的状态，print 的时候可能 overflow</span></span><br><span class="line"></span><br><span class="line">b = t.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])   <span class="comment"># 用 list 的数据创建 tensor</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">print(b.tolist())  <span class="comment"># 把 tensor 转为 list</span></span><br><span class="line"></span><br><span class="line">b_size = b.size()</span><br><span class="line">print(b_size)</span><br><span class="line"></span><br><span class="line">print(b.numel())   <span class="comment"># 返回 b 中元素总个数，等价于 b.nelement()</span></span><br><span class="line"></span><br><span class="line">c = t.Tensor(b_size)   <span class="comment"># 创建一个和 b 形状一样的 tensor</span></span><br><span class="line">d = t.Tensor((<span class="number">2</span>, <span class="number">3</span>))   <span class="comment"># 创建一个元素为 2 和 3 的 tensor</span></span><br><span class="line"></span><br><span class="line">print(c.shape)    <span class="comment"># 与 c.size() 等价</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fx8r4lflf0j214i06o74m.jpg" alt="">  </p><p><code>t.Tensor(*sizes)</code>　创建 tensor 时，系统不会马上分配空间，只会计算剩余的内存是否足够使用，使用到 tensor 时才会分配，而其他操作都是在创建完 tensor 之后马上进行空间分配。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">print(t.ones(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.zeros(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.arange(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(t.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.randn(<span class="number">2</span>, <span class="number">3</span>, device=t.device(<span class="string">'cpu'</span>)))</span><br><span class="line"></span><br><span class="line">print(t.randperm(<span class="number">5</span>))    <span class="comment"># 长度为 5 的随机排列</span></span><br><span class="line"></span><br><span class="line">print(t.eye(<span class="number">2</span>, <span class="number">3</span>, dtype=t.int))  <span class="comment"># 对角线为 1，不要求行数与列数一致</span></span><br><span class="line"></span><br><span class="line">scalar = t.tensor(<span class="number">3.14159</span>)</span><br><span class="line">print(<span class="string">'scalar: %s, shape of scalar: %s'</span> % (scalar, scalar.shape))</span><br><span class="line"></span><br><span class="line">vector = t.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(<span class="string">'vector: %s, shape of vector: %s'</span> % (vector, vector.shape))</span><br><span class="line"></span><br><span class="line">tensor = t.Tensor(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(tensor.shape)</span><br><span class="line"></span><br><span class="line">matrix = t.tensor([[<span class="number">0.1</span>, <span class="number">1.2</span>], [<span class="number">2.2</span>, <span class="number">3.1</span>], [<span class="number">4.9</span>, <span class="number">5.2</span>]])</span><br><span class="line">print(matrix)</span><br><span class="line">print(matrix.shape)</span><br><span class="line"></span><br><span class="line">ten = t.tensor([[<span class="number">0.11111</span>, <span class="number">0.22222</span>, <span class="number">0.33333</span>]], dtype=t.float64, device=t.device(<span class="string">'cpu'</span>))</span><br><span class="line">print(ten)</span><br><span class="line"></span><br><span class="line">empty_tensor = t.tensor([])</span><br><span class="line">print(empty_tensor.shape)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx8rusct9tj214h0kkgo1.jpg" alt="">  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 从接口的角度来讲，对 tensor 的操作可分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.function&lt;/code&gt;，如 &lt;code&gt;torch.save&lt;/code&gt; 等；&lt;/li&gt;
&lt;li&gt;另一类是 &lt;code&gt;tensor.function&lt;/code&gt;，如 &lt;code&gt;tensor.view&lt;/code&gt; 等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了方便使用，对 tensor 的大部分操作同时支持这两类接口，如 &lt;code&gt;torch.sum(torch.sum(a, b))&lt;/code&gt; 与 &lt;code&gt;tensor.sum(a.sum(b))&lt;/code&gt; 功能等价。&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（三）</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch3/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch3/</id>
    <published>2018-11-14T14:00:00.000Z</published>
    <updated>2018-11-15T07:27:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>对于常用的数据集，PyTorch 提供了封装好的接口供用户快速调用，这些主要保存在 <code>torchvision</code> 中。<code>torchvision</code> 实现了常用的图像数据加载功能，例如 Imagenet、CIFAR10、MNIST 等，以及常用的数据转换操作，这极大方便了数据加载，并且具有可重用性。<br><a id="more"></a><br>下面的程序训练网络 LeNet 对 CIFAR-10 数据集分类：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToPILImage</span><br><span class="line"></span><br><span class="line">show = ToPILImage()  <span class="comment"># 可以把 Tensor 转成 Image，方便可视化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义对数据的预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为 Tensor</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),  <span class="comment"># 归一化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">trainset = tv.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">'/home/abnerwang/tmp/data/'</span>,</span><br><span class="line">    train=<span class="keyword">True</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = t.utils.data.DataLoader(</span><br><span class="line">    trainset,</span><br><span class="line">    batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="keyword">True</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">testset = tv.datasets.CIFAR10(</span><br><span class="line">    <span class="string">'/home/abnerwang/tmp/data'</span>,</span><br><span class="line">    train=<span class="keyword">False</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line">testloader = t.utils.data.DataLoader(</span><br><span class="line">    testset,</span><br><span class="line">    batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="keyword">False</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># 定义优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU 训练网络</span></span><br><span class="line">device = t.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> t.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 输入数据</span></span><br><span class="line">        images, labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward</span></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印 log 信息</span></span><br><span class="line">        <span class="comment"># loss 是一个 scalar，需要使用 loss.item() 来获取数值，不能使用 loss[idx]</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:  <span class="comment"># 每 2000 个 batch 打印一下训练状态</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络在测试集上的效果</span></span><br><span class="line">correct = <span class="number">0</span>  <span class="comment"># 预测正确的图片数</span></span><br><span class="line">total = <span class="number">0</span>  <span class="comment"># 总共的图片数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于测试的时候不需要求导，可以暂时关闭 autograd，提高速度，节约内存</span></span><br><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = t.max(outputs, <span class="number">1</span>)   <span class="comment"># predicted 为每行概率最大值的索引，_ 为最大概率值</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'10000 张测试集中的准确率为： %d %%'</span> % (<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于常用的数据集，PyTorch 提供了封装好的接口供用户快速调用，这些主要保存在 &lt;code&gt;torchvision&lt;/code&gt; 中。&lt;code&gt;torchvision&lt;/code&gt; 实现了常用的图像数据加载功能，例如 Imagenet、CIFAR10、MNIST 等，以及常用的数据转换操作，这极大方便了数据加载，并且具有可重用性。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>理解 batch_size</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-batch_size/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-batch_size/</id>
    <published>2018-11-14T12:52:00.000Z</published>
    <updated>2018-11-14T13:20:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>梯度下降主要有以下三种：  </p><ul><li>batch gradient descent：即批梯度下降，其计算开销大，计算速度慢，不支持在线学习；</li><li>stochastic gradient descent：即随机梯度下降，收敛性能不太好，可能 hit 不到最优点；</li><li>mini-batch gradient descent：即小批量梯度下降，通常所说的 batch_size 指的就是这个批量的大小。</li></ul><a id="more"></a><p>batch_size = 1 时称为在线学习。</p><p>batch_size 的选择，首先决定的是下降方向，如果数据集较小，完全可以采用全数据集的形式。</p><p>增大 batch_size 的好处：</p><ul><li>内存的利用率提高了，大矩阵乘法的并行化效率提高了；</li><li>跑完一次 epoch（即全数据集）所需迭代次数减少，对于相同数据量的处理速度进一步加快；</li><li>在一定范围内，batch_size 越大，其确定的下降方向就越准确，引起训练震荡越小。</li></ul><p>盲目增大 batch_size 的坏处：</p><ul><li>当数据集太大时，内存撑不住；</li><li>跑完全数据集所需迭代次数减少了，但要达到相同的精度，时间开销大，参数的修正更加缓慢；</li><li>batch_size 增大到一定程度，其确定的下降方向已基本不再变化。</li></ul><p><strong>总结：</strong>  </p><p>1) batch_size 太小，而类别又比较多的时候，可能会导致 loss 函数震荡而不收敛，尤其是网络比较复杂的时候；</p><p>2) 随着 batch_size 的增大，处理相同数据量的速度越快；</p><p>3) 随着 batch_size 的增大，达到相同的精度所需的训练 epoch 数量越多；</p><p>4) 由于上述两种因素的矛盾，batch_size 增加到某个时候，达到时间上的最优；</p><p>5) 由于最终收敛精度会陷入不同的局部极值，因此，batch_size 增大到某些时候，达到最终收敛精度上的最优；</p><p>6) 过大的 batch_size 会使网络收敛到一些不好的局部最优点，同样太小的 batch_size 会使网络收敛太慢、不易收敛等；</p><p>7) 具体 batch_size 的选取和训练集的样本数目有关。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梯度下降主要有以下三种：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;batch gradient descent：即批梯度下降，其计算开销大，计算速度慢，不支持在线学习；&lt;/li&gt;
&lt;li&gt;stochastic gradient descent：即随机梯度下降，收敛性能不太好，可能 hit 不到最优点；&lt;/li&gt;
&lt;li&gt;mini-batch gradient descent：即小批量梯度下降，通常所说的 batch_size 指的就是这个批量的大小。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="batch_size" scheme="http://blog.keeplearning.group/tags/batch-size/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（二）</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch2/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch2/</id>
    <published>2018-11-14T10:26:00.000Z</published>
    <updated>2018-11-15T06:37:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 在 Tensor 上的所有操作，<code>autograd</code> 都能为它们自动提供微分，避免了手动计算导数的复杂过程，只需要设置 <code>tensor.requires_grad=True</code> 即可。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上一步等价于</span></span><br><span class="line"><span class="comment"># x = t.ones(2, 2)</span></span><br><span class="line"><span class="comment"># x.requires_grad = True</span></span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">y = x.sum()</span><br><span class="line">print(y)</span><br><span class="line"></span><br><span class="line">print(y.grad_fn)</span><br><span class="line"></span><br><span class="line">y.backward()   <span class="comment"># 反向传播计算梯度</span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fx7rvm1w6sj214e05ojrw.jpg" alt=""></p><p><code>grad</code> 在反向传播过程中是累加的，每一次运行反向传播，梯度都会累加之前的梯度，因此反向传播之前需要把梯度清零。</p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以下划线结束的函数是 inplace 操作，会修改自身的值</span></span><br><span class="line">x.grad.data.zero_()</span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7s1w5mz3j213y05imx9.jpg" alt=""><br><a id="more"></a><br>2 . <code>torch.nn</code> 是专门为神经网络设计的模块化接口，<code>nn</code> 构建于 Autograd 之上，可用来定义和运行网络，<code>nn.Module</code> 是 <code>nn</code> 中最重要的类，可以把它看成是一个网络的封装，包含网络定义以及 <code>forward</code> 方法，调用 <code>forward(input)</code> 方法，可返回前向传播的结果，下面是 LeNet 的实现。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># nn.Module 子类的函数必须在构造函数中执行父类的构造函数</span></span><br><span class="line">        <span class="comment"># 下式等价于 nn.Module.__init__(self)</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 卷积层 1 表示输入图片为单通道，6 表示输出通道数，5 表示卷积核为 5*5</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 卷积层</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 仿射层/全连接层， y=Wx+b</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 卷积 --&gt; 激活 --&gt; 池化</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># reshape, -1 表示自适应</span></span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fx7swean9dj214e06jdgl.jpg" alt=""></p><p>只要在 <code>nn.Module</code> 的子类中定义了 <code>forward</code> 函数，<code>backward</code> 函数就会自动被实现（利用 <code>autograd</code>），网络的可学习参数通过 <code>net.parameters()</code> 返回，<code>net.named_parameters</code> 可同时返回可学习的参数和名称。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameters <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, <span class="string">':'</span>, parameters.size())</span><br></pre></td></tr></table></figure><p>Output:</p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7t8t7uitj21470a3gn3.jpg" alt=""></p><p><code>forward</code> 函数的输入和输出都是 Tensor。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">out = net(input)</span><br><span class="line">print(out.size())</span><br></pre></td></tr></table></figure><p>Output:</p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx7tfza8rlj214l01amx0.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.zero_grad()   <span class="comment"># 所有参数的梯度清零</span></span><br><span class="line">out.backward(t.ones(<span class="number">1</span>, <span class="number">10</span>))    <span class="comment"># 反向传播</span></span><br></pre></td></tr></table></figure><p><code>torch.nn</code> 只支持 mini-batches，不支持一次只输入一个样本，即一次必须是一个 batch，但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code> 将 batch_size 设为 1。</p><p>3 . <code>nn</code> 实现了神经网络中大多数的损失函数，例如 <code>nn.MSELoss</code> 用来计算均方误差，<code>nn.CrossEntropyLoss</code> 用来计算交叉熵损失。</p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = t.arange(<span class="number">0</span>, <span class="number">10</span>).view(<span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target.float())</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><p>Output:</p><p><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx7uw9frqoj2149014aa1.jpg" alt=""></p><p><code>torch.LongTensor</code>   —-&gt;  <code>torch.FloatTensor</code>: <code>tensor.float()</code>  </p><p> <code>torch.FloatTensor</code>  —-&gt; <code>torch.LongTensor</code>: <code>tensor.long()</code>  </p><p> Input:  </p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行 backward，观察调用之前和调用之后的 grad</span></span><br><span class="line">net.zero_grad()    <span class="comment"># 把 net 中所有可学习参数的梯度清零</span></span><br><span class="line">print(<span class="string">'反向传播之前 conv1.bias 的梯度: '</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line">loss.backward()</span><br><span class="line">print(<span class="string">'反向传播之后 conv1.bias 的梯度: '</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure><p> Output:</p><p> <img src="http://wx2.sinaimg.cn/mw690/79225320ly1fx7v5aj09kj214c03vjrw.jpg" alt=""></p><p>4 . 在反向传播计算完所有参数的梯度之后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降（SGD）的更新策略手动实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.data * learning_rate)</span><br></pre></td></tr></table></figure><p><code>torch.optim</code> 中实现了深度学习中绝大多数的优化方法，例如 RMSProp、Adam、SGD 等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建一个优化器，指定要调整的参数和学习率</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练过程中</span></span><br><span class="line"><span class="comment"># 先梯度清零（与 net.zero_grad() 效果一样）</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target.float())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 在 Tensor 上的所有操作，&lt;code&gt;autograd&lt;/code&gt; 都能为它们自动提供微分，避免了手动计算导数的复杂过程，只需要设置 &lt;code&gt;tensor.requires_grad=True&lt;/code&gt; 即可。&lt;/p&gt;
&lt;p&gt;Input:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.ones(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 上一步等价于&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# x = t.ones(2, 2)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# x.requires_grad = True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y = x.sum()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(y)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(y.grad_fn)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y.backward()   &lt;span class=&quot;comment&quot;&gt;# 反向传播计算梯度&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://wx1.sinaimg.cn/mw690/79225320gy1fx7rvm1w6sj214e05ojrw.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;grad&lt;/code&gt; 在反向传播过程中是累加的，每一次运行反向传播，梯度都会累加之前的梯度，因此反向传播之前需要把梯度清零。&lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;y.backward()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y.backward()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 以下划线结束的函数是 inplace 操作，会修改自身的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x.grad.data.zero_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;y.backward()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.grad)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://wx4.sinaimg.cn/mw690/79225320gy1fx7s1w5mz3j213y05imx9.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（一）</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch1/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch1/</id>
    <published>2018-11-14T07:52:00.000Z</published>
    <updated>2018-11-15T06:37:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>Tensor</code> 可以认为是一个高维数组，可以使用 GPU 进行加速。   </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">x = t.Tensor(<span class="number">5</span>, <span class="number">3</span>)    <span class="comment"># 只分配了空间，未初始化</span></span><br><span class="line">x = t.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># 使用 [0, 1] 均匀分布随机初始化二维数组</span></span><br><span class="line">x = t.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x)</span><br><span class="line"><span class="comment"># 查看 x 的形状</span></span><br><span class="line">print(x.size())</span><br><span class="line"><span class="comment"># 查看 x 中列的个数，两种写法等价</span></span><br><span class="line">print(x.size()[<span class="number">1</span>])</span><br><span class="line">print(x.size(<span class="number">1</span>))</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7j8w2u2qj20z9092wf3.jpg" alt="运行结果"><br><code>torch.Size</code> 是 tuple 对象的子类，因此它支持 tuple 的所有操作，如 <code>x.size()[0]</code> 等。<br><a id="more"></a></p><p>2 . 加法的三种写法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">y = t.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(x + y)   <span class="comment"># 加法的第一种写法</span></span><br><span class="line">print(t.add(x, y))   <span class="comment"># 加法的第二种写法</span></span><br><span class="line"><span class="comment"># 加法的第三种写法：指定加法结果的输出目标为 result</span></span><br><span class="line">result = t.Tensor(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">t.add(x, y, out=result)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'最初 y'</span>)</span><br><span class="line">print(y)</span><br><span class="line"><span class="comment"># 普通加法，不改变 y 的内容</span></span><br><span class="line">y.add(x)</span><br><span class="line">print(y)</span><br><span class="line"><span class="comment"># inplace 加法，改变 y 的内容</span></span><br><span class="line">y.add_(x)</span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>函数名后面带下划线 <code>_</code> 的函数会改变 Tensor 本身。</p><p>3 . Tensor 与 Numpy。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x[:, <span class="number">1</span>]   <span class="comment"># 选取 x 的第一列所有内容，与 Numpy 相似</span></span><br></pre></td></tr></table></figure><p>Tensor 和 Numpy 的数组之间的互操作非常容易且快速，对于 Tensor 不支持的操作，可以先转换为 Numpy 数组处理，之后再转换回 Tensor。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensor <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = t.ones(<span class="number">5</span>)   <span class="comment"># 新建一个全 1 的 Tensor</span></span><br><span class="line">print(a)</span><br><span class="line">b = a.numpy()   <span class="comment"># Tensor ----&gt; Numpy</span></span><br><span class="line">print(b)</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line">print(a)</span><br><span class="line">b = t.from_numpy(a)   <span class="comment"># Numpy ----&gt; Tensor</span></span><br><span class="line">print(b)</span><br><span class="line"><span class="comment"># Tensor 与 Numpy 共享内存</span></span><br><span class="line">b.add_(<span class="number">1</span>)    <span class="comment"># 以 _ 结尾的函数会修改 Tensor 自身</span></span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx7kkcurzdj214m05mjrm.jpg" alt=""><br>Tensor 和 Numpy 对象共享内存，如果其中一个变了，另外一个也会随之改变。</p><p>4 . 如果想获取某一个元素的值，可以使用 <code>scalar.item</code>，直接 <code>tensor[idx]</code> 得到的还是一个 tensor，一个 0-dim 的 tensor，一般称为 scalar。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scalar = b[<span class="number">0</span>]</span><br><span class="line">print(scalar)</span><br><span class="line">print(scalar.size())  <span class="comment"># 0-dim</span></span><br><span class="line">print(scalar.item())</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx2.sinaimg.cn/mw690/79225320ly1fx7m1ulcjhj213s02uaa3.jpg" alt="">  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># 注意和 scalar 的区别</span></span><br><span class="line">tensor = t.tensor([<span class="number">2</span>])</span><br><span class="line">print(tensor)</span><br><span class="line">print(tensor.size())</span><br><span class="line"><span class="comment"># 只有一个元素的 tensor 也可以调用 tensor.item()</span></span><br><span class="line">print(tensor.item())</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx7misoww4j213x02yq2v.jpg" alt=""></p><p>5 . <code>t.tensor()</code> 总会进行数据拷贝，新 tensor 和原来的数据不共享内存，如果想要共享内存的话，建议使用 <code>torch.from_numpy()</code> 或者 <code>tensor.detach()</code> 来新建一个 tensor，二者共享内存。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tensor = t.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">old_tensor = tensor</span><br><span class="line">new_tensor = t.tensor(old_tensor)</span><br><span class="line">new_tensor[<span class="number">0</span>] = <span class="number">1111</span></span><br><span class="line">print(old_tensor)</span><br><span class="line">print(new_tensor)</span><br><span class="line">new_tensor = old_tensor.detach()</span><br><span class="line">new_tensor[<span class="number">0</span>] = <span class="number">1111</span></span><br><span class="line">print(old_tensor)</span><br><span class="line">print(new_tensor)</span><br></pre></td></tr></table></figure></p><p>Output:<br><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx7mtib98oj214103ojrh.jpg" alt=""></p><p>6 . Tensor 可以通过 <code>.cuda</code> 方法转换为 GPU 的 Tensor，从而享受 GPU 带来的加速运算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在不支持 CUDA 的机器上，下一步还是在 CPU 上运行</span></span><br><span class="line">device = t.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> t.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">x = x.to(device)</span><br><span class="line">y = y.to(device)</span><br><span class="line">z = x + y</span><br><span class="line">print(z)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;Tensor&lt;/code&gt; 可以认为是一个高维数组，可以使用 GPU 进行加速。   &lt;/p&gt;
&lt;p&gt;Input:&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.Tensor(&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)    &lt;span class=&quot;comment&quot;&gt;# 只分配了空间，未初始化&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.Tensor([[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;], [&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;]])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 使用 [0, 1] 均匀分布随机初始化二维数组&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;x = t.rand(&lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 查看 x 的形状&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.size())&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 查看 x 中列的个数，两种写法等价&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.size()[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(x.size(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Output:&lt;br&gt;&lt;img src=&quot;http://wx4.sinaimg.cn/mw690/79225320gy1fx7j8w2u2qj20z9092wf3.jpg&quot; alt=&quot;运行结果&quot;&gt;&lt;br&gt;&lt;code&gt;torch.Size&lt;/code&gt; 是 tuple 对象的子类，因此它支持 tuple 的所有操作，如 &lt;code&gt;x.size()[0]&lt;/code&gt; 等。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>自学编程那些事儿</title>
    <link href="http://blog.keeplearning.group/2017/07/31/2017/07-31-learn-program/"/>
    <id>http://blog.keeplearning.group/2017/07/31/2017/07-31-learn-program/</id>
    <published>2017-07-31T11:47:00.000Z</published>
    <updated>2018-11-07T08:29:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>我本人是从机械专业转过来学习编程的，在这个过程中我大概跟大多数半路出家的人一样，迷茫过，纠结过，努力过，也怅然若失过，同时也有在意想不到的拐角处收获到额外的惊喜。这一路不能说顺顺利利，但也算是跌跌撞撞地走过来了。虽然我的水平还远不至于到了能给别人指点一二的地步，不过我倒不介意把我认为最重要的几点浅薄的看法分享给大家。</p><a id="more"></a><h3 id="选择好一个方向很重要"><a href="#选择好一个方向很重要" class="headerlink" title="选择好一个方向很重要"></a>选择好一个方向很重要</h3><p>我相信大多数想入门编程并且未来想在 IT 领域中找到一份工作的人都听过几个热门的词，一会儿是人工智能（AI），一会儿是大数据，一会儿是 Android 开发 iOS 开发之类的，不然就是运维，或者前端、后端之类的。在最开始选择进入 IT 领域的时候，你必须花时间在这些五花八门的方向中找出一个你最感兴趣并且认为自己能够持之以恒地坚持学下去的方向。你可以向有经验的人了解这些方向都是干嘛的，找不到人的话也可以通过互联网求助，譬如知乎等等各大网络平台，搞清楚了这些方向是干嘛的，接下来就是好好规划自己的学习路线，第一步做什么，第二步做什么等等等等，现在网络资源这么发达，想找到自己的学习路线并不难，况且还有慕课网、coursera、edx 等在线教育平台可以利用。总之，利用一切你可以利用到的手段，选择一个你真正感兴趣的方向，即便为了钱而感兴趣也行，只要你认为碰到困难的时候你能坚持下去。  </p><h3 id="几个方向都精通只是锦上添花"><a href="#几个方向都精通只是锦上添花" class="headerlink" title="几个方向都精通只是锦上添花"></a>几个方向都精通只是锦上添花</h3><p>在寻找学习方向的时候，可能很多人会觉得自己好像对好几个方向都非常感兴趣，这个时候怎么办呢？我想对你说的是，虽然方向很多，但是在最开始学习的时候，你必须选择并且仅仅只能选择其中的一个，付出努力持之以恒地把它学好即可，其他的，无论你再有兴趣，也应该暂时地舍弃掉，不要去管它。我这么说主要是基于以下两点：</p><p>第一，你在真正找工作的时候，是凭借你的一技之长去投岗位的，请注意，一定是一技之长，每个公司列出那么多岗位给应聘者，并没有要求应聘者同时兼顾好几个方向的技能，相对于什么都知道一点但是什么都不够深入的人，他们一定是要那些在某一个方面有所长能够给他解决这一个方面的实际问题的人，至于其他方面，他们可以再去招聘那些在其他方面有所长的人，而不需要一个人兼顾这么多，大多数时候，一个人来做反而做不好；</p><p>第二，人每天的精力是有限的，无论你用它做了什么，每天用完即没有，我曾经对自己专注在一件事情上的时间做了一个粗略的统计，我指的是严格专注在一件事情之上，减去那些喝水、散步、走神的时间，一天顶多顶多只有 6 个小时，而且每周并不能做到每天都能有这么高的效率，通常工作日五天能有两天有这个效率就非常不错了，而且人会感觉非常疲劳。一天 24 小时，全身心铺在一件事情上的时间只能有 6 个小时，换而言之，只有 1/4 的有效工作时间，事实上 5 天的工作日只有顶多只有两天有这么高的效率，这个数字说出去是很吓人的，但是对不起，这真的就是事实，我不知道大家统计过没有，虽然每个人的精力略有差别，但是我相信这个数字的上下浮动不会太大。而随着你在某一个技术方向深入程度的增加，你一定会觉得你不会的东西需要弄明白的东西太多了（当然你也许会觉得有些东西我不需要弄得那么明白和熟练，用到的时候查一查会用就行，下文我会说明为什么这么想是有问题的），你的精力有限，想要弄明白这么多的东西，进一步做到让你的一技之长足够长，真的很难，这是我的切身体会，你真的不太可能让你的一技之长足够长的同时又能兼顾好其他方面的技术。</p><p>事实上，我见过在某一个技术方向足够熟练之后，由于个人的兴趣或者个人的需要在其他方面也做得比较好的人，但是，我还真没有见过同时一下子从很多个技术方向出发最后都能够做得非常非常好的人，我相信这种人即便有，也是在少数中的少数吧。先做好自己的一技之长，还有一个原因是，有很多技术方向是有共通点的，你做好了这个方向的一技之长，将来有需要精通其他方向的技术之后，触类旁通会相对容易一些。行业里所说的“全栈工程师”，这大多是环境要求，后来技术背景不断更换之后所带来的一个结果，而不是最初出发的一个目的。总而言之一句话，几个方向都精通只是锦上添花，但这并不要求是必须的，最初学习的时候，你最好只选择一个方向进行突破。  </p><h3 id="专注、持续、有效的投入"><a href="#专注、持续、有效的投入" class="headerlink" title="专注、持续、有效的投入"></a>专注、持续、有效的投入</h3><p>在最初选择技术方向的时候，我刚说了最好专注在一个方向上，事实上不仅如此，在你严格按照学习路线图进行学习的时候，你接触到某一方面的知识，譬如 Linux shell 脚本编程，那么，你也需要在短期内只专注 shell 怎么写，否则，一个基础没打牢固就跑去弄别的，最后的结果只能是捡了芝麻丢了西瓜，狗熊掰棒子而已。明确学习路线之后，需要一步步按照学习路线的规划，稳扎稳打地一个个把每一步要求的知识过关。</p><p>一个技术岗位的价值，首先取决于这个技术方向本身，如果你选择的技术方向是一个急需紧缺的类型，那么这个技术方向肯定价值会比较高（说的俗一点，就是钱多）；其次取决于选择这个技术岗位的人本身的不可替代性，也就是说，一个技术岗位的人能够把工作做得越出色，别人越难以取代，这个人的价值也就越大。那么，怎样变得越来越不可替代呢？或者，换句话说，怎样把工作做得比大多数人都优秀呢？那就需要在很多别人难以下功夫的地方多下功夫。譬如很多重要的东西，别人由于畏难而退却而你搞明白了，别人忽略的一些重要的细枝末节你掌握了，等等等等，就是静下心来把别人啃不好的硬骨头给啃下来，并且这些硬骨头还是能有效提高你能力的知识点。这也就是我上文说的需要把一些别人不在意的同时又很重要的一些东西弄得比较明白的原因，因为这是你与其他人的差别所在，你做的越好，你的不可替代性就会越强，从而你的价值就会越大。做到这些，就需要专注、持续、有效的投入了，所谓专注就是一心一意做好一件事情，do one thing and do one thing well，所谓持续就是不能三天打鱼两天晒网，大家估计都有体验，如果学习一门技术，刚入门然后断了一段时间，再次投入进去感觉很多东西特别陌生，捡起来重新开始花的时间代价会比较大，长此以往会陷入低水平重复的怪圈，所以我这里强调持续性，打铁要趁热，不要轻易被其他的事情所打断，所谓有效的投入指的是不能低水平重复，每一次的努力都要给自己带来实实在在的成长。</p><p>一万小时理论被认为是成为某个领域专家所花费时间的及格线，也是其必要条件，但如果只是低水平地做一些简单的重复工作的话，那再怎么样也成不了杰出专家，我们需要啃一些能有效地提高我们水平的硬骨头。顺便说一下一万小时，我算了一下，如果每天 6 个小时的投入的话，10000 小时的花费大概需要 1667 天，大概需要 4.5 年的时间，这还不包括周末和各种假日。所以，在一个领域内成为专家级的水平需要付出多少大概可想而知了，沉下心来戒骄戒躁好好努力才是王道。  </p><h3 id="系统的知识体系很重要"><a href="#系统的知识体系很重要" class="headerlink" title="系统的知识体系很重要"></a>系统的知识体系很重要</h3><p>做事情专业还是业余，成系统的知识在其中起着非常重要的作用。那么什么是专业呢？这不仅仅是面对已知的问题你能提供有效的解决方案，还有就是针对未知的问题，你能通过有效的规范的步骤，一步步地找出问题症结所在，一点点地让问题浮出水面，或者至少，你能提供一些有利于问题解决的有价值的参考出来。而业余的人，解决问题有时候就跟碰运气一样，有时候能得到解决，有时候却不怎么灵。在一个领域内比较专业的人，他的努力所带来的后果在很大程度上是可以预期的，究其原因，就是其背后有系统的知识体系作为支撑。拿算法来说吧，很多人觉得不重要，怎么说呢？重不重要的结论我先不给出来。我只想说，你认真地掌握了它，再来谈重不重要，得出的结论会更靠谱。实际上，在大多数编程场景中，很少让你写出具体的算法，但算法思想的应用还是比较多的，受过良好算法训练的人，拿来一些实现同样功能代码，他能很快分辨出代码是写的好还是烂，这就是专业与不专业的区别，而对于算法思想的领悟，又需要一定的计算机数学基础为铺垫，这背后就是一整套的系统知识体系。所以说，系统的知识体系，直接关系到你专业还是不专业，而你专业还是不专业，背后又关系到你是不是不可取代的，你的价值是多少。  </p><h3 id="到底要不要自学编程"><a href="#到底要不要自学编程" class="headerlink" title="到底要不要自学编程"></a>到底要不要自学编程</h3><p>经常听到有人问，我不是计算机科班出身的，可是我对编程很感兴趣，到底要不要学编程？或者，我现在年纪也不小了，二十四五岁的高龄了（我笑而不语），学编程还来得及吗？等等诸如此类的要不要学编程问题。我首先是不太愿意回答这样子的问题，原因是我觉得这些人中有相当一部分在提出这个问题的时候心里其实早就有答案了，就是觉得自己可能不太适合，而且岁数比较大等等各种能够说服自己放弃的理由，即便被我说服了，一个脑袋发热一头扎了进去，一碰到困难马上就觉得自己不合适了，想放弃了。对此，我只能说，你不仅仅只是不适合学习编程，你干其他的事情估计也不太可能干得好。我总认为，只要自己努力和积极向上，那么无论处于什么样糟糕的境地，都能找到自己的出路，没有这个自信人生简直都不要活了。有了这个自信的话，对于很多自己内心真正想从事的一些工作，只要是积极乐观的，又何妨一搏呢？梁漱溟先生不也是自学成才，当上了北大教授吗？一般人像他那样子才中学毕业，在这条路上估计早就自我放弃了，他不但通过自学成为了北大教授，而且还教出了像冯友兰、朱自清等一批优秀的学生。毛泽东主席在年轻的时候就写下了这样子的诗：自信人生两百年，会当水击三千里。当然了，不是一般人都有伟人一般的豪迈意气，但是对于我们每个人自己的人生，豪迈豪迈的勇气我们还是应该有的。总之，大胆去选择自己的梦想，只要是积极向上的，尽可努力去拼搏，人立于天地之间，无论现实怎么糟糕，都能找到自己的出路，反正，我就这么自信。  </p><p>最后，祝学习愉快。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我本人是从机械专业转过来学习编程的，在这个过程中我大概跟大多数半路出家的人一样，迷茫过，纠结过，努力过，也怅然若失过，同时也有在意想不到的拐角处收获到额外的惊喜。这一路不能说顺顺利利，但也算是跌跌撞撞地走过来了。虽然我的水平还远不至于到了能给别人指点一二的地步，不过我倒不介意把我认为最重要的几点浅薄的看法分享给大家。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>说说人工智能、大数据、医疗和教育（三）</title>
    <link href="http://blog.keeplearning.group/2017/07/23/2017/07-23-ai-bigdata/"/>
    <id>http://blog.keeplearning.group/2017/07/23/2017/07-23-ai-bigdata/</id>
    <published>2017-07-23T02:50:00.000Z</published>
    <updated>2018-11-07T08:31:08.000Z</updated>
    
    <content type="html"><![CDATA[<p>在正式开始今天的主题之前，我想跟大家说的是，我们真的处在了一个非常非常好的时代，很多行业里的人士普遍认为，中国仍然有很好的长期发展的机遇，从 1990 年开始的 40 年间，中国会超越美国成为世界第一大经济体，并且，如果政治稳定的话，之后还会伴随着这个浪潮的惯性继续向前发展很长一段时间。而我们这帮 90 后们，算算时间点的话，恰恰会是这个高速发展时代的中坚力量。我们运气很好地碰在了一个高速发展的国家里，并且，又同样运气很好地碰在了人类历史上第四次技术革命发展的转折点 —— 从信息时代向智能时代转变的关键点上。《必然》一书的作者凯文凯利认为，以后几十年里人类生活离不开的产品还远远没有被发明出来。其实一个人一生中很难碰到一次技术革命的浪潮，而我们居然在一个青春年华的合适年纪里神奇地遇上了，这是我们的幸运。对于这一波浪潮而言，我们可以将其看成是每个人一生中只能玩一次的游戏，错过了就是错过了，再没机会，到底是眼巴巴看着还是投身其中闯荡一番？我相信很多人心中其实都有了答案。那么，时代的机遇究竟在哪里呢？这是今天的文章重点谈的内容。首先，既然我们是处于第四次技术革命的关键转折点上，我们不妨来回顾一下之前的三次技术革命里都发生了什么。</p><a id="more"></a><p>人类历史上的第一次工业革命发生在 18 世纪末的英国，它是以蒸汽机的发明为标志的，因此也叫做蒸汽机的革命，我们都知道蒸汽机的发明者是瓦特。但是实际上，在瓦特之前蒸汽机就有了，瓦特只是和当时著名的工厂主马修博尔顿（关于马修博尔顿见<a href="https://zh.wikipedia.org/wiki/%E9%A9%AC%E4%BF%AE%C2%B7%E5%8D%9A%E5%B0%94%E9%A1%BF" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E9%A9%AC%E4%BF%AE%C2%B7%E5%8D%9A%E5%B0%94%E9%A1%BF</a>） 一起合作，改良了传统的效率低下的纽科门蒸汽机，在此基础之上发明了一种万用蒸汽机。瓦特的发明奠定了第一次工业革命的基础，并且， 他还发展出马力的概念以及以他名字命名的功率的国际标准单位——瓦特。最初的纽科门蒸汽机只是供英国的一些矿井使用的，不但效率低下，而且适用性很差。而瓦特发明的万用蒸汽机可以适用于诸如纺织、陶瓷等等许多在之前大部分工作只能通过人力操作的行业。很多原来存在的产业，只要运用上蒸汽机，就会带来生产效率的极大提升。第一次工业革命给人类社会带来的积极影响，可以用马克思的一句话来说明：“资产阶级在其不到 100 年的阶级统治中所创造的生产力，比过去一切时代创造的全部生产力还要多、还要大”。当然我们这里讲的是积极的一方面。消极的一方面是，在纺织、瓷器等等蒸汽机能够渗透到的领域，一些传统的小作坊被更有效率、更标准化、生产出的产品质量更好的蒸汽机所取代，之前的经济结构被摧毁，很多人纷纷破产，从中产阶级一下子沦为了赤贫。拥有生产资料的工厂主们为了便于竞争，就雇佣一些低工资的童工，或者随意延长工人的劳动时间。也就在那个时代，英国发生了空前绝后工人运动，催生了马克思主义。技术的进步给当时的英国社会带来了很大的社会动荡，最初诅咒它的人比拥抱它的人多很多。最初享受到技术革命带来的好处的，只是瓦特和像博尔顿这样懂得运用蒸汽机技术的工厂主，那个时候的英国贫富分化非常严重。</p><p>那么，英国是怎么消除蒸汽机革命所带来的负面影响的呢？</p><p>那就是开拓全球殖民地，推行自由贸易，进行资本输出。实际上我们也可以看到，那个时候的很多次战争，就中国而言，总是动不动让中国开放这个港口那个港口什么的，这就是推行自由贸易的结果。实际上通过这种手段，也让第一次工业革命的成果得以全球化。    </p><p>第二次工业革命是电的革命，同蒸汽机的革命一样，电的使用在原来存在的很多领域中也造成了革命性的冲击，带来了生产效率的极大提升。并且，最初受益的也只是少部分懂得如何使用电并且能够拥有电的人，新的技术的应用导致了大量的失业潮和破产潮，那个时候的很多代表性的人物，如通用电气（GE）公司的创始人爱迪生、AT&amp;T的创始人亚历山大贝尔、福特公司的创始人亨利福特、奔驰汽车的创始人卡尔本茨（德国），不少到现在也依然是很多创业家心中的偶像。实际上，那个时候，在美国的贫富分化程度达到了北美殖民以来的最高点，不少激进的工人运动也发生在那个时期，石油大王洛克菲勒聚集的财富占了全美国的 1%，说个题外话，很多人对洛克菲勒的认识可能只是知道他很有钱并且做慈善出资建立了北京协和医院和协和医学院，当年协和医院的奠基的时候，洛克菲勒的儿子小洛克菲勒，坐了个把月的轮船，亲自来到了北京参加了这个奠基仪式，当然现在的北京协和医院已经跟洛克菲勒家族没啥关系了（早就充公了）。美国人抵消第二次工业革命的消极影响是通过开发西部广袤的还未被开发出来的处女地。而德国人就没那么幸运了，空前的社会矛盾导致他们把希特勒代表的纳粹主义分子推上了台（所以中国的制度好还是国外的制度好？真的不好讲）。实际上，美国19世纪中期南北战争北方战胜，扫除了发展资本主义的障碍——奴隶制，促使美国在第二次工业革命中采用新技术，用了大约 30 年的时间一举超过了英国，成为了世界第一大经济体。  </p><p>第三次技术革命是二战之后以计算机技术的兴起为基础发展起来的信息革命。实际上在过去 30 年的时间里，真正上来说受益于信息时代的只有美国和中国，这两个国家贡献了全球一半以上的 GDP 增长，对于美国来说不需要讲太多，诞生了一大批信息时代的明星公司：Google、Apple、Microsoft等等等等（了解一下 Google 的话可以看看我公众号里之前的一篇文章《Google 传奇》），对于这些公司的创始人来说，他们都在自己年富力强的时候幸运地赶上了信息革命的大潮，与我们现在很多的 90 后赶上智能革命的浪潮一样（偷笑中）。不过对于美国而言，信息革命技术的进步造成的社会问题也是显而易见的，美国人的失业率比较高，有很多游手好闲无所事事吃低保的人，因为他们的工作机会被新的技术进步所取代了。譬如特斯拉汽车公司，在它的汽车装配间里全部都是一个个软件技术操纵的机械手臂，几乎没有工人。为此特斯拉的门前总能招来一些抗议者，他们抗议特斯拉不给他们提供工作机会而用一个个的机械手臂去代替他们，不过这个大的趋势真的是不可逆转不以人的意志为转移。去年美国总统大选，特朗普一直在说要解决美国人的就业问题，因为很多很多人没有了工作机会。与此相比，同为贡献 GDP 增长最多的两个国家，中国则要幸运得多，这是因为中国的基础太薄弱了，从 1979 年改革开放起，中国用了 30 多年的时间，走完了西方国家 200 多年的工业化道路，并且同时进行了信息革命，与整个时代一起步入了信息时代的末期智能时代的初期。在这期间的人力资源的消耗是非常大的，所以在中国基本上没有看到像美国那样子的情况发生。不过在智能时代中国可就没那么幸运了，所以诸位真的要未雨绸缪啊。实际上现在就可以看出一些迹象了，大家都知道中国是世界工厂，譬如富士康，像 iPhone 手机之类的电子产品大都是在富士康生产和组装的，这些精密的电子产品对于组装工人的技术性要求还是比较高的，但是，有消息称，富士康正在加紧研制智能机器人，将这些智能机器人投放到生产产线上，取代现在的技工的工作。    </p><p>其实不仅仅只是这些取代，我昨天的文章里也谈过，像医生之类的工作，也有可能被机器大部分取代掉，并且昨天的文章里也说了详尽的理由。其实不止医生啊，像记者编辑，有些人可能不知道，实际上在美国，《纽约时报》之类的报纸，很多的文章都是机器写的，通过大量的文章训练出一个模板，然后把新闻信息输入进去，机器就把文章写出来了。未来不止医生、记者之类的行业，实际上将现有的很多很多行业，融于智能时代的思维来思考的话，很多很多的东西都将产生变化，这些变化会像历史上几次技术革命所带来的影响一样。  </p><p>从以上历史的回顾之中我们可以知道，新的技术进步所带来的生产力的提升，会让很多猝不及防的人因此丢了工作，刚开始受益的也只是很少的那一部分掌握了新技术并且懂得如何应用新技术的群体。与此同时我们也应该看到，新的技术进步在取代原有的工作的同时，一定会诞生一些新的职业，现在的很多职业，譬如程序员等，在以前是不存在的。那么为什么历史上新的职业旧时代的人却做不了呢？因为学习成本高并且难以适应，学习一门新的技术，不仅仅是学习这些东西本身而已，还有对于过去职业习惯所养成的一些思维定势的改变，而这，对于过去从事一项熟悉的工作几十年的人来说，是非常困难的。这就涉及到我昨天说的今天想讲的一个话题了 —— 教育。  </p><p>其实在新的时代背景下，大家都找到了未来教育形式的一个方向，即互联网教育。像 Coursera、edx、网易公开课等在线教育平台，把一些世界名校的课程放在了互联网上，让大家都可以便捷地享受到以前只有少数人才能得到的知识服务，那么，我有一个疑问就是，这些宝贵的东西放在互联网上，为什么得不到爆炸式的反馈效果呢？选择在线学习的人依然是非常非常少的，原因在哪儿呢？还有，智能时代以后取代的是现在很多人所从事的一些智能化的工作，譬如算算表单的会计呀，画画图纸的设计师呀，等等这些工作在未来一定程度上甚至全部都可以被机器所取代掉，由此看来现在的教育教授的很多东西在未来是没有用的，那么什么样子的教育可以在一定程度上减轻未来随之到来的失业潮呢？以后究竟需要什么样的教育呢？而为了应付未来教育上的需求，我们现在应该做好哪些事情呢？对于这些问题，我没有特别好的答案，只有一些粗浅的思考分享给大家，欢迎对这些问题有见解的人通过文末的方式来和我交流。我觉得，如果真正解决了我上面所提的那些问题，那么未来教育的问题，也肯定就解决了。  </p><p>为什么互联网上放了那么多好的教育资源，但是选择在线教育的人还是很少？我觉得其中有一方面的问题就是教育的形式的问题。我们从小习惯了课堂上互动式的教育，也习惯了同学之间知识的交流，如果能在在线教育的形式中融于课堂仪式感和交流互动的元素，譬如利用 VR 技术营造一种课堂的氛围等等，那么在线教育的形式一定能够被越来越多的人所接受。  </p><p>什么样子的教育可以减轻未来随之而来的失业潮？这个问题的答案我不知道，更诚实的一点的说法是，我有一点看法，但是由于过于粗浅经不起斟酌，所以也就不说出来了。不过，我觉得过去之所以会出现那么大的失业潮，部分原因是因为不能教授旧时代的人新时代的思维和新时代的技能，其中有一个很重要的原因之一是，那个时候的教育被局限的很厉害，只能通过大学里校园教育的方式获得，而对于未来，互联网触手可及，这个限制可以说不存在了，所以，我觉得可以用互联网教育的方式更快地传播新的技能以减轻失业潮造成的影响。  </p><p>未来需要什么样子的教育呢？机器智能所取代的是人现在所从事的很多智能化的工作，也就是说，现在人所从事的很多技能教育将来都不需要了。既然很多技能教育未来不需要，那么我认为其实未来教育教人的东西，应该是更让人能获得快乐和精神享受的东西，譬如诗词歌赋、绘画艺术、历史哲学等等，人的双手被解放了之后，很多工作完全不需要人来做了，所以人会更想追求精神层面的快乐。  </p><p>以上就是我对教育方面的一些粗浅的认识，欢迎持有观点的朋友通过下面的方式来和我交流。我一直觉得，人的生活一定要健康、幸福和快乐，并且我乐于用自己的所学给更多的人带来健康、幸福和快乐，所以其实从第二篇文章开始，我主要谈了未来智能社会里医疗和教育的话题，因为，医疗能带给人健康，而教育，能使人快乐。我们每天所写的并不仅仅只是一行行有趣的代码而已，而且这些东西，在未来，能让更多的人健康、幸福和快乐起来。我想在未来，在医疗和教育领域，我要么成为一个开拓者，要么加入别人成为追随者，因为这两样东西，我感觉是渗透在我的灵魂深处让我真正想去做的事业。希望在智能时代我们这帮 90 后们都能把握自己的机遇，找到自己一生的事业，无负于一个伟大的时代对我们最有价值的馈赠吧！  </p><p>谢谢耐心看完这三篇文章的人，对于这个话题，以这三篇文章的篇幅，到此告一段落。以后有机会我们可以接着聊。  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在正式开始今天的主题之前，我想跟大家说的是，我们真的处在了一个非常非常好的时代，很多行业里的人士普遍认为，中国仍然有很好的长期发展的机遇，从 1990 年开始的 40 年间，中国会超越美国成为世界第一大经济体，并且，如果政治稳定的话，之后还会伴随着这个浪潮的惯性继续向前发展很长一段时间。而我们这帮 90 后们，算算时间点的话，恰恰会是这个高速发展时代的中坚力量。我们运气很好地碰在了一个高速发展的国家里，并且，又同样运气很好地碰在了人类历史上第四次技术革命发展的转折点 —— 从信息时代向智能时代转变的关键点上。《必然》一书的作者凯文凯利认为，以后几十年里人类生活离不开的产品还远远没有被发明出来。其实一个人一生中很难碰到一次技术革命的浪潮，而我们居然在一个青春年华的合适年纪里神奇地遇上了，这是我们的幸运。对于这一波浪潮而言，我们可以将其看成是每个人一生中只能玩一次的游戏，错过了就是错过了，再没机会，到底是眼巴巴看着还是投身其中闯荡一番？我相信很多人心中其实都有了答案。那么，时代的机遇究竟在哪里呢？这是今天的文章重点谈的内容。首先，既然我们是处于第四次技术革命的关键转折点上，我们不妨来回顾一下之前的三次技术革命里都发生了什么。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
  <entry>
    <title>说说人工智能、大数据、医疗和教育（二）</title>
    <link href="http://blog.keeplearning.group/2017/07/22/2017/07-22-ai-bigdata/"/>
    <id>http://blog.keeplearning.group/2017/07/22/2017/07-22-ai-bigdata/</id>
    <published>2017-07-22T02:50:00.000Z</published>
    <updated>2018-11-07T08:32:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>在昨天的文章中我们详细说明了什么是机器智能，用数据驱动获取机器智能的方法，谈了机器智能的一个必要因素就是大数据，并且讲了大数据对于人类生活的预测指导作用，今天我们在此基础之上，主要来谈谈医疗的内容，基于一篇文章的篇幅所限，明天我们再来谈教育。  </p><p>当今医疗领域所面对的问题主要有三个方面：第一，看病贵；第二，医疗资源分配不均衡，好的医疗设备和技术精湛的名医大都集中在经济比较发达的地区，经济不发达地区的人面临的不仅仅是看不起病的问题，同时还有生了重病找不到好的医生的问题；第三，很多疾病，譬如癌症，人类仍然没有找到可靠的能够使其得到稳定治愈的方法。其实针对这三个方面的问题，大数据和机器智能的发展都能够提供一个理论上行得通的解决方案。下面我们一一来谈。</p><a id="more"></a><p>首先是看病贵的问题。那么为什么看病会如此昂贵呢？大致有以下三个方面的因素。  </p><p>第一，医学人才培养的成本很高，对于怎样成为一个顶尖的医学技术人才来说，就中国而言，维基百科中是这么描述的：  </p><blockquote><p>从大学第一个本科开始的。一般有五年制，七年制和八年制，分别授予医学学士，医学硕士和医学博士的学位。七年制和八年制统称为长学制。获得医学学士学位的毕业生，可以参加执业医师考试。五年制毕业的医学学士，可以继续攻读医学硕士及博士，两者同样需时三年。七年制毕业的医学硕士，可以继续攻读医学博士，如果攻读本校的医学博士，可以只参加转博考试，通过考试及面试后，再读3年，可以获得医学博士学位。  </p></blockquote><p>也就是说，从高中毕业算起，到获得本科学士学位再到参加执业医师考试获得从业资格，至少需要 5 ~ 6 年的时间，而这种技术水平也仅仅只是从医资格的起步水平，大多数情况下，这种技术水平的医生是很难看好病治好病的。获取博士学位，达到高级医师的水平，高中毕业后最短需要接受 8 年、最长需要接受 11 年的医学知识技能教育，这还不包括毕业获得从业资格之后经过大量临床经验和病例积累增长技术水平所用的时间。这么来看，一个优秀医师培养起来不仅仅是耗费了大量青春年华的宝贵时间，还有大量的金钱投入。事实上在美国以及其他国家也与此类似，美国有个笑话是这么说的：一个人从最初接受医学教育到获得外科医生的行医执照的时候，他的中学同学已经结婚生孩子并且事业略有所成了，而他才找到第一份 50 万美元年薪的工作。当然在美国第一份工作就能获得 50 万美元年薪的外科医师是很少的，事实上有不少人在这个过程中要么读不下去中途放弃了，要么读了个半吊子什么都没学好。年薪 50 万美元什么概念呢？我们可以和美国总统的工资比一下就知道到了，这相当于美国总统年薪的 1.25 倍（2016年美国总统的年薪是 40 万美元）。这是相当可观的一笔收入，在中国估计也是不相上下的。从培养一个高级医师的艰难过程来看的话，一个医生的年薪值这么多钱也是可以理解的。   </p><p>那么针对以上所说的医学人才培养成本高的问题，从机器智能和大数据的角度来看，有什么样的解决方法呢？事实上，我们在平时看病的时候，总喜欢找一些年长的医生，这背后的逻辑其实很简单，从业历史长，那么见的病例一定多，经验一定很丰富，最后做出诊断和治疗的精确性也会越高。也就是说，一个医生的医疗水平，是跟他见过的临床医疗病例的数量有正相关关系的。由此我们也可以想到，如果我们给定一个机器学习的模型，给它提供大量的病例数据以训练参数，那么它是不是也可以变得很聪明直至最后超过大部分医生的水平呢？答案当然是肯定的。事实上，在 2012 年的时候，当时有一个新闻不知道大家还记不记得，美国一个高中生，用 760 万例乳腺癌病例，训练出了一个可以精确定位乳腺癌癌细胞位置的算法，其判断准确率达到了 96% 以上，超过了外科医生的水平，这就是一个典型的例子。在 IBM，通过机器学习的方法训练出来了一个沃特森机器人，现在的语音识别技术赋予了这个机器人与人交流的能力，而这个机器人给人看病的水平，相当于一般中级医师，那么为什么这个机器人给人看病的水平还不能超过现在的大部分高级医师呢？因为人类积累的可供机器阅读的电子病例的数量还不够多，换句话说，机器比较笨，它需要大量大量的，比高级医师所见的多很多的数据，才能通过学习保证自己的水平超过高级医师。那么这些电子病例数据量的积累速度是怎样的呢？IBM 给出的数据是，每隔 73 天会翻一番，这种增长速度会保持到 2020 年。</p><p>因此，从这个方面来看的话，机器给人看病超过医师的水平，也只是时间早晚的问题，并且这个趋势一旦形成就基本上不可能逆转，没有任何一个医生可以与它相匹敌了，因为任何一个人的生命都是有限的，在有限的生命中，任何一个医生所见过的病例的数量都不可能比机器多，水平自然会没有机器高。这是未来的一个大趋势。既然在未来可以通过机器给人看病，并且诊断和治疗的精确度还很高，那么也许以后的哪一天，给人看病的话可以采用一种机器流水线的操作，机器 24 小时可以不停歇地运行，而且还不用花那么多的时间和金钱去培养职业的医学技术从业者，自然而然，看病的成本就降下来了。  </p><p>导致看病昂贵的第二个方面的因素是医疗体制造成的。医疗服务的提供方主要有两个：医院和医生的医疗诊所，而这两个服务提供方提供服务的方式，基本上就是一揽子合同，将诊断和治疗融为一体，将经验医疗和精确医疗融为一体，全部包办。这是什么意思呢？下面我来简单解释一下。  </p><p>医生给人看病的时候可以将病人症状的基本情况分为两类，第一类是一看就能够确诊是什么病，背后是什么机理引起的，这种类型我们称之为精确医疗；第二类是单单从表面症状上看，并不能确定背后的患病机理和患病类型，需要医生通过自己的临床经验，进行试探性地检查和探索，直到最后搞清楚所有的患病机理和患病类型，我们把这个过程称之为经验医疗。</p><p>在实际场景中，这两种形式的医疗都同时存在并且融合在同一个医疗服务的提供方之中的。然而我们回过头来看这两种类型的医疗时，他们虽然有着各自的不同特点，但是在最终的结合点上却有一点是共通的。对于精确医疗来说，一旦确诊之后，这种类型疾病大体可以按照流程划分为规范性的治疗步骤，只要按照这种规范性步骤操作，最后就一定能够得到理想的结果，这种规范性的步骤是通过大量有经验的权威性的医师总结出来的，对于这样的疾病类型，完全可以做到将其治疗的过程外包出去，一旦确诊之后，可以通过专门的医疗机构通过预先总结出来的权威性规范性的措施，一步步治疗达到最后的理想结果，这个过程完全可以不用夹杂在同一个医疗服务提供方之中。而对于经验医疗来说，医生通过一系列的试探性的措施和步骤，最后搞清楚各种状况之后，也就是一个各种精确医疗组合的问题了，对于这种情况而言，也可以通过之前说过的精确医疗的方式进行治疗。如果在实际场景中我们可以将疾病的诊断和治疗两个方面分开，对于其中的一些疾病，采用更低成本的人力配置，采用标准化和流水线化的操作，是完全可以很大程度上降低成本的。当然对于医疗体制方面的问题还有许多许多，譬如在疾病的治疗过程中完全是供决定需的关系，就是说，医生在具体的治疗过程按照项目收费，进行哪些项目完全是由医生决定的，不说决定，病人甚至都没法参与进去，这些都是医疗体制中存在的问题，这些体制问题能否找到行之有效的解决途径？讲一讲这些问题可能还需要好多篇文章的篇幅，对于这些问题，我们以后再说吧。  </p><p>影响看病贵的最后一个因素，是药物研发的问题。实际上，对于很多药物而言，其研发大约需要 20 年的时间里投入 20 亿美元的资金，而对于这种药物的专利，在其研发过程的早期就申请了，因为不申请的话就被别人申请走了，从最早期的专利申请，到药物走过几期临床试验最终面世，大约需要十几年的时间，而专利的保护年限只有 20 年，所以等药物真正面世之后，可能只有几年的时间把早期的研发成本收回来，而到了专利到期的那天，一夜之间，药物的价格大约会下降 80% 以上。所以基于这些因素的考虑，一款新药上市之后价格是非常昂贵的。但是对于很多疾病来说，我们是可以通过早期的一些身体数据的异常，利用大数据分析的方法，推测患某种疾病的可能性，从而做到对很多疾病防患于未然的。在疾病的预防上，人们做的还远远不够，现在的医学的发展也更加重视得了病之后怎么治的问题，而对于很多疾病的早期跟踪和预防问题，显然做的还很不够。  </p><p>上文我们针对医疗领域看病贵的问题，分别谈了三个方面的因素，并且针对每个不同的因素，聊了一下在大数据和机器智能时代有哪些可以优化的措施。现在我们接着来谈下一个问题，也就是医疗资源分配不均衡的问题。  </p><p>我们经常说，互联网能够拉近人与人的距离，在地理位置上相隔几千公里的甚至无论多远的两个人，都可以通过互联网的方式将彼此联系起来，而在医疗资源之中，最重要的资源就是医生资源，我们是否可以通过互联网的手段将一些医术高超的医生和患者跨越地理位置的距离而将其联系起来呢？答案当然是肯定的。不过为了达到在线医疗的预期效果，我们需要建立一种标准格式的在线病历系统，患者保留对自己病历的所有权，并且通过患者自己的授权，这个病历能够被网上任何一个由他自己指定的医生查阅到，同时搭建一个患者与患者之间、医生与医生之间（会诊）、患者与医生之间的交流平台，并且能够根据患者借助于医疗设备做完检查之后，电子病历系统实时更新的变化，用大数据的方法，针对每个患者不同的病情，推荐与之最匹配的医生。这些是完全可以实现的，一旦这些工作落到实处了，那么医疗资源分配不均衡的情况，也可以在某种程度上得到有效的缓解。其实我们回过头来说，如果到了机器智能给人看病超过人类医生的那一天，医疗资源分配不均衡的情况就已经得到解决了，在那些医疗技术水平落后的地区，放一些机器就行了，机器能与人对话，并且随着将来可穿戴设备的普及，人体各项体征数据的采集基本上只要带一个可穿戴设备就行了。  </p><p>对于最后一个方面，很多疾病难以治愈的问题，其实我们可以通过换一种思维，即大数据的思维来解决疾病治疗的问题。拿癌症来说，癌症之所以难以治愈，主要原因是它与诸如感冒之类的疾病致病机理完全不一样，主要体现在以下三个方面：  </p><ul><li>癌症是由于人类自身的细胞在细胞复制的过程中产生基因错误而导致的，它不比感冒这种由于细菌感染所引起的疾病，对于后者，我们可以用青霉素等药物，破坏病毒细胞的细胞壁，杀死病毒细胞，从而可以得到有效的治愈。但是对于癌细胞而言，它来源于人体自身，是没有细胞壁的，所以不能通过这种方式来把癌细胞杀死。实际上，同一种癌症其背后的致病机理可能是由不同的基因错误引起的，所以有些抗癌药物对于有些患者有效，而对于另外一些患者无效，就是因为虽然是同一种癌症，其背后的基因错误是不一样的。很多时候，医生在给癌症患者进行药物治疗之前，需要进行基因比对，以确定这类药物是否对该患者有效，就是这个道理。  </li><li>癌症是由于自身细胞复制的过程中产生基因错误引起的，那么既然人体细胞在复制的过程中能够产生一次错误，自然更容易产生第二次错误（也就是说，坏了的东西其实更容易损坏），这就是我们可能听说过，以前有一位患者癌症治好了，但是突然在某一天复发之后，原来的药物不见效，很快就去世了。就是这个原因，第二次的基因错误和第一次不一样。</li><li>癌细胞既然是由于基因错误引起的，那么这种基因错误的癌细胞，它自身在复制的过程中，也更容易产生错误，也就是说，癌细胞变异进而引起其他癌症的风险很大。</li></ul><p>基于以上三个方面的原因，所以癌症这种疾病变得极难治愈。目前我所了解到的针对癌症治疗的科学进展，主要有两个：其一是所谓的“饿死癌细胞”，这个观点是清华大学的颜宁教授提出来的，其背后的机理是，在提供其他营养物质维持正常细胞代谢的同时，阻断葡萄糖运输特异，饿死癌细胞。至于这种阻断的方式是什么？目前好像还不知道，科学家正在研究中。不过这是一个方向。其二是换一种思维，即采用大数据的思维来解决癌症治疗的问题。下面我具体来谈谈这种方法。  </p><p>癌症是由于基因错误引起的，那么其实我们可以通过研制治疗这种基因错误的药物，来治疗癌症。但是，遗憾的是，目前已知的导致肿瘤的基因错误在万这个数量级上，已知的癌症在百这个数量级上，如果考虑所有可能的基因复制错误和各种癌症的组合，种类将是几百万到上千万种，对于人类来说，这个数字是非常庞大的，不过，与此相对应的，这个数字对于计算机而言，却是一个很小的数字，如果能够利用大数据技术，在这些几百万到上千万种组合中找到真正引起癌症的组合，并且针对这些组合研制相对应的治疗药物，那么治疗癌症的梦想就可以实现了。实际上，国际上有很多大公司都在做相应的努力，譬如 Google 旗下的 Calico，以及美国的 Grail 公司等。  </p><p>以上就是我对于医疗领域了解到的一些知识和自己的一些想法，希望分享给大家能够有所帮助。明天，我们来聊一聊智能时代背景下的教育问题。  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在昨天的文章中我们详细说明了什么是机器智能，用数据驱动获取机器智能的方法，谈了机器智能的一个必要因素就是大数据，并且讲了大数据对于人类生活的预测指导作用，今天我们在此基础之上，主要来谈谈医疗的内容，基于一篇文章的篇幅所限，明天我们再来谈教育。  &lt;/p&gt;
&lt;p&gt;当今医疗领域所面对的问题主要有三个方面：第一，看病贵；第二，医疗资源分配不均衡，好的医疗设备和技术精湛的名医大都集中在经济比较发达的地区，经济不发达地区的人面临的不仅仅是看不起病的问题，同时还有生了重病找不到好的医生的问题；第三，很多疾病，譬如癌症，人类仍然没有找到可靠的能够使其得到稳定治愈的方法。其实针对这三个方面的问题，大数据和机器智能的发展都能够提供一个理论上行得通的解决方案。下面我们一一来谈。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://blog.keeplearning.group/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="公众号" scheme="http://blog.keeplearning.group/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"/>
    
  </entry>
  
</feed>
