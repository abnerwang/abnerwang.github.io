<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qiyexuxu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.keeplearning.group/"/>
  <updated>2019-08-19T13:08:18.385Z</updated>
  <id>http://blog.keeplearning.group/</id>
  
  <author>
    <name>David Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>logistic 回归模型</title>
    <link href="http://blog.keeplearning.group/2019/08/19/2019/08-19-logistic_regression/"/>
    <id>http://blog.keeplearning.group/2019/08/19/2019/08-19-logistic_regression/</id>
    <published>2019-08-19T08:49:00.000Z</published>
    <updated>2019-08-19T13:08:18.385Z</updated>
    
    <content type="html"><![CDATA[<p>logistic 回归模型用于分类问题，它是概率模型，同时属于对数线性模型。这是什么意思呢？概率模型，说明最后学到的模型形式是 $P(Y|X=x)$，其中 $X$ 是特征，$Y$ 是类别。对数线性模型，说明与学得的模型 $P(Y|X=x)$ 有关的某种对数形式与线性函数 $w \cdot x + b$（$w$ 为权重参数，$b$ 为偏置参数）相等。事实上，基于这两点，我们可以推导出 logistic 回归模型的数学形式。</p><p>首先，我们从最简单的用于二分类的二项 logistic 回归模型说起，之后推广到用于多分类的多项 logistic 回归模型。</p><h3 id="二项-logistic-回归模型"><a href="#二项-logistic-回归模型" class="headerlink" title="二项 logistic 回归模型"></a>二项 logistic 回归模型</h3><p>对于二分类问题，设分类标签 $Y \in \{1, 0\}$。我们首先观察一下线性函数 $w \cdot x + b$，$w$ 和 $b$ 是需要学习的参数，对它们的取值范围不加任何限制，也就是说，其取值范围为 $(-\infty, +\infty)$，而 $P(Y|X=x) \in [0, 1]$，将一个 $[0, 1]$ 范围内的数映射到 $(-\infty, +\infty)$ 范围上，需要经过某种变换，在这里，我们使用对数几率变换，即 logit 变换。</p><a id="more"></a><p>在介绍 logit 变换之前，先介绍一下一个事件发生的几率的概念。设一个事件发生的概率是 $p$，那么该事件不发生的概率为 $1-p$，将 $\frac{p}{1-p}$ 定义为该事件发生的几率，加上对数后得 $log \frac{p}{1-p}$（$log$ 代表以自然对数 $e$ 为底的对数），上式称为某事件发生的对数几率，与该式对应的变换称为 logit 变换。</p><p>将 logit 变换应用到二分类的概率模型 $P(Y|X=x)$ 上，得：</p><p>$$log\frac{P(Y=1|X=x)}{1-P(Y=1|X=x)}=w \cdot x + b$$</p><p>由上式求得：</p><p>$$P(Y=1|X=x)=\frac{exp(w \cdot x + b)}{1 + exp(w \cdot x + b)}$$</p><p>进而：</p><p>$$P(Y=0|X=x)=1-P(Y=1|X=x)=\frac{1}{1 + exp(w \cdot x + b)}$$</p><p>以上两式就是我们所说的二项 logistic 回归模型。</p><p>通常，我们将 $b$ 也纳入权重参数 $w$ 中，若 $w、x \in \mathbb{R}^n$，则纳入 $b$ 后 $w$ 为 $(w_1, w_2, …, w_n, b)$，$x$ 相应地变化为 $(x_1, x_2, …, x_n, 1)$，新的 $w \in \mathbb{R}^{n+1}$，新的 $x \in \mathbb{R}^{n+1}$。则以上二项 logistic 回归模型变为：</p><p>$$P(Y=1|X=x)=\frac{exp(w \cdot x)}{1 + exp(w \cdot x)}\\<br>P(Y=0|X=x)=\frac{1}{1 + exp(w \cdot x)}$$</p><p>对于以上问题，模型我们已知了，但是参数 $w$ 却是未知的，我们可以采用极大似然估计的方法得到这里的参数 $w$。</p><p>设训练集为 $\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), …, (x^{(N)}, y^{(N)})\}$，共有 $N$ 个样本，$x^{(i)}$ 为样本特征，$y^{(i)}$ 为对应样本的类别标签，$y^{(i)} \in \{0, 1\}$。令样本 $i$ 是类别 $1$ 的概率为 $\pi(x^{(i)})=\frac{exp(w \cdot x^{(i)})}{1 + exp(w \cdot x^{(i)})}$，则其是类别 $0$ 的概率为 $1-\pi(x^{(i)})=\frac{1}{1 + exp(w \cdot x^{(i)})}$，由此可得似然函数为：</p><p>$$l(w)=\Pi_1^N\pi(x^{(i)})^{y^{(i)}}(1-\pi(x^{(i)}))^{1-y^{(i)}}$$</p><p>取对数似然：</p><p>$$L(w)=\Sigma_{i=1}^N [y^{(i)}log\pi(x^{(i)})+(1-y^{(i)})log(1-\pi(x^{(i)}))]\\<br>=\Sigma_{i=1}^N[y^{(i)}log \frac{\pi(x^{(i)})}{1-\pi(x^{(i)})}+log(1-\pi(x^{(i)})]\\<br>=\Sigma_{i=1}^N[y^{(i)} \cdot wx^{(i)}-log(1+exp(wx^{(i)}))]$$</p><p>由此，二项 logistic 回归模型的学习问题变为以上对数似然函数的极大化问题。logistic 回归模型的学习算法通常采用梯度下降法和 <a href="http://abnerwang.github.io/2019/08/16/2019/08-16-newton-method/" target="_blank" rel="noopener">拟牛顿法</a>。</p><h3 id="多项-logistic-回归模型"><a href="#多项-logistic-回归模型" class="headerlink" title="多项 logistic 回归模型"></a>多项 logistic 回归模型</h3><p>多项 logistic 回归模型用于多分类问题。设在多分类问题中的类别标签共有 $K$ 个，样本 $x$ 属于类别 $k$ 的概率是 $p_k$，这里用到的 logit 变换为 $log\frac{p_k}{p_K}$。将该 logit 变换用到这里的多项 logistic 回归模型中，得：</p><p>$$log\frac{P(Y=k|X=x)}{P(Y=K|X=x)}=w^{(k)}x$$</p><p>即有：</p><p>$$\frac{P(Y=k|X=x)}{P(Y=K|X=x)}=exp(w^{(k)}x)\\<br>\Rightarrow \frac{1-P(Y=K|X=x)}{P(Y=K|X=x)}=\Sigma_{k=1}^{K-1}exp(w^{(k)}x)$$</p><p>综上，可求得：</p><p>$$P(Y=K|X=x)=\frac{1}{1+\Sigma_{k=1}^{K-1}exp(w^{(k)}x)}$$</p><p>将上式代入一开始通过 logistic 变换得到的式子中，求得：</p><p>$$P(Y=k|X=x)=\frac{exp(w^{(k)}x)}{1+\Sigma_{k=1}^{K-1}exp(w^{(k)}x)}$$</p><p>以上两式即为多项 logistic 回归模型。二项 logistic 回归模型的参数估计方法可推广到多项 logistic 回归模型中，不再赘述。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;logistic 回归模型用于分类问题，它是概率模型，同时属于对数线性模型。这是什么意思呢？概率模型，说明最后学到的模型形式是 $P(Y|X=x)$，其中 $X$ 是特征，$Y$ 是类别。对数线性模型，说明与学得的模型 $P(Y|X=x)$ 有关的某种对数形式与线性函数 $w \cdot x + b$（$w$ 为权重参数，$b$ 为偏置参数）相等。事实上，基于这两点，我们可以推导出 logistic 回归模型的数学形式。&lt;/p&gt;
&lt;p&gt;首先，我们从最简单的用于二分类的二项 logistic 回归模型说起，之后推广到用于多分类的多项 logistic 回归模型。&lt;/p&gt;
&lt;h3 id=&quot;二项-logistic-回归模型&quot;&gt;&lt;a href=&quot;#二项-logistic-回归模型&quot; class=&quot;headerlink&quot; title=&quot;二项 logistic 回归模型&quot;&gt;&lt;/a&gt;二项 logistic 回归模型&lt;/h3&gt;&lt;p&gt;对于二分类问题，设分类标签 $Y \in \{1, 0\}$。我们首先观察一下线性函数 $w \cdot x + b$，$w$ 和 $b$ 是需要学习的参数，对它们的取值范围不加任何限制，也就是说，其取值范围为 $(-\infty, +\infty)$，而 $P(Y|X=x) \in [0, 1]$，将一个 $[0, 1]$ 范围内的数映射到 $(-\infty, +\infty)$ 范围上，需要经过某种变换，在这里，我们使用对数几率变换，即 logit 变换。&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="logistic 回归模型" scheme="http://blog.keeplearning.group/tags/logistic-%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>拉格朗日对偶</title>
    <link href="http://blog.keeplearning.group/2019/08/19/2019/08-19-lagrange_duality/"/>
    <id>http://blog.keeplearning.group/2019/08/19/2019/08-19-lagrange_duality/</id>
    <published>2019-08-19T03:35:00.000Z</published>
    <updated>2019-08-19T12:36:18.373Z</updated>
    
    <content type="html"><![CDATA[<p>拉格朗日对偶常用于带约束条件的最优化问题上，为了系统讲述这种方法的思想，我们首先引入一个带约束条件的最优化问题：</p><p>$$\quad min_xf(x) \\ s.t.\quad c_i(x) \leq 0 \quad (i=1, 2, …, k) \\ \quad h_j(x)=0 \quad (j=1, 2, …, l)$$</p><p>其中，$f(x)、c_i(x)、h_j(x)$ 分别是定义在 $x \in \mathbb{R}^n$ 上的连续可微函数。对于此类带约束条件的优化问题，一般我们考虑使用拉格朗日乘子法解决，首先引入拉格朗日乘子 $\alpha_i(\alpha_i \geq 0), \beta_j$，构造广义拉格朗日函数如下：  </p><p>$$L(x, \alpha, \beta)=f(x)+\Sigma_{i=1}^{k} \alpha_i \cdot c_i(x)+\Sigma_{j=1}^l \beta_j \cdot h_j(x)$$</p><a id="more"></a><p>定义关于 $x$ 的函数：</p><p>$$\theta_p(x)=max_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)=max_{\alpha,\beta:\alpha \geq 0}f(x)+\Sigma_{i=1}^{k} \alpha_i \cdot c_i(x)+\Sigma_{j=1}^l \beta_j \cdot h_j(x)$$</p><p>当 $c_i(x)$ 或 $h_j(x)$ 不满足原始约束条件时，即 $c_i(x)&gt;0$ 或者 $h_j(x) \neq 0$ 时，$\theta_p(x)=max_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)=+\infty$，而当二者都满足原始约束条件时，$\theta_p(x)=max_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)=f(x)$，即：</p><p>$$\theta_p(x)=\begin{cases}+\infty&amp; \text{$c_i(x)$ 或 $h_j(x)$ 不满足约束条件}\\<br>f(x)&amp; \text{otherwise}<br>\end{cases}$$</p><p>因此，$min_x \theta_p(x)$ 等价于 $c_i(x)$ 或 $h_j(x)$ 同时满足约束条件下的 $min_xf(x)$ 问题，即其与原始问题等价。也就是说，带约束条件的原始问题与不带约束条件的广义拉格朗日函数的极小极大问题等价，即与如下问题等价：</p><p>$$min_x \theta_p(x)=min_xmax_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)$$</p><p>设原始问题的最优值为 $p^*$，即有：$p^*=min_x \theta_p(x)$。</p><p>某些时候，直接求原始问题的解计算很复杂，这种情况下，我们通常需要将广义拉格朗日函数的极小极大问题转化为其对偶问题来进行求解，与之相对应的对偶问题为广义拉格朗日函数的极大极小化问题，定义如下：</p><p>$$max_{\alpha,\beta:\alpha \geq 0}min_xL(x, \alpha, \beta)$$</p><p>设以上对偶问题的最优值是 $d^*$。既然我们将原始问题转化为对偶问题进行求解，那么接下来的一个问题自然就是原始问题的最优值 $p^*$ 与对偶问题的的最优值 $d^*$ 之间有什么关系呢？下面直接以定理的形式给出这二者之间的关系。</p><p><strong>定理1.</strong> &nbsp;&nbsp;&nbsp;&nbsp;如果原始问题和对偶问题都有最优值，则有：</p><p>$$d^*=max_{\alpha,\beta:\alpha \geq 0}min_xL(x, \alpha, \beta) \leq min_xmax_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)=p^*$$</p><p><strong>证明：</strong> &nbsp;显然，$\forall_{x, \alpha, \beta}$，有如下不等式成立：</p><p>$$min_xL(x, \alpha, \beta) \leq min_{x, \alpha \geq 0, \beta}L(x, \alpha, \beta) \leq max_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)$$</p><p>即：$min_xL(x, \alpha, \beta) \leq max_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)$，进一步地，有：</p><p>$$max_{\alpha,\beta:\alpha \geq 0}min_xL(x, \alpha, \beta) \leq min_xmax_{\alpha,\beta:\alpha \geq 0}L(x, \alpha, \beta)$$</p><p>即：$d^* \leq p^*$，得证。</p><p>既然我们需要通过求对偶问题的最优值 $d^*$ 来求原始问题的最优值 $p^*$，而 $d^* \leq p^*$，那么接下来我们需要关心的问题是，$d^*$ 与 $p^*$ 能否相等？以及二者何时相等呢？针对这两个问题，下面不加证明地给出如下两个定理。</p><p><strong>定理2.</strong> &nbsp;&nbsp;&nbsp;&nbsp;对于以上所述的原始问题和对偶问题，当原始优化问题的可行域定义在 $x$ 的凸集上（也就是说，对于可行域内的每一对点，连接该对点的直线段上的每个点也在可行域内），$f(x)$、$c_i(x)$ 是一个凸函数，$h_j(x)$ 是一个仿射函数，并且存在某个 $x$，使得对所有的 $i$，都有 $c_i(x)&lt;0$，那么必定存在 $x^*$、$\alpha^*$ 和 $\beta^*$，使得 $x^*$ 是原始问题的解，$\alpha^*$ 和 $\beta^*$ 是对偶问题的解，同时：</p><p>$$d^*=p^*=L(x^*, \alpha^*, \beta^*)$$</p><p><strong>定理3.</strong> &nbsp;&nbsp;&nbsp;&nbsp;如果原始问题和对偶问题满足定理 2，那么，$x^*$ 是原始问题的解，$\alpha^*$ 和 $\beta^*$ 是对偶问题的解，并且 $d^*=p^*=L(x^*, \alpha^*, \beta^*)$ 的充分必要条件是 $x^*$、$\alpha^*$ 和 $\beta^*$ 同时满足如下所述的 KKT (Karush-Kuhn-Tucker) 条件：</p><p>$$\nabla_xL(x^*, \alpha^*, \beta^*)=0\\<br>\nabla_{\alpha}L(x^*, \alpha^*, \beta^*)=0\\<br>\nabla_{\beta}L(x^*, \alpha^*, \beta^*)=0\\<br>\alpha_i^*c_i(x^*)=0 \quad i=1, 2, …, k\\<br>\alpha_i^* \geq 0 \quad i=1, 2, …, k\\<br>c_i(x^*) \leq 0 \quad i=1, 2, …, k\\<br>h_j(x^*)=0 \quad j=1, 2, …, l$$</p><p>其中，$\alpha_i^*c_i(x^*)=0$ 称为 KKT 的对偶互补条件，通过此式可知，如果 $\alpha_i^*&gt;0$，则必有 $c_i(x^*)=0$。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;拉格朗日对偶常用于带约束条件的最优化问题上，为了系统讲述这种方法的思想，我们首先引入一个带约束条件的最优化问题：&lt;/p&gt;
&lt;p&gt;$$\quad min_xf(x) \\ s.t.\quad c_i(x) \leq 0 \quad (i=1, 2, …, k) \\ \quad h_j(x)=0 \quad (j=1, 2, …, l)$$&lt;/p&gt;
&lt;p&gt;其中，$f(x)、c_i(x)、h_j(x)$ 分别是定义在 $x \in \mathbb{R}^n$ 上的连续可微函数。对于此类带约束条件的优化问题，一般我们考虑使用拉格朗日乘子法解决，首先引入拉格朗日乘子 $\alpha_i(\alpha_i \geq 0), \beta_j$，构造广义拉格朗日函数如下：  &lt;/p&gt;
&lt;p&gt;$$L(x, \alpha, \beta)=f(x)+\Sigma_{i=1}^{k} \alpha_i \cdot c_i(x)+\Sigma_{j=1}^l \beta_j \cdot h_j(x)$$&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="拉格朗日对偶" scheme="http://blog.keeplearning.group/tags/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E5%AF%B9%E5%81%B6/"/>
    
  </entry>
  
  <entry>
    <title>牛顿法与拟牛顿法</title>
    <link href="http://blog.keeplearning.group/2019/08/16/2019/08-16-newton-method/"/>
    <id>http://blog.keeplearning.group/2019/08/16/2019/08-16-newton-method/</id>
    <published>2019-08-16T02:55:00.000Z</published>
    <updated>2019-08-19T03:08:23.777Z</updated>
    
    <content type="html"><![CDATA[<p>本文首先从一个不带约束条件的最优化问题出发，导出用以解决此类问题的牛顿法，接着针对牛顿法涉及的计算复杂问题，阐述在此基础上改进得到的拟牛顿法的思路。  </p><h3 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h3><p>我们首先引入如下最优化问题：  </p><p>$$min_{x\in{\mathbb{R}^n}}f(x)$$</p><p>若 $f(x)$ 在其定义域内具有二阶连续偏导数，在其定义域内任取一点 $x^{(k)}$，将 $f(x)$ 在点 $x^{(k)}$ 处进行二阶泰勒展开，得：  </p><p>$$f(x)=f(x^{(k)})+g_k^T(x-x^{(k)})+\frac{1}{2}(x-x^{(k)})^TH_k(x-x^{(k)})$$</p><p>其中，${g_k}=\nabla_{x^{(k)}}f(x^{(k)})$，$H_k=(\frac{\partial^2 f(x^{(k)})}{\partial x_i \partial x_j})_{n \times n}$，该矩阵 $H_k$ 称为 Hessian 矩阵，当 $x \to x^{(k)}$ 时上式成立（即在 $x^{(k)}$ 的小邻域内成立）。由以上引入的最优化问题，我们需要得到 $x^{(k)}$ 小邻域内的极小值，而 $f(x)$ 在 $x^{(k)}$ 小邻域内取得极值的必要条件是 $\nabla_{x}f(x)=0$，在上式中令 $\nabla_{x}f(x)=0$，得：</p><a id="more"></a>  <p>$$\nabla_{x}f(x)={g_k}+H_k(x-x^{(k)})=0$$</p><p><strong>当 Hessian 矩阵 $H_k$ 正定时（$H_k^{-1}$ 也正定），$f(x)$ 在 $x^{(k)}$ 小邻域内取得极小值，这也是牛顿法能够收敛的条件</strong>，由上式解得此时 $x=x^{(k)}-{H_k}^{-1}g_k$，将此时得到的 $x$ 置为新的 $x^{(k)}$，如此循环迭代，最后必能得到函数 $f(x)$ 的全局最小值。特别地，当 $x \in \mathbb{R}$ 时，以上过程如下图所示：</p><p><img src="https://res.cloudinary.com/dtfjnb8ft/image/upload/v1565928813/640px-NewtonIteration_Ani.gif" alt="newton method"></p><p>上图中的 $f(x_i),(i=1, 2, 3, 4, 5)$ 实际上指的是目标函数的一阶导数，而 $f’(x_i),(i=1,2,3,4,5)$ 则指的是目标函数的二阶导数。由上图可知，随着 $x$ 值的不断更新，使得一阶导数 $f(x_i)$ 逐渐接近其零点 $x_5$，而一阶导数为 $0$ 时，其对应的目标函数取得全局最优值。</p><p>我们将由牛顿法求目标函数全局最小值的过程概括成如下步骤：</p><p>输入：目标函数 $f(x)$，梯度 $g(x)$ 和 Hessian 矩阵 $H(x)$，精度 $\varepsilon$。<br>输出：目标函数 $f(x)$ 取得极小值时的最优值 $x^*$ 。<br>(1) 取初始点为 $x^{(0)}$，置 $k$ 为 $0$；<br>(2) 计算梯度 $g_k=g(x^{(k)})$，若 $\Arrowvert g_k \Arrowvert&lt;\varepsilon$，则停止计算，令 $x^*=x^{(k)}$，否则继续下一步；<br>(3) 计算 Hessian 矩阵 $H_k=H(x^{(k)})$，按 $x^{(k+1)}=x^{(k)}-H_k^{-1}g_k$ 更新 $x$ 的值，令 $k=k+1$，返回步骤（2）。</p><p>泰勒公式反映了一种重要的微积分思想，那就是“以曲代曲”，即用简单曲线代替复杂曲线的求解过程。二阶泰勒展开是一种二次曲线，当目标函数也是二次函数时，或者目标函数不是二次函数，但是二次形态较强，亦或者迭代点已经进入极小点的邻域，那么牛顿法的收敛速度是非常快的。但是，对于非二次型的目标函数，有时会使函数值上升，即出现 $f(x^{(k+1)})&gt;f(x^{(k)})$ 的情况，在严重的情况下甚至能够引起迭代点列 $\{x^{(k)}\}$ 发散而导致计算失败。针对这一问题，考虑在原始牛顿法的基础上引入迭代步长，这种方法称为 <strong>阻尼牛顿法</strong>，记最优的步长因子为 $\lambda_k(&gt;0)$，其值通过以下过程获得：</p><p>$$\lambda_k=argmin_{\lambda \in \mathbb{R}}f(x^{(k)}-\lambda H_k^{-1}g_k)$$</p><p>将以上步骤 (3) 中的 $x^{(k+1)}=x^{(k)}-H_k^{-1}g_k$ 改为 $x^{(k+1)}=x^{(k)}-\lambda H_k^{-1}g_k$，原过程其他部分保持不变，即得阻尼牛顿法求目标函数全局最小值的过程。</p><p>Hessian 矩阵 $H_k$ 正定（$H_k^{-1}$ 也正定）是牛顿法收敛的重要条件，也就是说这样可以保证牛顿法的搜索方向是目标函数的下降方向，下面我们进一步说明这是为什么。若记 $p_k=-H_kg_k$，同时引入步长因子 $\lambda_k$，则 $x$ 的更新过程为：$x^{(k+1)}=x_k+\lambda_k p_k$，将此式代入目标函数的二阶泰勒展开式中，可得：  </p><p>$$f(x^{(k+1)}) \thickapprox f(x^{(k)})-\lambda_k {g_k}^T {H_k}^{-1}g_k$$</p><p>由于 $H_k^{-1}$ 正定，由矩阵正定的定义可得 ${g_k}^T {H_k}^{-1}g_k &gt; 0$，而步长因子 $\lambda_k &gt; 0$，因此，$f(x^{(k+1)}) &lt; f(x^{(k)})$，所以 Hessian 矩阵 $H_k$ 正定保证了牛顿法的搜索方向始终是目标函数的下降方向。</p><p>由以上过程可知，牛顿法在进行数值更新的时候，每次都要计算 Hessian 矩阵的逆 $H_k^{-1}$。这种计算是很复杂的。由此我们考虑是否可以用某个矩阵来代替这里的 Hessian 矩阵 $H_k$ 或者 $H_k^{-1}$，基于这种思想发展出了后来称之为拟牛顿法的优化方法。</p><h3 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h3><p>我们分别考虑用矩阵 $G_k$ 来近似矩阵 $H_k^{-1}$，用矩阵 $B_k$ 来近似矩阵 $H_k$，分别得到 DFP 算法和 BFGS 算法。</p><h4 id="DFP-Davidon-Fletcher-Powell-算法"><a href="#DFP-Davidon-Fletcher-Powell-算法" class="headerlink" title="DFP(Davidon-Fletcher-Powell) 算法"></a>DFP(Davidon-Fletcher-Powell) 算法</h4><p>用矩阵 $G_k$ 来近似矩阵 $H_k^{-1}$，则由原始牛顿法的迭代过程可知，此时 $x^{(k+1)}=x^{(k)}-G_kg_k$，在不满足迭代终止条件的时候，为了继续得到 $x^{(k+2)}$ 使得迭代过程继续下去，那么接下来的一个问题是 $x=x^{(k+1)}$ 时 $G_{k+1}$ 是多少呢？不妨对目标函数 $f(x)$ 在 $x=^{(k+1)}$ 处做二阶泰勒展开，得：  </p><p>$$f(x)=f(x^{(k+1)})+g_{k+1}^T(x-x^{(k+1)})+\frac{1}{2}(x-x^{(k+1)})^TH_{k+1}(x-x^{(k+1)})$$</p><p>上式两边分别对 $x$ 求梯度，得：</p><p>$$\nabla_xf(x)=g_{k+1}+H_{k+1}(x-x^{(k+1)})$$</p><p>上式中，因为 $x^{(k)}$ 在 $x^{(k+1)}$ 的一个小邻域内，故可取 $x=x^{(k)}$，代入上式，可得：</p><p>$$g_{k+1}-g_k=H_{k+1}(x^{(k+1)}-x^{(k)})$$</p><p>记 $y_k=g_{k+1}-g_k$，$\delta_k=x^{(k+1)}-x^{(k)}$，上式变为：</p><p>$$y_k=H_{k+1}\delta_k$$</p><p>或：</p><p>$$\delta_k=H_{k+1}^{-1}y_k$$</p><p>以上两式称为 <strong>拟牛顿条件</strong>。</p><p>当使用 $G_{k+1}$ 来近似 $H_{k+1}^{-1}$ 时，它必须满足上文所述的拟牛顿条件，即 $\delta_k=G_{k+1} \cdot y_k$。对 $G_k$ 进行更新得到 $G_{k+1}$，令 $G_{k+1} = G_k + \Delta G$，考虑 $\Delta G$ 为两个附加项 $P_k$、$Q_k$ 之和，即有：  </p><p>$$G_{k+1}=G_k+P_k+Q_k$$</p><p>上式等号两边同时乘以 $y_k$ 可得：</p><p>$$G_{k+1}y_k=G_k y_k + P_k y_k + Q_k y_k$$</p><p>上式等号左端为 $\delta_k$，若令右端 $P_k y_k = \delta_k$，则必须有 $Q_k y_k = -G_k y_k$，为满足上述条件，可取 $P_k=\frac{\delta_k{\delta_k}^T}{\delta_k^T y_k}$，$Q_k=-\frac{G_ky_ky_k^TG_k}{y_k^TG_ky_k}$，则有：</p><p>$$G_{k+1}=G_k+\frac{\delta_k{\delta_k}^T}{\delta_k^T y_k}-\frac{G_ky_ky_k^TG_k}{y_k^TG_ky_k}$$</p><p>以上就是 DFP 算法用到的 $G_{k+1}$ 的迭代公式。细心的读者可能发现此时 $G_{k+1}$ 是否正定呢？事实上，只要初始的 $G_0$ 是正定的，后续迭代得到的每个 $G_{k+1}$ 都是正定的。</p><p>经过以上对 $G_k$ 迭代过程的分析，我们得到 DFP 算法的具体步骤如下：</p><p>输入：目标函数 $f(x)$，梯度 $g(x)$，精度 $\varepsilon$。<br>输出：目标函数 $f(x)$ 取得极小值时的最优值 $x^*$ 。<br>(1) 取初始点为 $x^{(0)}$，取 $G_0$ 为正定对称矩阵，置 $k$ 为 $0$；<br>(2) 计算梯度 $g_k=g(x^{(k)})$，若 $\Arrowvert g_k \Arrowvert&lt;\varepsilon$，则停止计算，得 $x^*=x^{(k)}$，否则继续下一步；<br>(3) 令 $p_k=-G_kg_k$，执行一维搜索，求步长因子 $\lambda_k$ 使得 $\lambda_k=argmin_{\lambda}f(x^{(k)}+\lambda p_k)$；<br>(4) 计算 $x^{(k+1)}=x^{(k)}+\lambda_k p_k$ ；<br>(5) 计算梯度 $g_{k+1}=g(x^{(k+1)})$，若 $\Arrowvert g_{k+1} \Arrowvert&lt;\varepsilon$，则停止计算，得 $x^*=x^{(k+1)}$，否则按式 $G_{k+1}=G_k+\frac{\delta_k{\delta_k}^T}{\delta_k^T y_k}-\frac{G_ky_ky_k^TG_k}{y_k^TG_ky_k}$ 计算 $G_{k+1}$，令 $k=k+1$，返回步骤（3）。</p><h4 id="BFGS-Broyden-Fletcher-Goldfarb-Shanno-算法"><a href="#BFGS-Broyden-Fletcher-Goldfarb-Shanno-算法" class="headerlink" title="BFGS(Broyden-Fletcher-Goldfarb-Shanno) 算法"></a>BFGS(Broyden-Fletcher-Goldfarb-Shanno) 算法</h4><p>用矩阵 $B_k$ 来近似矩阵 $H_k$ 的时候，同上文所述，需要 $B_{k+1}$ 满足以上提到的拟牛顿条件，即：$y_k=B_{k+1}\delta_k$。同理考虑：</p><p>$$B_{k+1}=B_k+P_k+Q_k$$</p><p>上式等号两边同乘 $y_k$，得：</p><p>$$B_{k+1}\delta_k=B_k\delta_k+P_k\delta_k+Q_k\delta_k$$</p><p>如上文所述一样，可令 $P_k\delta_k=y_k$，$Q_k\delta_k=-B_k\delta_k$，取 $P_k=\frac{y_ky_k^T}{y_k^T\delta_k}$，$Q_k=-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}$，得：</p><p>$$B_{k+1}=B_k+\frac{y_ky_k^T}{y_k^T\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}$$</p><p>以上即为 $B_{k+1}$ 更新的迭代公式。只要初始矩阵 $B_0$ 是正定的，这个迭代过程得到的每个 $B_{k+1}$ 都是正定的。</p><p>经过以上过程，得到 BFGS 算法如下：</p><p>输入：目标函数 $f(x)$，梯度 $g(x)$，精度 $\varepsilon$。<br>输出：目标函数 $f(x)$ 取得极小值时的最优值 $x^*$ 。<br>(1) 取初始点为 $x^{(0)}$，取 $B_0$ 为正定对称矩阵，置 $k$ 为 $0$；<br>(2) 计算梯度 $g_k=g(x^{(k)})$，若 $\Arrowvert g_k \Arrowvert&lt;\varepsilon$，则停止计算，得 $x^*=x^{(k)}$，否则继续下一步；<br>(3) 令 $B_kp_k=-g_k$，求出 $p_k$，执行一维搜索，求步长因子 $\lambda_k$ 使得 $\lambda_k=argmin_{\lambda}f(x^{(k)}+\lambda p_k)$；<br>(4) 计算 $x^{(k+1)}=x^{(k)}+\lambda_k p_k$ ；<br>(5) 计算梯度 $g_{k+1}=g(x^{(k+1)})$，若 $\Arrowvert g_{k+1} \Arrowvert&lt;\varepsilon$，则停止计算，得 $x^*=x^{(k+1)}$，否则按式 $B_{k+1}=B_k+\frac{y_k{y_k}^T}{y_k^T \delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}$ 计算 $B_{k+1}$，令 $k=k+1$，返回步骤（3）。</p><h4 id="Broyden-类算法"><a href="#Broyden-类算法" class="headerlink" title="Broyden 类算法"></a>Broyden 类算法</h4><p>在介绍 Broyden 类算法之前，首先介绍一下 Sherman-Morrison 公式。</p><p><strong>Sherman-Morrison 公式</strong></p><p>假设 $A$ 是 n 阶可逆矩阵，$u、v$ 是 n 维向量，且 $A+uv^T$ 也是可逆矩阵，则：</p><p>$$(A+uv^T)^{-1}=A^{-1}-\frac{A^Tuv^TA^{-1}}{1+v^TA^{-1}u}$$</p><p>上式称为 Sherman-Morrison 公式。</p><p>对于上文所述的 <strong>DFP 算法</strong> 和 <strong>BFGS 算法</strong> 而言，容易验证：$G_k=B_k^{-1}$，$G_{k+1}=B_{k+1}^{-1}$，对 <strong>BFGS 算法</strong> 的迭代公式 $B_{k+1}=B_k+\frac{y_ky_k^T}{y_k^T\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}$ 两次应用 Sherman-Morrison 公式可得：</p><p>$$G_{k+1}=(I-\frac{\delta_ky_k^T}{\delta_k^Ty_k})G_k(I-\frac{\delta_ky_k^T}{\delta_k^Ty_k})^T+\frac{\delta_k\delta_k^T}{\delta_k^Ty_k}$$</p><p>以上称为 <strong>BFGS 算法</strong> 关于 $G_{k+1}$ 的迭代公式。</p><p>若将 <strong>DFP 算法</strong> 中关于 $G_{k+1}$ 的迭代公式得到的 $G_{k+1}$ 记为 $G^{DFP}$，以上 <strong>BFGS 算法</strong> 得到的 $G_{k+1}$ 记为 $G^{BFGS}$，它们都满足拟牛顿条件，故而它们的线性组合：</p><p>$$G_{k+1}=\alpha G^{DFP}+(1-\alpha)G^{BFGS} (0\leq\alpha\leq1)$$</p><p>也满足拟牛顿条件，并且是正定的。这样便可以得到一类拟牛顿法，称之为 <strong>Broyden 类算法</strong>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文首先从一个不带约束条件的最优化问题出发，导出用以解决此类问题的牛顿法，接着针对牛顿法涉及的计算复杂问题，阐述在此基础上改进得到的拟牛顿法的思路。  &lt;/p&gt;
&lt;h3 id=&quot;牛顿法&quot;&gt;&lt;a href=&quot;#牛顿法&quot; class=&quot;headerlink&quot; title=&quot;牛顿法&quot;&gt;&lt;/a&gt;牛顿法&lt;/h3&gt;&lt;p&gt;我们首先引入如下最优化问题：  &lt;/p&gt;
&lt;p&gt;$$min_{x\in{\mathbb{R}^n}}f(x)$$&lt;/p&gt;
&lt;p&gt;若 $f(x)$ 在其定义域内具有二阶连续偏导数，在其定义域内任取一点 $x^{(k)}$，将 $f(x)$ 在点 $x^{(k)}$ 处进行二阶泰勒展开，得：  &lt;/p&gt;
&lt;p&gt;$$f(x)=f(x^{(k)})+g_k^T(x-x^{(k)})+\frac{1}{2}(x-x^{(k)})^TH_k(x-x^{(k)})$$&lt;/p&gt;
&lt;p&gt;其中，${g_k}=\nabla_{x^{(k)}}f(x^{(k)})$，$H_k=(\frac{\partial^2 f(x^{(k)})}{\partial x_i \partial x_j})_{n \times n}$，该矩阵 $H_k$ 称为 Hessian 矩阵，当 $x \to x^{(k)}$ 时上式成立（即在 $x^{(k)}$ 的小邻域内成立）。由以上引入的最优化问题，我们需要得到 $x^{(k)}$ 小邻域内的极小值，而 $f(x)$ 在 $x^{(k)}$ 小邻域内取得极值的必要条件是 $\nabla_{x}f(x)=0$，在上式中令 $\nabla_{x}f(x)=0$，得：&lt;/p&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="牛顿法与拟牛顿法" scheme="http://blog.keeplearning.group/tags/%E7%89%9B%E9%A1%BF%E6%B3%95%E4%B8%8E%E6%8B%9F%E7%89%9B%E9%A1%BF%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>朴素贝叶斯算法详解</title>
    <link href="http://blog.keeplearning.group/2019/06/19/2019/06-14-naive-bayes/"/>
    <id>http://blog.keeplearning.group/2019/06/19/2019/06-14-naive-bayes/</id>
    <published>2019-06-19T14:00:00.000Z</published>
    <updated>2019-08-19T03:02:24.682Z</updated>
    
    <content type="html"><![CDATA[<p>朴素贝叶斯算法主要解决什么问题？它是怎么解决这类问题的？如何证明这种解决方法是有效的？本节将从这三方面详细介绍朴素贝叶斯算法的相关内容。</p><h3 id="朴素贝叶斯算法主要解决什么问题"><a href="#朴素贝叶斯算法主要解决什么问题" class="headerlink" title="朴素贝叶斯算法主要解决什么问题"></a>朴素贝叶斯算法主要解决什么问题</h3><p>朴素贝叶斯算法主要用来处理分类问题，也就是说，我们给算法某个对象的输入特征（通常是向量形式），算法能够根据这个输入特征判断含有这种特征的对象属于预先定义的哪一类。我们所熟悉的垃圾邮件分类问题是朴素贝叶斯算法的典型应用场景。为方便下文论述，我们首先对使用朴素贝叶斯算法解决分类问题进行数学建模，设分类问题的输入空间 $\chi \subseteq \mathbb{R}^n$，输出空间 $\mathbb{y} = ${$c_1, c_2, …, c_K$}$ $，即输入空间定义为 $n$ 维实数向量的子空间，输出空间包含了 $K$ 个预先定义的类别。某一特定问题的输入 $x \in \chi$，输出 $y \in \mathbb{y}$，即朴素贝叶斯算法建立了一个从某个对象提取的特征向量（这里的特征向量和线性代数里面的特征向量是不同的概念，这里指的是能反映该对象特征的向量）到该对象所属类别的一个映射。 </p><a id="more"></a> <h3 id="朴素贝叶斯算法如何解决分类问题"><a href="#朴素贝叶斯算法如何解决分类问题" class="headerlink" title="朴素贝叶斯算法如何解决分类问题"></a>朴素贝叶斯算法如何解决分类问题</h3><p>通常，我们可以将机器学习的过程描述为，机器学习算法从一大堆训练数据中学习解决某类问题的通用经验，然后利用学得的经验，对新的未知问题的结果进行预测。对于朴素贝叶斯算法也是如此。我们首先拿到一大堆训练数据，这类数据中包含有一大堆对象的特征向量和标注好的类别标签（监督学习过程）。比如对于垃圾邮件分类问题（二分类问题），我们拿到的训练数据是一大堆标记为垃圾邮件和非垃圾邮件的特征向量，这些特征向量可能是邮件中出现的某些特殊含义的单词（如购买、价格、618活动等），也有可能是这些特殊含义的单词在邮件中出现的次数、每个单词的赋予的权重等。统计机器学习假设数据存在一定的统计规律，也就是说，我们的训练样本和真实世界中的样本是服从于某一个由输入、输出空间组成的联合概率分布并且由这个概率分布独立同分布（IID）产生的。这是统计机器学习的一个基本前提，读者在学习统计机器学习相关内容时一定要注意这个假设。  </p><p>依照这个模式，我们再回过头来看一下朴素贝叶斯算法分类的过程。我们拿到一大堆训练数据，这些训练数据和真实世界中无穷无尽的样本数据之间符合统计机器学习的基本假设前提，即这二者是由特征向量和标签组成的某个联合概率分布独立同分布产生的。如果设 $X$ 为特征向量，$Y$ 为标签向量，则二者的联合概率分布为 $P(X, Y)$，由于训练数据是由联合概率分布 $P(X, Y)$ 产生的，因此理论上我们可以通过训练数据来估计联合概率分布 $P(X,Y)$。而朴素贝叶斯算法要解决的问题是，给定一个提取的特征向量，判断该特征向量所对应的分类对象归属的类别。我们可以将这个过程建模为已知 $P(X, Y)$，求 $P(Y|X)$，对于这个问题，我们自然而然会联想到贝叶斯公式：  </p><p> $$P(Y|X) = \frac {P(X,Y)} {P(X)} = \frac {P(X|Y)P(Y)} {P(X)}$$  </p><p> 第一部分我们假设 Y 有 K 个类别，因此：  </p><p> $$P(Y|X) = \frac {P(X|Y)P(Y)} {\sum_{i=1}^KP(X|Y=c_i)P(Y=c_i)}$$</p><p>  我们将 $P(Y)$ 称为先验概率，$P(Y|X)$ 称为后验概率。接下来我问你一个问题，如果是你来做分类的话，给你输入向量 $X=x$，根据上面的贝叶斯公式，你会把这个对象归属于 $c_1$ 到 $c_K$ 中的哪一类呢？单纯从直觉上来看的话，我们也应该知道将 X 归为后验概率最大的那一类中，实际上，朴素贝叶斯算法也还真是这么做的。也就是说，我们要从 $c_1$ 到 $c_K$ 中找到一个类，将它的分布代入到贝叶斯公式之后，使得在给定 X 的条件下，X 归属为这个类的可能性是最大的。当然这仅仅是从直觉上来说的，直觉毕竟没有说服力，在下一部分证明这种解决方法有效性的时候，我会给出详细的数学证明，这里先埋一个伏笔。我们先接着看下去。上面一段话，翻译成数学语言就是：  </p><p> $$argmax_{c_k}P(Y=c_k|X=x)=argmax_{c_k}\frac {P(X=x|Y=c_k)P(Y=c_k)}{\sum_{i=1}^KP(X=x|Y=c_i)P(Y=c_i)}$$</p><p> 上式中，分母部分与某一个具体的 $c_k$ 无关，因此上式等价于：  </p><p> $$argmax_{c_k}P(Y=c_k|X=x)=argmax_{c_k}{P(X=x|Y=c_k)P(Y=c_k)}$$  </p><p> 对于上式等号右侧，根据极大似然估计或者贝叶斯估计，由训练样本估计先验概率 $P(Y)$ 是简单的，下文我们会详细讲解。但是对于条件概率 $P(X|Y)$ 的估计则非常困难，因为它的参数数量是指数级的，如果特征向量 $x$ 有 n 个分量，即 $x=(x_1, x_2, …, x_n)$，每个分量可能的取值个数为 $s^{(i)} (i=1, 2, …, n)$，分类类别为 $K$ 个，则参数数量为 $K\prod_{i=1}^{n}s^{(i)}$，这种估计极度复杂，实际是不可行的，因此，朴素贝叶斯算法对于条件概率的估计做了一个错误的假设，即特征向量的各个分量之间各自是独立出现没有联系的。举个例子，例如某封邮件中，前文中出现 “618京东活动日” 这个单词的话，那后文中出现 “购买”、“价格” 等单词的可能性应该是很大的，但是朴素贝叶斯算法却假设这种前后联系的概率是不存在的，也即是这几个彼此存在某种联系的单词的出现从概率上是彼此独立的，这显然是一种错误的假设，但是，这种错误的假设能够极大地简化运算，并且在实际应用中还取得了不错的效果。事实上，也是因为这种错误的假设，所以我们的算法才叫朴素贝叶斯算法（naive bayes algorithm）而不直接叫贝叶斯算法。基于这种 “错误” 的假设，我们可以得到：  </p><p> $$P(X=x|Y=c_k)=P(X^{(1)}=x_1|Y=c_k)P(X^{(2)}=x_2|Y=c_k) \cdots P(X^{(n)}=x_n|Y=c_k)$$  </p><p> 这样，极大简化了计算。  </p><p>回到最初的问题，我们既然要将新的输入样本归入后验概率最大的类中，就要估计概率 $P(X=x|Y=c_k)$ 和 $P(Y=c_k)$，从而才能确定最大后验概率。参数估计是指模型已知、模型参数未知的情况下，根据样本数据对模型参数进行估计的方法，下面我们分别使用极大似然估计和贝叶斯估计对这两个概率进行估计。  </p><h4 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h4><p>在第一部分，我们在输出空间中预先定义了 $K$ 个类，分别为：$c_1, c_2, …, c_K$，设这 K 个类出现的概率分别为 $\theta_1, \theta_2, …, \theta_K$，那么：$\theta_1+\theta_2+\cdots+\theta_K=1$，再设训练样本集中总的样本数为 $N$，这 $K$ 个类在训练样本集中出现的次数分别为：$m_1, m_2, …, m_K$，则有：$m_1+m_2+\cdots+m_K = N$。显然，这是一个多项分布，设训练样本集中的样本类别分别为 $y_1, y_2, …, y_N$，得似然函数为：</p><p>$$L(\theta)=P(y_1, y_2, …, y_N|\theta)=\theta_1^{m_1} \cdot \theta_2^{m_2} \cdots \theta_K^{m_K}$$ $$<br>s.t.\qquad \theta_1+\theta_2+\cdots+\theta_K=1$$</p><p>对数似然为：</p><p>$$\ell(\theta)=logL(\theta)=m_1log\theta_1+m_2log\theta_2+\cdots+m_Klog\theta_K $$ $$<br>s.t.\qquad \theta_1+\theta_2+\cdots+\theta_K=1$$</p><p>我们需要最大化对数似然，以求得模型参数。求解带有约束条件的极值问题需要用到拉格朗日乘子法。首先构造拉格朗日函数为：</p><p>$$L(\theta_1, \theta_2, …, \theta_K, \lambda)=m_1log\theta_1+m_2log\theta_2+\cdots+m_Klog\theta_K+\lambda \cdot (\theta_1+\theta_2+\cdots+\theta_K-1)$$</p><p>要求得以上拉格朗日函数的极值，对每一个 $\theta_i (i=1, 2, …, K)$，令：</p><p>$$\nabla_{\theta_i}L(\theta_1, \theta_2, …, \theta_K, \lambda)=\frac{m_i}{\theta_i}+\lambda=0$$ </p><p>从而得：</p><p>$$\theta_i=-\frac{m_i}{\lambda}$$</p><p>由 $\theta_1+\theta_2+\cdots+\theta_K=1, m_1+m_2+\cdots+m_K = N$，得：$\lambda=-N$  </p><p>将 $\lambda$ 代入上式，可得：</p><p>$$P(Y=c_k)=\theta_k=\frac{m_k}{N}$$</p><p>此即为 $P(Y=c_k)$ 的极大似然估计。</p><p>根据以上对朴素贝叶斯算法 “错误” 假设的陈述，如果要对 $P(X=x|Y=c_k)$ 进行极大似然估计，只需对每一个 $x_i (i=1,2, …, n)$，对 $P(X^{(i)}=x_i|Y=c_k)$ 进行极大似然估计即可。设在训练样本集中类别为 $c_k$ 的样本总数为 $S$ 个，在这 $S$ 个样本中，特征向量的第 $i$ 个分量为 $x_i$ 的样本个数为 $t_i$ 个，同理，经过以上极大似然估计过程，得 $P(X^{(i)}=x_i|Y=c_k)$ 的极大似然估计为：</p><p>$$P(X^{(i)}=x_i|Y=c_k)=\frac{t_i}{S}$$</p><p>根据训练样本集对 $P(Y=c_k)$ 和 $P(X^{(i)}=x_i|Y=c_k)$ 的极大似然估计结果，我们就可以根据后验概率最大化原则对输入的特征向量 $x$ 进行分类了。但是，请注意了，这里有一个问题，考虑一个极端情况，当输入的特征向量的某一个具体的分量 $x_i$ 具有非常重要的判别性质，但是在训练样本集的任何一个类对应的任何一个特征向量，其分量都不包含 $x_i$，这种情况下，对任意类 $c_k$，$P(X^{(i)}=x_i|Y=c_k)=0$，所有的后验概率都为 0，这样我们的分类器就无所适从了。为了更好地应对这种情况，我们需要引入拉普拉斯平滑，而引入拉普拉斯平滑的数学基础，就是我们接下来要介绍的贝叶斯估计的朴素贝叶斯算法。</p><h4 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h4><p>极大似然估计认为先验概率 $P(Y=c_k)$ （即 $\theta_k$）是一个 “确定的” 量，注意我的定语是 “确定”，也就是说，$P(Y=c_k)$ 一定取某个确定的值，并且我们由训练样本集估计出的它取这个值的可能性是百分百的。而贝叶斯估计则不然，贝叶斯估计进行统计建模的出发点是认为先验概率 $P(Y=c_k)$ 是一个随机事件，也就是说，$P(Y=c_k)$ 取得某个值是有一定的概率的，这个概率实际上可以通俗地认为是概率的概率。我们通常使用 $\beta$ 分布或 $Dirichlet$ 分布来对概率的概率引入先验信息，前者适用于两个变量的情况，而后者适用于多变量的情况。显然，我们这里应该使用 $Dirichlet$ 分布，即概率密度函数为：</p><p>$$P(\theta)=P(\theta_1,\theta_2,\cdots,\theta_K|\alpha_1, \alpha_2,\cdots,\alpha_K)=\frac{\prod_{i=1}^K{\Gamma(\alpha_i)}}{\Gamma(\Sigma_{i=1}^K{\alpha_i})}\theta_1^{\alpha_1-1}\theta_2^{\alpha_2-1}\cdots\theta_K^{\alpha_K-1}$$</p><p>在引入先验信息的时候，我们假设 $\theta_1, \theta_2, …, \theta_K$ 出现的可能性是相同的，这样的假设是怎么来的呢？猜的，对，没错，就是猜的。在没有任何根据，没有得到任何有关训练样本集信息的情况下，事先人为地进行猜测的。事实上，这也是一直以来频率学派诟病贝叶斯学派的地方，他们认为在没有任何样本信息的情况下你怎么能胡乱引入先验信息瞎猜呢？关于频率学派和贝叶斯学派相爱相杀的故事有兴趣的读者可以私下了解一下，这里就不赘述了。当然，说是瞎猜，其实也并不是瞎猜的，一切都要以模型的实际工作效果为标准，你猜的模型实际工作的时候效果一塌糊涂，说明你猜错了，先验信息有问题，回过头去重新猜吧。事实上，我们在引入等可能性假设的 $Dirichlet$ 先验信息之后，模型的实际工作效果很好，所以，我们先接受这个等可能性假设，接着往下看吧。</p><p>这里的等可能性假设，反映在上面的概率密度函数上，我们可以令 $\alpha_1=\alpha_2=…=\alpha_K=\alpha&gt;1$，在以上概率密度函数的等号右侧，$\frac{\prod_{i=1}^K{\Gamma(\alpha_i)}}{\Gamma(\Sigma_{i=1}^K{\alpha_i})}$ 是一个与 $\theta_i(i=1,2,…,K)$ 无关的归一化因子，它的存在是为了保证所有的概率之和为 1，所以，以上概率密度函数可以写成如下形式：</p><p>$$P(\theta)=P(\theta_1,\theta_2,\cdots,\theta_K|\alpha)=\frac{\Gamma(\alpha)^K}{\Gamma(K\cdot{\alpha})}\theta_1^{\alpha-1}\theta_2^{\alpha-1}\cdots\theta_K^{\alpha-1} \propto \theta_1^{\alpha-1}\theta_2^{\alpha-1}\cdots\theta_K^{\alpha-1} $$</p><p>先验信息引入之后，我们来看一下后验概率：</p><p>$$P(\theta|y_1, y_2, …, y_N)=\frac{P(\theta, y_1, y_2, …, y_N)}{P(y_1, y_2, …, y_N)}\\<br>\propto P(\theta, y_1, y_2, …, y_N)\\<br>=P(y_1, y_2, …, y_N|\theta)P(\theta)\\<br>=\theta_1^{m_1}\theta_2^{m_2}\cdots\theta_K^{m_K}\theta_1^{\alpha-1}\theta_2^{\alpha-1}\cdots\theta_K^{\alpha-1}\\<br>=\theta_1^{\alpha+m_1-1}\theta_2^{\alpha+m_2-1}\cdots\theta_K^{\alpha+m_K-1}$$</p><p>即：$P(\theta|y_1, y_2, …, y_N)\propto\theta_1^{\alpha+m_1-1}\theta_2^{\alpha+m_2-1}\cdots\theta_K^{\alpha+m_K-1}$，极大化后验概率，即需要极大化 $\theta_1^{\alpha+m_1-1}\theta_2^{\alpha+m_2-1}\cdots\theta_K^{\alpha+m_K-1}$，先取对数，其求解过程与极大似然估计部分相同（建议读者自行动笔试一试），这里直接给出结论：</p><p>$$P(Y=c_k)=\theta_{c_k}=\frac{m_k+(\alpha-1)}{N+K(\alpha-1)}$$</p><p>取 $\alpha-1=\eta$，得：</p><p>$$P(Y=c_k)=\theta_{c_k}=\frac{m_k+\eta}{N+K\cdot\eta}$$</p><p>同理，若设特征向量 $x$ 的第 $i$ 个分量 $x_i$ 的取值共有 $\Lambda$ 种情况，可以得到条件概率 $P(X^{(i)}=x_i|Y=c_k)$ 的贝叶斯估计为：</p><p>$$P(X^{(i)}=x_i|Y=c_k)=\frac{t_i+\eta}{S+\Lambda\cdot\eta}$$</p><p>当 $\eta=0$ 时，贝叶斯估计就是极大似然估计。 当 $\eta=1$ 时，以上过程称为拉普拉斯平滑。可以看到，拉普拉斯平滑能有效避免上文所提到的极大似然估计面对的极端情形。</p><p>以上我们分别通过极大似然估计和贝叶斯估计两种方法，解决了将输入的特征向量归属为最大的后验概率所属类别时所需要解决的参数估计问题。那么，接下来一个问题是，为什么我们要把输入的特征向量归属为最大的后验概率所属的类别呢？上文我们给出了直觉的解释，下面我们将从数学的角度，严格证明为什么这么做是对的。</p><h3 id="为什么这种解决方法是有效的"><a href="#为什么这种解决方法是有效的" class="headerlink" title="为什么这种解决方法是有效的"></a>为什么这种解决方法是有效的</h3><p>对于机器学习问题，我们通常使用损失函数来度量机器学习模型的好坏程度。对于分类问题，我们通常使用 $0-1$ 损失。若将分类模型看做由输入的特征向量到输出类别的一个映射，我们可以将分类过程写成由 $f(X) \to Y$ 的函数形式，$Y^*$ 是该特征向量的类别标签，$Y^*\in ${$c_1, c_2, …, c_K$}$ $，则 $0-1$ 损失定义为：</p><p>$$L(f(X),Y^*)=<br>\begin{cases}<br>0&amp; \text{$f(X)$ = $Y^*$}\\<br>1&amp; \text{$f(x)$ $\neq$ $Y^*$}<br>\end{cases}$$</p><p>以上损失函数是我们的经验风险函数。我们的目的是希望模型在未知数据上的表现比较好，所以我们希望将期望风险最小化。其过程如下：</p><p>$$\mathbb{E}(L(f(X), Y^*)=\sum_{Y^*}\sum_{X}L(f(X), Y^*)P(X, Y^*)\\<br>=\sum_{Y^*}\sum_{X}L(f(X), Y^*)P(Y^*|X)P(X)\\<br>=\sum_{X}[\sum_{Y^*}L(f(X), Y^*)P(Y^*|X)]P(X)$$</p><p>要最小化期望风险，只需对于每一个 $X$，最小化上式中括号的部分即可。$I(\blacktriangle)$ 是一个指示函数，当 $\blacktriangle$ 代表的内容为真时，该指示函数取值为 $1$，否则取值为 $0$。进一步求解如下：</p><p>$$<br>\sum_{Y^*}L(f(X), Y^*)P(Y^*|X)\\<br>=\sum_{Y^*}I(f(X) \neq Y^*)P(Y^*|X)\\<br>=\sum_{Y^*}(1-I(f(X) = Y^*))P(Y^*|X)\\<br>=1-I(f(X) = Y^*)P(Y^*|X)<br>$$</p><p>要想最小化上式，只需最大化 $I(f(X) = Y^*)P(Y^*|X)$ 即可，而最大化此式就等价于将 $X$ 归属为后验概率最大的类别中。由此证明朴素贝叶斯算法将输入的特征向量归属到后验概率最大的类中是有效的。 </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;朴素贝叶斯算法主要解决什么问题？它是怎么解决这类问题的？如何证明这种解决方法是有效的？本节将从这三方面详细介绍朴素贝叶斯算法的相关内容。&lt;/p&gt;
&lt;h3 id=&quot;朴素贝叶斯算法主要解决什么问题&quot;&gt;&lt;a href=&quot;#朴素贝叶斯算法主要解决什么问题&quot; class=&quot;headerlink&quot; title=&quot;朴素贝叶斯算法主要解决什么问题&quot;&gt;&lt;/a&gt;朴素贝叶斯算法主要解决什么问题&lt;/h3&gt;&lt;p&gt;朴素贝叶斯算法主要用来处理分类问题，也就是说，我们给算法某个对象的输入特征（通常是向量形式），算法能够根据这个输入特征判断含有这种特征的对象属于预先定义的哪一类。我们所熟悉的垃圾邮件分类问题是朴素贝叶斯算法的典型应用场景。为方便下文论述，我们首先对使用朴素贝叶斯算法解决分类问题进行数学建模，设分类问题的输入空间 $\chi \subseteq \mathbb{R}^n$，输出空间 $\mathbb{y} = ${$c_1, c_2, …, c_K$}$ $，即输入空间定义为 $n$ 维实数向量的子空间，输出空间包含了 $K$ 个预先定义的类别。某一特定问题的输入 $x \in \chi$，输出 $y \in \mathbb{y}$，即朴素贝叶斯算法建立了一个从某个对象提取的特征向量（这里的特征向量和线性代数里面的特征向量是不同的概念，这里指的是能反映该对象特征的向量）到该对象所属类别的一个映射。 &lt;/p&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="朴素贝叶斯算法" scheme="http://blog.keeplearning.group/tags/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十六）</title>
    <link href="http://blog.keeplearning.group/2018/11/21/2018/11-21-pytorch16/"/>
    <id>http://blog.keeplearning.group/2018/11/21/2018/11-21-pytorch16/</id>
    <published>2018-11-21T00:42:00.000Z</published>
    <updated>2018-11-21T02:20:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>gerattr</code>、<code>setattr</code> 与 <code>__getattr__</code>、<code>__setattr__</code>。  </p><ul><li><code>result = obj.name</code> 会调用 buildin 函数 <code>getattr(obj, &#39;name&#39;)</code>，如果该属性找不到，会调用 <code>obj.__getattr__(&#39;name&#39;)</code>，没有实现 <code>__getattr__</code> 或者 <code>__getattr__</code> 也无法处理的就会 <code>raise AttributeError</code>;</li><li><code>obj.name = value</code> 会调用 buildin 函数 <code>setattr(obj, &#39;name&#39;, value)</code>，如果 obj 对象实现了 <code>__setattr__</code> 方法，<code>setattr</code> 会直接调用 <code>obj.__setattr__(&#39;name&#39;, value)</code>。</li></ul><p><code>nn.Module</code> 实现了自定义的 <code>__setattr__</code> 函数，当执行 <code>module.name = value</code> 时，会在 <code>__setattr__</code> 中判断 value 是否为 <code>Parameter</code> 或 <code>nn.Module</code> 对象，如果是则将这些对象加到 <code>_parameters</code> 和 <code>_modules</code> 这两个字典中，而如果是其他类型的对象，如 <code>Variable</code>、<code>list</code>、<code>dict</code> 等，则调用默认的操作，将这个值保存在 <code>__dict__</code> 中。  </p><p> Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">module = nn.Module()</span><br><span class="line">module.param = nn.Parameter(t.ones(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">print(module._parameters)</span><br><span class="line"></span><br><span class="line">submodule1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">submodule2 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">module_list = [submodule1, submodule2]</span><br><span class="line"><span class="comment"># 对于 list 对象，调用 buildin 函数，保存在 __dict__ 中</span></span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'_modules: '</span>, module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']:"</span>, module.__dict__.get(<span class="string">'submodules'</span>))</span><br><span class="line"></span><br><span class="line">module_list = nn.ModuleList(module_list)</span><br><span class="line">module.submodules = module_list</span><br><span class="line">print(<span class="string">'ModuleList is instance of nn.Module: '</span>, isinstance(module_list, nn.Module))</span><br><span class="line">print(<span class="string">'_modules: '</span>, module._modules)</span><br><span class="line">print(<span class="string">"__dict__['submodules']: "</span>, module.__dict__.get(<span class="string">'submodules'</span>))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542763477/Snip20181121_2.png" alt="">  </p><p>因 <code>_modules</code> 和 <code>_parameters</code> 中的 item 未保存在 <strong>dict</strong> 中，所以默认的 <code>getattr</code> 方法无法获取它，因而 <code>nn.Module</code> 实现了自定义的 <code>__getattr__</code> 方法，如果默认的 <code>getattr</code> 无法处理，就调用自定义的 <code>__getattr__</code> 方法，尝试从 <code>_modules</code>、<code>_parameters</code> 和 <code>_buffers</code> 这三个字典中获取。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">print(getattr(module, <span class="string">'training'</span>))  <span class="comment"># 等价于 module.training</span></span><br><span class="line"><span class="comment"># error</span></span><br><span class="line"><span class="comment"># print(module.__getattr__('training'))</span></span><br><span class="line"></span><br><span class="line">module.attr1 = <span class="number">2</span></span><br><span class="line">print(getattr(module, <span class="string">'attr1'</span>))</span><br><span class="line"><span class="comment"># error</span></span><br><span class="line"><span class="comment"># print(module.__getattr__('attr1'))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 即 module.param，会调用 module.__getattr__('param')</span></span><br><span class="line">print(getattr(module, <span class="string">'param'</span>))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542764618/Snip20181121_3.png" alt=""><br><a id="more"></a></p><p>2 . 在 PyTorch 中保存模型十分简单，所有的 Module 对象都具有 <code>state_dict()</code> 函数，返回当前 Module 所有的状态数据，将这些状态数据保存后，下次使用模型时即可利用 <code>model.load_state_dict()</code> 函数将状态加载进来。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">t.save(net.state_dict(), <span class="string">'net.pth'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载已保存的模型</span></span><br><span class="line">net2 = Net()</span><br><span class="line">net2.load_state_dict(t.load(<span class="string">'net.pth))</span></span><br></pre></td></tr></table></figure><p>3 . 将 Module 放在 GPU 上运行只需两步：  </p><ul><li><code>model = model.cuda()</code>：将模型的所有参数都转存到 GPU 上；</li><li><code>input.cuda()</code>：将输入数据也放置到 GPU 上。</li></ul><p>如何在多个 GPU 上并行计算，PyTorch 也提供了两个函数，可实现简单高效的并行 GPU 计算：  </p><ul><li><code>nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None</code>)</li><li><code>class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)</code></li></ul><p>二者的参数十分相似，通过 <code>device_ids</code> 参数可以指定在哪些 GPU 上进行优化，<code>output_device</code> 指定输出到哪个 GPU 上，唯一的不同就在于前者直接利用多GPU并行计算得出结果，而后者返回一个新的 Module，能够自动在多GPU上进行并行计算。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># method1</span></span><br><span class="line">output = nn.parallel.data_parallel(net, input, device_ids=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># method2</span></span><br><span class="line">new_net = nn.DataParallel(net, device_ids=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">output = new_net(input)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;gerattr&lt;/code&gt;、&lt;code&gt;setattr&lt;/code&gt; 与 &lt;code&gt;__getattr__&lt;/code&gt;、&lt;code&gt;__setattr__&lt;/code&gt;。  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;result = obj.name&lt;/code&gt; 会调用 buildin 函数 &lt;code&gt;getattr(obj, &amp;#39;name&amp;#39;)&lt;/code&gt;，如果该属性找不到，会调用 &lt;code&gt;obj.__getattr__(&amp;#39;name&amp;#39;)&lt;/code&gt;，没有实现 &lt;code&gt;__getattr__&lt;/code&gt; 或者 &lt;code&gt;__getattr__&lt;/code&gt; 也无法处理的就会 &lt;code&gt;raise AttributeError&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;obj.name = value&lt;/code&gt; 会调用 buildin 函数 &lt;code&gt;setattr(obj, &amp;#39;name&amp;#39;, value)&lt;/code&gt;，如果 obj 对象实现了 &lt;code&gt;__setattr__&lt;/code&gt; 方法，&lt;code&gt;setattr&lt;/code&gt; 会直接调用 &lt;code&gt;obj.__setattr__(&amp;#39;name&amp;#39;, value)&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;nn.Module&lt;/code&gt; 实现了自定义的 &lt;code&gt;__setattr__&lt;/code&gt; 函数，当执行 &lt;code&gt;module.name = value&lt;/code&gt; 时，会在 &lt;code&gt;__setattr__&lt;/code&gt; 中判断 value 是否为 &lt;code&gt;Parameter&lt;/code&gt; 或 &lt;code&gt;nn.Module&lt;/code&gt; 对象，如果是则将这些对象加到 &lt;code&gt;_parameters&lt;/code&gt; 和 &lt;code&gt;_modules&lt;/code&gt; 这两个字典中，而如果是其他类型的对象，如 &lt;code&gt;Variable&lt;/code&gt;、&lt;code&gt;list&lt;/code&gt;、&lt;code&gt;dict&lt;/code&gt; 等，则调用默认的操作，将这个值保存在 &lt;code&gt;__dict__&lt;/code&gt; 中。  &lt;/p&gt;
&lt;p&gt; Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;module = nn.Module()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;module.param = nn.Parameter(t.ones(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(module._parameters)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;submodule1 = nn.Linear(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;submodule2 = nn.Linear(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;module_list = [submodule1, submodule2]&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 对于 list 对象，调用 buildin 函数，保存在 __dict__ 中&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;module.submodules = module_list&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&#39;_modules: &#39;&lt;/span&gt;, module._modules)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;__dict__[&#39;submodules&#39;]:&quot;&lt;/span&gt;, module.__dict__.get(&lt;span class=&quot;string&quot;&gt;&#39;submodules&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;module_list = nn.ModuleList(module_list)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;module.submodules = module_list&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&#39;ModuleList is instance of nn.Module: &#39;&lt;/span&gt;, isinstance(module_list, nn.Module))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&#39;_modules: &#39;&lt;/span&gt;, module._modules)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(&lt;span class=&quot;string&quot;&gt;&quot;__dict__[&#39;submodules&#39;]: &quot;&lt;/span&gt;, module.__dict__.get(&lt;span class=&quot;string&quot;&gt;&#39;submodules&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542763477/Snip20181121_2.png&quot; alt=&quot;&quot;&gt;  &lt;/p&gt;
&lt;p&gt;因 &lt;code&gt;_modules&lt;/code&gt; 和 &lt;code&gt;_parameters&lt;/code&gt; 中的 item 未保存在 &lt;strong&gt;dict&lt;/strong&gt; 中，所以默认的 &lt;code&gt;getattr&lt;/code&gt; 方法无法获取它，因而 &lt;code&gt;nn.Module&lt;/code&gt; 实现了自定义的 &lt;code&gt;__getattr__&lt;/code&gt; 方法，如果默认的 &lt;code&gt;getattr&lt;/code&gt; 无法处理，就调用自定义的 &lt;code&gt;__getattr__&lt;/code&gt; 方法，尝试从 &lt;code&gt;_modules&lt;/code&gt;、&lt;code&gt;_parameters&lt;/code&gt; 和 &lt;code&gt;_buffers&lt;/code&gt; 这三个字典中获取。  &lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;print(getattr(module, &lt;span class=&quot;string&quot;&gt;&#39;training&#39;&lt;/span&gt;))  &lt;span class=&quot;comment&quot;&gt;# 等价于 module.training&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# error&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# print(module.__getattr__(&#39;training&#39;))&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;module.attr1 = &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(getattr(module, &lt;span class=&quot;string&quot;&gt;&#39;attr1&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# error&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# print(module.__getattr__(&#39;attr1&#39;))&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 即 module.param，会调用 module.__getattr__(&#39;param&#39;)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(getattr(module, &lt;span class=&quot;string&quot;&gt;&#39;param&#39;&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:  &lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://res.cloudinary.com/dtfjnb8ft/image/upload/v1542764618/Snip20181121_3.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十五）</title>
    <link href="http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch15/"/>
    <id>http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch15/</id>
    <published>2018-11-20T10:34:00.000Z</published>
    <updated>2018-11-20T13:19:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>nn</code> 中的大多数 layer，在 <code>nn.functional</code> 中都有一个与之相对应的函数，<code>nn.functional</code> 中的函数和 <code>nn.Module</code> 的主要区别在于，用 <code>nn.Module</code> 实现的 layers 是一个特殊的类，都是由 <code>class layer(nn.Module)</code> 定义，会自动提取可学习的参数，而 <code>nn.functional</code> 中的函数更像是纯函数，由 <code>def function(input)</code> 定义。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">model = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">output1 = model(input)</span><br><span class="line">output2 = nn.functional.linear(input, model.weight, model.bias)</span><br><span class="line">print(output1 == output2)</span><br><span class="line"></span><br><span class="line">b = nn.functional.relu(input)</span><br><span class="line">b2 = nn.ReLU()(input)</span><br><span class="line">print(b == b2)</span><br></pre></td></tr></table></figure><p>Output:<br><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxeq3vk9axj20wy05a74n.jpg" alt="">  </p><p>如果模型有可学习的参数，最好用 <code>nn.Module</code>，否则既可以用 <code>nn.Module</code> 也可以使用 <code>nn.functional</code>，二者在性能上没有太大差异。但 dropout 操作虽然没有可学习的参数，但还是建议使用 <code>nn.Dropout</code> 而不是 <code>nn.functional.dropout</code>，因为 dropout 在训练和测试两个阶段的行为有所差异，使用 <code>nn.Module</code> 对象能够通过 <code>model.eval</code> 操作加以区分。  </p><p>在模型中搭配使用 <code>nn.Module</code> 和 <code>nn.functional</code>：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.pool(F.relu(self.conv1(x)), <span class="number">2</span>)</span><br><span class="line">        x = F.pool(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数 <code>__init__</code> 中。<br><a id="more"></a><br>2 . 在深度学习中参数的初始化非常重要，良好的初始化能让模型更快收敛，并达到更高水平，而糟糕的初始化则可能使模型迅速瘫痪。<code>nn.Module</code> 模块的参数都采取了较为合理的初始化策略，因此一般不需要我们考虑。而当我们使用 Parameter 时，自定义初始化尤其重要，PyTorch 中 <code>nn.init</code> 模块就是专门为初始化而设计的，如果某种初始化策略 <code>nn.init</code> 不提供，用户也可以自己直接初始化。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用 nn.init 初始化</span></span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line">init.xavier_normal_(linear.weight)  <span class="comment"># 等价于 linear.weight.data.normal_(0, std)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接初始化</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">t.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xavier 初始化的计算公式</span></span><br><span class="line">std = math.sqrt(<span class="number">2</span>)/math.sqrt(<span class="number">7.</span>)</span><br><span class="line">linear.weight.data.normal_(<span class="number">0</span>, std)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对模型的所有参数进行初始化</span></span><br><span class="line"><span class="keyword">for</span> name, params <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> name.find(<span class="string">'linear'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="comment"># init linear</span></span><br><span class="line">        params[<span class="number">0</span>] <span class="comment"># weight</span></span><br><span class="line">        params[<span class="number">1</span>] <span class="comment"># bias</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'conv'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">elif</span> name.find(<span class="string">'norm'</span>) != <span class="number">-1</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>3 . <code>nn.Module</code> 基类的构造函数：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self._parameters = OrderedDict()</span><br><span class="line">    self._modules = OrderedDict()</span><br><span class="line">    self._buffers = OrderedDict()</span><br><span class="line">    self._backward_hooks = OrderedDict()</span><br><span class="line">    self._forward_hooks = OrderedDict()</span><br><span class="line">    self.training = <span class="keyword">True</span></span><br></pre></td></tr></table></figure><ul><li><code>_parameters</code>：保存用户直接设置的 <code>Parameter</code>;</li><li><code>_modules</code>：指定的子 module 会保存于此；</li><li><code>_buffers</code>：缓存，如 batchnorm 使用 momentum 机制，每次前向传播需用到上一次前向传播的结果；</li><li><code>_backward_hooks</code> 与 <code>_forward_hooks</code>：钩子技术，用来提取中间变量，类似 variable 的 hook；</li><li><code>training</code>：BatchNorm 与 Dropout 层在训练阶段和测试阶段会分别采取不同的策略，通过判断 training 的值来决定前向传播策略。  </li></ul><p>上述几个属性中，<code>_parameters</code>、<code>_modules</code> 和 <code>_buffers</code> 这三个字典中的值，都可以通过 <code>self.key</code> 方式获得，效果等价于 <code>self._parameters[key]</code>。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># 等价于 self.register_parameter('param1', nn.Patameter(t.rand(3, 3)))</span></span><br><span class="line">        self.param1 = nn.Parameter(t.rand(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">        self.submodel1 = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        x = self.param1.mm(input)</span><br><span class="line">        x = self.submodel1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(<span class="string">'-------- Net 结构 --------'</span>)</span><br><span class="line">print(net)</span><br><span class="line">print(<span class="string">'-------- Net 结构 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的子 module --------'</span>)</span><br><span class="line">print(net._modules)</span><br><span class="line">print(<span class="string">'-------- Net 中的子 module --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的自定义参数 --------'</span>)</span><br><span class="line">print(net._parameters)</span><br><span class="line">print(<span class="string">'-------- Net 中的自定义参数 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中 para1 的参数 --------'</span>)</span><br><span class="line">print(net.param1)  <span class="comment"># 等价于 net._parameters['param1']</span></span><br><span class="line">print(<span class="string">'-------- Net 中 para1 的参数 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的参数大小 --------'</span>)</span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    print(name, parameter.size())</span><br><span class="line">print(<span class="string">'-------- Net 中的参数大小 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'-------- Net 中的子 module 及其名称 --------'</span>)</span><br><span class="line"><span class="keyword">for</span> name, submodel <span class="keyword">in</span> net.named_modules():</span><br><span class="line">    print(name, submodel)</span><br><span class="line">print(<span class="string">'-------- Net 中的子 module 及其名称 --------'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>)</span><br><span class="line"></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">2</span>)</span><br><span class="line">input = t.rand(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">output = bn(input)</span><br><span class="line">print(bn._buffers)</span><br></pre></td></tr></table></figure><p><code>nn.Module</code> 在实际使用中可能层层嵌套，一个 module 包含若干个子 module，每一个子 module 又包含更多的子 module，<code>children</code> 方法可以查看直接子 module，<code>module</code> 函数可以查看所有的子 module（包含当前 module）。与之相对应的还有函数 <code>named_children</code> 和 <code>named_modules</code>，其能够在返回 module 列表的同时返回它们的名字。  </p><p>dropout 在训练和测试阶段采取不同策略举例：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input = t.arange(<span class="number">0</span>, <span class="number">12</span>).view(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">model = nn.Dropout()</span><br><span class="line"><span class="comment"># 在训练阶段，会有一半左右的数被随机置为 0</span></span><br><span class="line">print(model(input))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试阶段，dropout 什么都不做</span></span><br><span class="line">model.training = <span class="keyword">False</span></span><br><span class="line">model(input)</span><br></pre></td></tr></table></figure><p>如果一个模型具有多个 dropout 层，不需要为每个 dropout 层指定 training 属性，更为推荐的做法是调用 <code>model.train()</code> 函数，它会将当前 module 及其子 module 中的所有 training 属性都设置为 True，相应的，<code>model.eval()</code> 函数会把 <code>training</code> 属性都设为 False。  </p><p><code>register_forward_hook</code> 与 <code>register_backward_hook</code>，这两个函数的功能类似于 <code>variable</code> 函数的 <code>register_hook</code>，可在 module 前向传播或反向传播时注册钩子。每次前向传播执行结束后会执行钩子函数（hook）。前向传播的钩子函数具有如下形式：<code>hook(module, input, output) -&gt; None</code>，而反向传播则具有如下形式：<code>hook(module, grad_input, grad_output) -&gt; Tensor or None</code>。钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载。钩子函数主要用在获取某些中间结果的情景，如中间某一层的输出或某一层的梯度。这些结果本应写在 forward 函数中，但如果在 forward 函数中专门加上这些处理，可能会使处理逻辑比较复杂，这时候使用钩子技术就更合适一些。  </p><p>下面考虑一种场景，有一个预训练好的模型，需要提取模型的某一层（不是最后一层）的输出作为特征进行分类，但又不希望修改其原有的模型定义文件，这时就可以利用钩子函数。下面给出实现的伪代码。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = VGG()</span><br><span class="line">features = t.Tensor()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hook</span><span class="params">(module, input, output)</span>:</span></span><br><span class="line">    <span class="string">'''把这层的输出拷贝到 features 中'''</span></span><br><span class="line">    features.copy_(output.data)</span><br><span class="line">    </span><br><span class="line">handle = model.layer8.register_forward_hook(hook)</span><br><span class="line">_ = model(input)</span><br><span class="line"><span class="comment"># 用完 hook 后删除</span></span><br><span class="line">handle.remove()</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;nn&lt;/code&gt; 中的大多数 layer，在 &lt;code&gt;nn.functional&lt;/code&gt; 中都有一个与之相对应的函数，&lt;code&gt;nn.functional&lt;/code&gt; 中的函数和 &lt;code&gt;nn.Module&lt;/code&gt; 的主要区别在于，用 &lt;code&gt;nn.Module&lt;/code&gt; 实现的 layers 是一个特殊的类，都是由 &lt;code&gt;class layer(nn.Module)&lt;/code&gt; 定义，会自动提取可学习的参数，而 &lt;code&gt;nn.functional&lt;/code&gt; 中的函数更像是纯函数，由 &lt;code&gt;def function(input)&lt;/code&gt; 定义。  &lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;model = nn.Linear(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output1 = model(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output2 = nn.functional.linear(input, model.weight, model.bias)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(output1 == output2)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = nn.functional.relu(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b2 = nn.ReLU()(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(b == b2)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;Output:&lt;br&gt;&lt;img src=&quot;http://wx1.sinaimg.cn/mw690/79225320gy1fxeq3vk9axj20wy05a74n.jpg&quot; alt=&quot;&quot;&gt;  &lt;/p&gt;
&lt;p&gt;如果模型有可学习的参数，最好用 &lt;code&gt;nn.Module&lt;/code&gt;，否则既可以用 &lt;code&gt;nn.Module&lt;/code&gt; 也可以使用 &lt;code&gt;nn.functional&lt;/code&gt;，二者在性能上没有太大差异。但 dropout 操作虽然没有可学习的参数，但还是建议使用 &lt;code&gt;nn.Dropout&lt;/code&gt; 而不是 &lt;code&gt;nn.functional.dropout&lt;/code&gt;，因为 dropout 在训练和测试两个阶段的行为有所差异，使用 &lt;code&gt;nn.Module&lt;/code&gt; 对象能够通过 &lt;code&gt;model.eval&lt;/code&gt; 操作加以区分。  &lt;/p&gt;
&lt;p&gt;在模型中搭配使用 &lt;code&gt;nn.Module&lt;/code&gt; 和 &lt;code&gt;nn.functional&lt;/code&gt;：  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch.nn &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; functional &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; F&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nn.Module)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(Net, self).__init__()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv1 = nn.Conv2d(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.conv2 = nn.Conv2d(&lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc1 = nn.Linear(&lt;span class=&quot;number&quot;&gt;16&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;120&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc2 = nn.Linear(&lt;span class=&quot;number&quot;&gt;120&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;84&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.fc3 = nn.Linear(&lt;span class=&quot;number&quot;&gt;84&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.pool(F.relu(self.conv1(x)), &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.pool(F.relu(self.conv2(x)), &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = x.view(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;16&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt; * &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.relu(self.fc1(x))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = F.relu(self.fc2(x))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = self.fc3(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; x&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;对于不具备可学习参数的层（激活层、池化层等），将它们用函数代替，这样则可以不用放置在构造函数 &lt;code&gt;__init__&lt;/code&gt; 中。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十四）</title>
    <link href="http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch14/"/>
    <id>http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch14/</id>
    <published>2018-11-20T06:47:00.000Z</published>
    <updated>2018-11-20T10:16:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . PyTorch 实现了如今最常用的三种循环神经网络（RNN）：RNN(vanilla RNN)、LSTM 和 GRU，此外还有对应的三种 RNNCell，RNN 和 RNNCell 层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上 RNN 层的一种后端实现方式就是调用 RNNCell 来实现的。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"><span class="comment"># 输入：batch_size=3，序列长度都为 2，序列中每个元素占 4 维</span></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># lstm 输入向量 4 维，隐藏元 3，1 层</span></span><br><span class="line">lstm = nn.LSTM(<span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 初始状态：1 层，batch_size=3，3 个隐藏元</span></span><br><span class="line">h0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">c0 = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out, hn = lstm(input, (h0, c0))</span><br><span class="line">print(out)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 一个 LSTMCell 对应的层数只能是一层</span></span><br><span class="line">lstm = nn.LSTMCell(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">hx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">cx = t.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">out = []</span><br><span class="line"><span class="keyword">for</span> i_ <span class="keyword">in</span> input:</span><br><span class="line">    hx, cx = lstm(i_, (hx, cx))</span><br><span class="line">    out.append(hx)</span><br><span class="line">t.stack(out)</span><br></pre></td></tr></table></figure><p>词向量在自然语言中应用十分普及，PyTorch 同样提供了 Embedding 层。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 有 4 个词，每个词用 5 维的向量表示</span></span><br><span class="line">embedding = nn.Embedding(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 可以用训练好的词向量初始化 embedding</span></span><br><span class="line">embedding.weight.data = t.arange(<span class="number">0</span>, <span class="number">20</span>).view(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = t.arange(<span class="number">3</span>, <span class="number">0</span>, <span class="number">-1</span>).long()</span><br><span class="line">output = embedding(input)</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 在深度学习中要用到各种各样的损失函数（loss function），这些损失函数可以看成是一种特殊的 layer，PyTorch 也将这些损失函数实现为 <code>nn.Module</code> 的子类，然而在实际应用中通常将这些 loss function 专门提取出来，和主模型互相独立。  </p><p>交叉熵损失（CrossEntropyLoss)：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># batch_size=3，计算对应每个类别的分数（只有两个类别）</span></span><br><span class="line">score = t.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 三个样本分别属于 1， 0， 1 类，label 必须是 LongTensor</span></span><br><span class="line">label = t.Tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]).long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># loss 与普通的 layer 无差异</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(score, label)</span><br><span class="line">print(loss)</span><br></pre></td></tr></table></figure><p>3 . PyTorch 将深度学习中常用的优化方法全部封装在 <code>torch.optim</code> 中，能够方便地扩展成自定义的优化方法，所有的优化方法都是继承基类 <code>optim.Optimizer</code>，并实现了自己的优化步骤，下面以随机梯度下降（SGD）说明：  </p><ul><li>优化方法的基本使用方法；</li><li>如何对模型的不同部分设置不同的学习率；</li><li>如何调整学习率。  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义一个 LeNet 网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.features = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        )</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">120</span>, <span class="number">84</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.features(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = self.classifier(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(params=net.parameters(), lr=<span class="number">1</span>)</span><br><span class="line">optimizer.zero_grad()  <span class="comment"># 梯度清零，等价于 net.zero_grad()</span></span><br><span class="line"></span><br><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = net(input)</span><br><span class="line">output.backward(output)  <span class="comment"># fake backward</span></span><br><span class="line"></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p>为不同的子网络设置不同的学习率，在 finetune 中经常用到，如果对某个参数不指定学习率，就使用最外层的默认学习率。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>: <span class="number">1e-2</span>&#125;</span><br><span class="line">], lr=<span class="number">1e-5</span>)</span><br><span class="line"></span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure><p>只为两个全连接层设置较大的学习率，其余层的学习率较小。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">special_layers = nn.ModuleList([net.classifier[<span class="number">0</span>], net.classifier[<span class="number">3</span>]])</span><br><span class="line">special_layers_params = list(map(id, special_layers.parameters()))</span><br><span class="line">base_params = filter(<span class="keyword">lambda</span> p: id(p) <span class="keyword">not</span> <span class="keyword">in</span> special_layers_params, net.parameters())</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: base_params&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: special_layers.parameters(), <span class="string">'lr'</span>: <span class="number">0.01</span>&#125;</span><br><span class="line">], lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure><p><code>id</code>：用于获取对象的内存地址。<br><code>map(function, iterable,...)</code>: 第一个参数 <code>function</code> 以参数序列中的每一个元素调用 <code>function</code> 函数，返回包含每次 <code>function</code> 函数返回值的新列表。<br><code>filter(function, iterable)</code>：用于过滤掉不符合条件的元素，返回由符合条件的元素组成的新列表。<code>iterable</code> 是可迭代对象。  </p><p>对于如何调整学习率，主要有两种做法，一种是修改 <code>optimizer.param_groups</code> 中对应的学习率，另一种是更简单也是较为推荐的做法 —— 新建优化器，但是后者对于使用动量的优化器（如 Adam），会丢失动量状态信息，可能会造成损失函数的收敛出现震荡等情况。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法1：调整学习率，手动 decay，保存动量</span></span><br><span class="line"><span class="keyword">for</span> param_group <span class="keyword">in</span> optimizer.param_groups:</span><br><span class="line">    param_group[<span class="string">'lr'</span>] *= <span class="number">0.1</span>   <span class="comment"># 学习率为之前的 0.1 倍</span></span><br><span class="line">print(optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法2：调整学习率，新建一个 optimizer</span></span><br><span class="line">old_lr = <span class="number">0.1</span></span><br><span class="line">optimizer1 = optim.SGD([</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.features.parameters()&#125;,</span><br><span class="line">    &#123;<span class="string">'params'</span>: net.classifier.parameters(), <span class="string">'lr'</span>: old_lr * <span class="number">0.1</span>&#125;</span><br><span class="line">    ], lr=<span class="number">1e-5</span>)</span><br><span class="line">print(optimizer1)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . PyTorch 实现了如今最常用的三种循环神经网络（RNN）：RNN(vanilla RNN)、LSTM 和 GRU，此外还有对应的三种 RNNCell，RNN 和 RNNCell 层的区别在于前者能够处理整个序列，而后者一次只处理序列中一个时间点的数据，前者封装更完备更易于使用，后者更具灵活性。实际上 RNN 层的一种后端实现方式就是调用 RNNCell 来实现的。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;t.manual_seed(&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 输入：batch_size=3，序列长度都为 2，序列中每个元素占 4 维&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# lstm 输入向量 4 维，隐藏元 3，1 层&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lstm = nn.LSTM(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 初始状态：1 层，batch_size=3，3 个隐藏元&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;h0 = t.randn(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c0 = t.randn(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;out, hn = lstm(input, (h0, c0))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(out)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;t.manual_seed(&lt;span class=&quot;number&quot;&gt;1000&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 一个 LSTMCell 对应的层数只能是一层&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lstm = nn.LSTMCell(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hx = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;cx = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;out = []&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; i_ &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; input:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    hx, cx = lstm(i_, (hx, cx))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    out.append(hx)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;t.stack(out)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;词向量在自然语言中应用十分普及，PyTorch 同样提供了 Embedding 层。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 有 4 个词，每个词用 5 维的向量表示&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;embedding = nn.Embedding(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 可以用训练好的词向量初始化 embedding&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;embedding.weight.data = t.arange(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;20&lt;/span&gt;).view(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;5&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.arange(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;).long()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output = embedding(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(output)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十三）</title>
    <link href="http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch13/"/>
    <id>http://blog.keeplearning.group/2018/11/20/2018/11-20-pytorch13/</id>
    <published>2018-11-20T00:56:00.000Z</published>
    <updated>2018-11-20T04:43:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 图像的卷积操作。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToTensor, ToPILImage</span><br><span class="line"></span><br><span class="line">to_tensor = ToTensor()</span><br><span class="line">to_pil = ToPILImage()</span><br><span class="line">lena = Image.open(<span class="string">'path to your image'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入是一个 batch，batch_size=1</span></span><br><span class="line">input = to_tensor(lena).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 锐化卷积核</span></span><br><span class="line">kernel = t.ones(<span class="number">3</span>, <span class="number">3</span>)/<span class="number">-9</span></span><br><span class="line">kernel[<span class="number">1</span>][<span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">conv = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, <span class="number">3</span>), <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">conv.weight.data = kernel.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">out = conv(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 池化层可以看作是一种特殊的卷积层，用来下采样，但池化层没有可学习的参数，其 weight 是固定的。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pool = nn.AvgPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">print(list(pool.parameters()))  <span class="comment"># []</span></span><br><span class="line"></span><br><span class="line">out = pool(input)</span><br><span class="line">to_pil(out.data.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>3 . 除了卷积层和池化层，深度学习中还将常用到以下几层：  </p><ul><li>Linear：全连接层；</li><li>BatchNorm：批规范化层，分为 1D、2D 和 3D。除了标准的 BatchNorm 之外，还有在风格迁移中常用到的 InstanceNorm 层；</li><li>Dropout：dropout 层用来防止过拟合，同样分为 1D、2D 和 3D。  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入 batch_size = 2，维度３</span></span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">linear = nn.Linear(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">h = linear(input)</span><br><span class="line">print(h)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4 channel，初始化标准差为 4，均值为 0</span></span><br><span class="line">bn = nn.BatchNorm1d(<span class="number">4</span>)</span><br><span class="line">bn.weight.data = t.ones(<span class="number">4</span>) * <span class="number">4</span></span><br><span class="line">bn.bias.data = t.zeros(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">bn_out = bn(h)</span><br><span class="line"><span class="comment"># 注意输出的均值和方差</span></span><br><span class="line"><span class="comment"># 方差是标准差的平方，计算无偏方差分母会减 1</span></span><br><span class="line"><span class="comment"># 使用 unbiased=False，分母不减 1</span></span><br><span class="line">print(bn_out.mean(<span class="number">0</span>), bn_out.var(<span class="number">0</span>, unbiased=<span class="keyword">False</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个元素以 0.5 的概率舍弃</span></span><br><span class="line">dropout = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">o = dropout(bn_out)</span><br><span class="line">print(o)  <span class="comment"># 有一半的数变为０</span></span><br></pre></td></tr></table></figure><p>4 . PyTorch 实现了常见的激活函数，这些激活函数可作为独立的 layer 使用，这里介绍一下常用的激活函数 ReLU，其数学表达式为 $ReLU(x)=max(0, x)$。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">relu = nn.ReLU(inplace=<span class="keyword">True</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(input)</span><br><span class="line">output = relu(input)  <span class="comment"># 等价于 input.clamp(min=0)</span></span><br><span class="line">print(output)   <span class="comment"># 小于 0  的被截断为 0</span></span><br></pre></td></tr></table></figure><p>ReLU 函数有个 inplace 参数，如果设置为 True，它会把输出直接覆盖到输入中，这样可以节省内存/显存。之所以可以覆盖是因为在计算 ReLU 的反向传播时，只需根据输出就能够推算反向传播的梯度。但是只有少数的 autograd 操作支持 inplace 操作（如 <code>tensor.sigmoid_()</code>)，除非你明确地知道自己在做什么，否则一般不要使用 inplace 操作。</p><p>5 . 在以上例子中，基本上都是将每一层的输出直接作为下一层的输入，这种网络称为前馈传播网络，对于此类网络如果每次都写复杂的 forward 函数会有些麻烦，有两种简化方式，<code>ModuleList</code> 和 <code>Sequential</code>，其中 <code>Sequential</code> 是一个特殊的 Module，它包含几个子 Module，前向传播时会将输入一层接一层地传递下去，<code>ModuleList</code> 也是一个极其特殊的 Module，可以包含几个子 Module，可以像 list 一样使用它，但不能直接把输入传递给 ModuleList。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sequential 的三种写法</span></span><br><span class="line">net1 = nn.Sequential()</span><br><span class="line">net1.add_module(<span class="string">'conv'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">'batchnorm'</span>, nn.BatchNorm2d(<span class="number">3</span>))</span><br><span class="line">net1.add_module(<span class="string">'activation_layer'</span>, nn.ReLU())</span><br><span class="line"></span><br><span class="line">net2 = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">    nn.BatchNorm2d(<span class="number">3</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">net3 = nn.Sequential(OrderedDict[</span><br><span class="line">                         (<span class="string">'conv1'</span>, nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)),</span><br><span class="line">                         (<span class="string">'bn1'</span>, nn.BatchNorm2d(<span class="number">3</span>)),</span><br><span class="line">                         (<span class="string">'relu1'</span>, nn.ReLU())</span><br><span class="line">                     ])</span><br><span class="line"></span><br><span class="line">print(<span class="string">'net1:'</span>, net1)</span><br><span class="line">print(<span class="string">'net2:'</span>, net2)</span><br><span class="line">print(<span class="string">'net3:'</span>, net3)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可根据名字或序号取出子 Module</span></span><br><span class="line">print(net1.conv, net2[<span class="number">0</span>], net3.conv1)</span><br><span class="line"></span><br><span class="line">input = t.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">output = net1(input)</span><br><span class="line">output = net2(input)</span><br><span class="line">output = net3(input)</span><br><span class="line">output = net3.relu1(net1.batchnorm(net1.conv(input)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">modellist = nn.ModuleList([nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU(), nn.Linear(<span class="number">4</span>, <span class="number">2</span>)])</span><br><span class="line">input = t.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">for</span> model <span class="keyword">in</span> modellist:</span><br><span class="line">    input = model(input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面会报错，因为 modellist 没有实现 forward 方法</span></span><br><span class="line"><span class="comment"># output = modellist(input)</span></span><br></pre></td></tr></table></figure><p><code>ModuleList</code> 是 <code>Module</code> 的子类，当在 <code>Module</code> 中使用它的时候，就能自动识别为子 module，而 python 自带的 list 则不行。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyModule, self).__init__()</span><br><span class="line">        self.list = [nn.Linear(<span class="number">3</span>, <span class="number">4</span>), nn.ReLU()]</span><br><span class="line">        self.module_list = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>), nn.ReLU()])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModule()</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    print(name, param)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxec1gj8swj20u00xt0wx.jpg" alt="">  </p><p>可见，list 中的子 Module 并不能被主 Module 所识别，而 ModuleList 中的子 Module  能够被主 Module 所识别，这意味着如果用 list 保存子 Module，将无法调整其参数，因其未加入到主 Module 的参数中。  </p><p>在实际应用中，如果在构造函数 <code>__init__</code> 中用到 list、tuple、dict 等对象时，一定要思考是否应该用 ModuleList 或 ParameterList 代替。</p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 图像的卷积操作。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; PIL &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; Image&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torchvision.transforms &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; ToTensor, ToPILImage&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_tensor = ToTensor()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_pil = ToPILImage()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;lena = Image.open(&lt;span class=&quot;string&quot;&gt;&#39;path to your image&#39;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 输入是一个 batch，batch_size=1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = to_tensor(lena).unsqueeze(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 锐化卷积核&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kernel = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)/&lt;span class=&quot;number&quot;&gt;-9&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;kernel[&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;] = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conv = nn.Conv2d(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, (&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;), &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, bias=&lt;span class=&quot;keyword&quot;&gt;False&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;conv.weight.data = kernel.view(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;out = conv(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;to_pil(out.data.squeeze(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十二）</title>
    <link href="http://blog.keeplearning.group/2018/11/19/2018/11-19-pytorch12/"/>
    <id>http://blog.keeplearning.group/2018/11/19/2018/11-19-pytorch12/</id>
    <published>2018-11-19T06:08:00.000Z</published>
    <updated>2018-11-19T11:19:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>torch.nn</code> 是专门为深度学习而设计的模块，它的核心数据结构是 <code>Module</code>，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 <code>nn.Module</code>，撰写自己的网络/层。 下面自定义一个全连接层。 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">layer = Linear(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">input = t.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">output = layer(input)</span><br><span class="line">print(output)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> layer.named_parameters():</span><br><span class="line">    print(name, parameter)</span><br></pre></td></tr></table></figure><a id="more"></a> <p>有几点需要注意：  </p><ul><li>自定义层必须继承 <code>nn.Module</code>，并且在其构造函数中需调用 <code>nn.Module</code> 的构造函数；</li><li>在构造函数 <code>__init__</code> 中必须自己定义可学习的参数，并封装成 <code>nn.Parameter</code>，<code>Parameter</code> 是一种特殊的 Tensor，但其默认需要求导（requires_grad=True);</li><li><code>forward</code> 函数实现前向传播过程，其输入可以是一个或多个 tensor；</li><li>无需写反向传播函数，<code>nn.Module</code> 能够利用 autograd 自动实现反向传播；</li><li><code>Module</code> 中的可学习参数可以通过 <code>named_parameters()</code> 或者 <code>parameters()</code> 返回迭代器，前者会给每个 parameter 都附上名字，使其更具有辨识度。</li></ul><p>2 . <code>Module</code> 能够自动检测到自己的 <code>Parameter</code>，并将其作为学习参数，除了 <code>Parameter</code> 之外，<code>Module</code> 还包含子 <code>Module</code>，主 <code>Module</code> 能够递归查找子 <code>Module</code> 中的 <code>Parameter</code>，下面实现一个多层感知机。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Linear</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features)</span>:</span></span><br><span class="line">        super(Linear, self).__init__()  <span class="comment"># 等价于 nn.Module.__init__(self)</span></span><br><span class="line">        self.w = nn.Parameter(t.randn(in_features, out_features))</span><br><span class="line">        self.b = nn.Parameter(t.randn(out_features))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.mm(self.w)</span><br><span class="line">        <span class="keyword">return</span> x + self.b.expand_as(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, hidden_features, out_features)</span>:</span></span><br><span class="line">        super(Perceptron, self).__init__()</span><br><span class="line">        self.layer1 = Linear(in_features, hidden_features)</span><br><span class="line">        self.layer2 = Linear(hidden_features, out_features)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = t.sigmoid(x)</span><br><span class="line">        <span class="keyword">return</span> self.layer2(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perceptron = Perceptron(<span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> name, parameter <span class="keyword">in</span> perceptron.named_parameters():</span><br><span class="line">    print(name, parameter.size())</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxdg714iv6j20s704c3yx.jpg" alt=""></p><p><code>Module</code> 中 parameter 的命名规范：</p><ul><li>对于类似 <code>self.param_name = nn.Parameter(t.randn(3, 4))</code>，命名为 <code>param_name</code>;</li><li>对于子 <code>Module</code> 中的 <code>parameter</code>，会其名字之前加上当前 <code>Module</code> 的名字。如对于 <code>self.sub_module = SubModel()</code>，<code>SubModel</code>中有个 <code>parameter</code> 的名字叫做 <code>param_name</code>，那么二者拼接而成的 <code>parameter name</code> 就是 <code>sub_module.param_name</code>。  </li></ul><p>3 . 为方便用户使用，PyTorch 实现了神经网络中绝大多数的 layer，这些 layer 都继承于 <code>nn.Module</code>，封装了可学习参数 <code>Parameter</code>，并实现了 <code>forward</code> 函数。这些自定义的 layer 对输入形状都有假设：输入的不是单个数据，而是一个 batch，输入只有一个数据，则必须调用 <code>tensor.unsqueeze(0)</code> 或 <code>tensor[None]</code> 将数据伪装成 batch_size=1 的 batch。  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;torch.nn&lt;/code&gt; 是专门为深度学习而设计的模块，它的核心数据结构是 &lt;code&gt;Module&lt;/code&gt;，这是一个抽象的概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。在实际使用中，常见的做法是继承 &lt;code&gt;nn.Module&lt;/code&gt;，撰写自己的网络/层。 下面自定义一个全连接层。 &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;from&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; nn&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(nn.Module)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, in_features, out_features)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        super(Linear, self).__init__()  &lt;span class=&quot;comment&quot;&gt;# 等价于 nn.Module.__init__(self)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.w = nn.Parameter(t.randn(in_features, out_features))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        self.b = nn.Parameter(t.randn(out_features))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self, x)&lt;/span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        x = x.mm(self.w)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; x + self.b.expand_as(x)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;layer = Linear(&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;input = t.randn(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;output = layer(input)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(output)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; name, parameter &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; layer.named_parameters():&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    print(name, parameter)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十一）</title>
    <link href="http://blog.keeplearning.group/2018/11/18/2018/11-18-pytorch11/"/>
    <id>http://blog.keeplearning.group/2018/11/18/2018/11-18-pytorch11/</id>
    <published>2018-11-18T04:28:00.000Z</published>
    <updated>2018-11-18T13:39:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  </p><p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">print(a.data)  <span class="comment"># 还是同一个 tensor</span></span><br><span class="line"></span><br><span class="line">print(a.data.requires_grad)  <span class="comment"># 但是已经独立于计算图之外了</span></span><br><span class="line"></span><br><span class="line">d = a.data.sigmoid_()  <span class="comment"># sigmoid_ 是一个 inplace 操作，会修改 a 自身的值</span></span><br><span class="line">print(a)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">print(a.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错</span></span><br><span class="line">tensor = a.detach()</span><br><span class="line">print(tensor.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计 tensor 的一些指标，不希望被记录</span></span><br><span class="line">mean = tensor.mean()</span><br><span class="line">std = tensor.std()</span><br><span class="line">maximum = tensor.max()</span><br><span class="line">print(mean, std, maximum)</span><br><span class="line"></span><br><span class="line">tensor[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># 下面会报错： RuntimeError: one of the variables needed for gradient</span></span><br><span class="line"><span class="comment">#              computation has been modified by an inplace operation.</span></span><br><span class="line"><span class="comment"># 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确</span></span><br><span class="line"><span class="comment"># c.sum().backward()</span></span><br></pre></td></tr></table></figure></p><a id="more"></a><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxc4jy2emjj20wu0edmy2.jpg" alt=""></p><p>2 . 在反向传播过程中非叶子节点的导数计算完之后即被清空，若想查看这些变量的梯度，有两种方法：  </p><ul><li>使用 <code>autograd.grad</code> 函数；</li><li>使用 <code>hook</code>。</li></ul><p>推荐使用 <code>hook</code> 方法，但是在实际应用中应尽量避免修改 grad 的值。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一种方法：使用 grad 获取中间变量的梯度</span></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line">z = y.sum()</span><br><span class="line"><span class="comment"># z 对 y 的梯度，隐式调用 backward()</span></span><br><span class="line">print(t.autograd.grad(z, y))  <span class="comment"># (tensor([1., 1., 1.]),)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二种方法：使用 hook</span></span><br><span class="line"><span class="comment"># hook 是一个函数，输入是梯度，无返回值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">variable_hook</span><span class="params">(grad)</span>:</span></span><br><span class="line">    print(<span class="string">'y 的梯度：'</span>, grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x</span><br><span class="line"><span class="comment"># 注册 hook</span></span><br><span class="line">hook_handle = y.register_hook(variable_hook)</span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除非你每次都要用 hook，否则用完之后记得移除 hook</span></span><br><span class="line">hook_handle.remove()</span><br></pre></td></tr></table></figure><p>3 . 看看 variable 中的 grad 属性和 backward 函数 grad_variables 参数的含义。  </p><ul><li>variable x 的梯度是目标函数 $f(x)$ 对 x 的梯度，$\frac{df(x)}{dx}=(\frac{df(x)}{dx_{0}},\frac{df(x)}{dx_{1}},…,\frac{df(x)}{dx_N})$，形状和 x 一致；</li><li>对于 <code>y.backward(grad_variables)</code> 中的 <code>grad_variables</code> 相当于链式求导法则 $\frac{\partial{z}}{\partial{x}}=\frac{\partial{z}}{\partial{y}}\cdot \frac{\partial{y}}{\partial{x}}$ 中的 $\frac{\partial{z}}{\partial{y}}$，z 是目标函数，一般是一个标量，故而 $\frac{\partial{z}}{\partial{y}}$ 的形状与 variable y 的形状一致，<code>z.backward()</code> 在一定程度上等价于 <code>y.backward(grad_y)</code>。<code>z.backward()</code> 省略了 <code>grad_variables</code> 参数，因为 z 是一个标量，而 $\frac{\partial{z}}{\partial{z}}=1$。  </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">z.backward()  <span class="comment"># 从 z 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">3</span>).float()</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = x ** <span class="number">2</span> + x * <span class="number">2</span></span><br><span class="line">z = y.sum()</span><br><span class="line">y_gradient = t.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># dz/dy</span></span><br><span class="line">y.backward(y_gradient)  <span class="comment"># 从 y 开始反向传播</span></span><br><span class="line">print(x.grad)  <span class="comment"># tensor([2., 4., 6.])</span></span><br></pre></td></tr></table></figure><p>另外需要注意，只有对 variable 的操作才能使用 autograd，如果对 variable 的 data 直接进行操作，将无法使用反向传播，除了对参数初始化，一般我们不会修改 variable.data 的值。  </p><p><strong>总结</strong>  </p><p>PyTorch 中计算图的特点可总结如下：  </p><ul><li><code>autograd</code> 根据用户对 variable 的操作构建计算图，对变量的操作抽象为 <code>Function</code>；</li><li>对于那些不是任何函数的输出，由用户创建的节点称为叶子节点，叶子节点的 <code>grad_fn</code> 为 None，叶子节点中需要求导的 variable，具有 <code>AccumulateGrad</code> 标识，因其梯度是累加的；</li><li>variable 默认是不需要求导的，即 <code>requires_grad</code> 属性默认为 False，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都为 <code>True</code>；</li><li>variable 的 <code>volatitle</code> 属性默认为 <code>False</code>，如果某一个 variable 的 <code>volatitle</code> 属性被设置为 <code>True</code>，那么所有依赖它的节点的 <code>volatitle</code> 属性都为 <code>True</code>，<code>volatitle</code> 为 <code>True</code> 的节点不会求导，<code>volatitle</code> 的优先级比 <code>requires_grad</code> 高；</li><li>多次反向传播时，梯度是累加的，反向传播的中间缓存会被清空，为进行多次反向传播需指定 <code>retian_graph=True</code> 来保存这些缓存；</li><li>非叶子节点的梯度计算完之后即被清空，可以使用 <code>autograd.grad</code> 或 <code>hook</code> 技术获取非叶子节点值；</li><li>variable 的 grad 与 data 形状一致，应避免直接修改 variable.data，因为对 data 的直接操作无法利用 autograd 进行反向传播；</li><li>反向传播函数 <code>backward</code> 的参数 <code>grad_variables</code> 可以看成链式求导的中间结果，如果是标量，可以省略，默认为 1；</li><li>PyTorch 采用动态图设计，可以很方便地查看中间层的输出，动态地设计计算图结构。   </li></ul><p>4 . 目前绝大多数函数都可以使用 <code>autograd</code> 实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办？那就需要自己写一个 <code>Function</code>，实现它的前向传播和反向传播代码。  </p><p>此外实现了自己的 <code>Function</code> 之后，还可以使用 <code>gradcheck</code> 函数来检测实现是否正确，<code>gradcheck</code> 通过数值逼近来计算梯度，可能具有一定的误差，通过控制 <code>eps</code> 的大小可以控制容忍的误差。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiplyAdd</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, w, x, b)</span>:</span></span><br><span class="line">        ctx.save_for_backward(w, x)</span><br><span class="line">        output = w * x + b</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        w, x = ctx.saved_tensors</span><br><span class="line">        grad_w = grad_output * x</span><br><span class="line">        grad_x = grad_output * w</span><br><span class="line">        grad_b = grad_output * <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> grad_w, grad_x, grad_b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始前向传播</span></span><br><span class="line">z = MultiplyAdd.apply(w, x, b)</span><br><span class="line"><span class="comment"># 开始反向传播</span></span><br><span class="line">z.backward()</span><br><span class="line"><span class="comment"># x 不需要求导，中间过程还是会计算它的导数，但随后被清空</span></span><br><span class="line">print(x.grad, w.grad, b.grad)  <span class="comment"># (None, tensor([1.]), tensor([1.]))</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Function</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sigmoid</span><span class="params">(Function)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(ctx, x, )</span>:</span></span><br><span class="line">        output = <span class="number">1</span> / (<span class="number">1</span> + t.exp(-x))</span><br><span class="line">        ctx.save_for_backward(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(ctx, grad_output)</span>:</span></span><br><span class="line">        output, = ctx.saved_tensors</span><br><span class="line">        grad_x = output * (<span class="number">1</span> - output) * grad_output</span><br><span class="line">        <span class="keyword">return</span> grad_x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用数值逼近方式检验计算梯度的公式对不对</span></span><br><span class="line">test_input = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">test_input.requires_grad_()</span><br><span class="line">t.autograd.gradcheck(Sigmoid.apply, (test_input,), eps=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  &lt;/p&gt;
&lt;p&gt;Input:&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;32&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = t.ones(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c = a * b&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.data)  &lt;span class=&quot;comment&quot;&gt;# 还是同一个 tensor&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.data.requires_grad)  &lt;span class=&quot;comment&quot;&gt;# 但是已经独立于计算图之外了&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d = a.data.sigmoid_()  &lt;span class=&quot;comment&quot;&gt;# sigmoid_ 是一个 inplace 操作，会修改 a 自身的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(d.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tensor = a.detach()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(tensor.requires_grad)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 统计 tensor 的一些指标，不希望被记录&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;mean = tensor.mean()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;std = tensor.std()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;maximum = tensor.max()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(mean, std, maximum)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;tensor[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;] = &lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 下面会报错： RuntimeError: one of the variables needed for gradient&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;#              computation has been modified by an inplace operation.&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# c.sum().backward()&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（十）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch10/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch10/</id>
    <published>2018-11-17T11:13:00.000Z</published>
    <updated>2018-11-17T14:25:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . <code>torch.autograd</code> 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在创建 tensor 的时候指定 requires_grad</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">b = t.zeros(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">c = a.add(b)   <span class="comment"># 也可以写成 c = a + b</span></span><br><span class="line"></span><br><span class="line">d = c.sum()</span><br><span class="line">d.backward()   <span class="comment"># 反向传播</span></span><br><span class="line">print(d.requires_grad)   <span class="comment"># d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，</span></span><br><span class="line"><span class="comment"># 因此 c 的 requires_grad 属性会自动设置为 True</span></span><br><span class="line">print(a.requires_grad, b.requires_grad, c.requires_grad)   <span class="comment"># (True, True, True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否为叶子节点</span></span><br><span class="line">print(a.is_leaf, b.is_leaf, c.is_leaf)   <span class="comment"># (True, True, False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，</span></span><br><span class="line"><span class="comment"># 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了</span></span><br><span class="line">print(c.grad <span class="keyword">is</span> <span class="keyword">None</span>)    <span class="comment"># True</span></span><br></pre></td></tr></table></figure><p>2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  </p><p>$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  </p><a id="more"></a><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    y = x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">'''手动求导函数'''</span></span><br><span class="line">    dx = <span class="number">2</span> * x * t.exp(x) + x ** <span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line"></span><br><span class="line">y.backward(t.ones(y.size()))</span><br><span class="line">print(x.grad)</span><br><span class="line">print(gradf(x))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxbcx53kogj20yc078gmn.jpg" alt="">  </p><p>3 . 每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个 variable 的梯度，这些函数的函数名通常以 Backward 结尾。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>)</span><br><span class="line">b = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = w * x  <span class="comment"># 等价于 y = w.mul(x)</span></span><br><span class="line">z = y + b  <span class="comment"># 等价于 z = y.add(b)</span></span><br><span class="line"></span><br><span class="line">print(x.requires_grad, b.requires_grad, w.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># grad_fn 可以查看这个 variable 的反向传播函数，</span></span><br><span class="line"><span class="comment"># z 是 add 函数的输出，所以它的反向传播函数是 AddBackward</span></span><br><span class="line">print(z.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># next_functions 保存 grad_fn 的输入，是一个 tuple，tuple 的元素也是 Function</span></span><br><span class="line"><span class="comment"># 第一个是 y，它是乘法（mul）的输出，所以对应的反向传播函数 y.grad_fn 是 MulBackward</span></span><br><span class="line"><span class="comment"># 第二个是 b，它是叶子节点，由用户创建，grad_fn 为 None，但是需要求导，其梯度是累加的</span></span><br><span class="line">print(z.grad_fn.next_functions)</span><br><span class="line">print(z.grad_fn.next_functions[<span class="number">0</span>][<span class="number">0</span>] == y.grad_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一个是 w，叶子节点，需要求导，梯度是累加的</span></span><br><span class="line"><span class="comment"># 第二个是 x，叶子节点，不需要求导，所以为 None</span></span><br><span class="line">print(y.grad_fn.next_functions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 叶子节点的 grad_fn 是 None</span></span><br><span class="line">print(w.grad_fn, x.grad_fn)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxbdwxg9bwj214406owfg.jpg" alt="">  </p><p>计算 $w$ 的梯度的时候，需要用到 $x$ 的数值（$\frac{\partial y}{\partial w}=x$），这些数值在前向过程中会保存成 buffer，在计算完梯度之后会自动清空，为了能够多次反向传播需要指定 <code>retain_graph</code> 来保留这些 buffer。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">z.backward(retain_graph=<span class="keyword">True</span>)</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多次反向传播，梯度会累加，这也就是 w 中 AccumulateGrad 标识的含义</span></span><br><span class="line">z.backward()</span><br><span class="line">print(w.grad)   <span class="comment"># tensor([2.])</span></span><br></pre></td></tr></table></figure><p>4 . PyTorch 使用的是动态图，它的计算图在每次前向传播时都是从头开始构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abs</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> x.data[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([1.])</span></span><br><span class="line"></span><br><span class="line">x = <span class="number">-1</span> * t.ones(<span class="number">1</span>)  <span class="comment"># 写成 x = -1 * t.ones(1, requires_grad=True) 时，x 不计算梯度</span></span><br><span class="line">x = x.requires_grad_()</span><br><span class="line">y = abs(x)</span><br><span class="line">y.backward()</span><br><span class="line">print(x.grad)  <span class="comment"># tensor([-1.])</span></span><br></pre></td></tr></table></figure><p>变量的 <code>requires_grad</code> 属性默认 <code>False</code>，如果某一个节点 <code>requires_grad</code> 被设置为 <code>True</code>，那么所有依赖它的节点 <code>requires_grad</code> 都是 <code>True</code>。  </p><p>5 . 有时候可能不希望 autograd 对 tensor 求导，因为求导需要缓存许多中间结构，增加额外的内存/显存开销，同时降低运行速度，那么我们可以关闭自动求导，譬如在模型训练完毕转而进行测试推断的时候。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> t.no_grad():  <span class="comment"># 也可以使用 t.set_grad_enable(False) 设置（无需 with），并且以下代码无缩进</span></span><br><span class="line">    x = t.ones(<span class="number">1</span>)</span><br><span class="line">    w = t.rand(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">    y = x * w</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y 虽然依赖于 w 和 x，虽然 w.requires_grad=True，但是 y.requires_grad=False</span></span><br><span class="line">    print(x.requires_grad, w.requires_grad, y.requires_grad)  <span class="comment"># (False, True, False)</span></span><br></pre></td></tr></table></figure><p>关闭自动求导后可以使用 <code>t.set_grad_enable(True)</code> 恢复设置。</p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . &lt;code&gt;torch.autograd&lt;/code&gt; 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 在创建 tensor 的时候指定 requires_grad&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 或者&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;).requires_grad_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 或者&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a.requires_grad=&lt;span class=&quot;keyword&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;b = t.zeros(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;).requires_grad_()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c = a.add(b)   &lt;span class=&quot;comment&quot;&gt;# 也可以写成 c = a + b&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d = c.sum()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d.backward()   &lt;span class=&quot;comment&quot;&gt;# 反向传播&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(d.requires_grad)   &lt;span class=&quot;comment&quot;&gt;# d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 因此 c 的 requires_grad 属性会自动设置为 True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.requires_grad, b.requires_grad, c.requires_grad)   &lt;span class=&quot;comment&quot;&gt;# (True, True, True)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 判断是否为叶子节点&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.is_leaf, b.is_leaf, c.is_leaf)   &lt;span class=&quot;comment&quot;&gt;# (True, True, False)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(c.grad &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;None&lt;/span&gt;)    &lt;span class=&quot;comment&quot;&gt;# True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  &lt;/p&gt;
&lt;p&gt;$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  &lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（九）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch9/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch9/</id>
    <published>2018-11-17T09:03:00.000Z</published>
    <updated>2018-11-17T09:14:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>本节实现一个训练线性回归参数的例子，线性回归的损失函数为：$loss=\frac{1}{2} \sum_{i=1}^{N}(y_{i}-(wx_{i}+b))^2$，然后利用随机梯度下降法更新参数 $w$ 和 $b$ 来最小化损失函数，最终学得 $w$ 和 $b$ 的值。<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">device = t.device(<span class="string">'cuda:0'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span></span><br><span class="line">t.manual_seed(<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_fake_data</span><span class="params">(batch_size=<span class="number">8</span>)</span>:</span></span><br><span class="line">    x = t.rand(batch_size, <span class="number">1</span>, device=device) * <span class="number">5</span></span><br><span class="line">    y = x * <span class="number">2</span> + <span class="number">3</span> + t.randn(batch_size, <span class="number">1</span>, device=device)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机初始化参数</span></span><br><span class="line">w = t.rand(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line">b = t.zeros(<span class="number">1</span>, <span class="number">1</span>).to(device)</span><br><span class="line"></span><br><span class="line">lr = <span class="number">0.02</span>  <span class="comment"># 设置学习率</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ii <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    x, y = get_fake_data(batch_size=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward: 计算loss</span></span><br><span class="line">    y_pred = x.mm(w) + b.expand_as(y)</span><br><span class="line">    loss = <span class="number">0.5</span> * (y_pred - y) ** <span class="number">2</span></span><br><span class="line">    loss = loss.mean()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># backward: 手动计算梯度</span></span><br><span class="line">    dloss = <span class="number">1</span></span><br><span class="line">    dy_pred = dloss * (y_pred - y)</span><br><span class="line"></span><br><span class="line">    dw = x.t().mm(dy_pred)</span><br><span class="line">    db = dy_pred.sum()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    w.sub_(lr * dw)</span><br><span class="line">    b.sub_(lr * db)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ii % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 画图</span></span><br><span class="line">        x = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        x = x.float()</span><br><span class="line">        y = x.mm(w.cpu()) + b.cpu().expand_as(x)</span><br><span class="line">        plt.plot(x.cpu().numpy(), y.cpu().numpy())  <span class="comment"># predicted</span></span><br><span class="line"></span><br><span class="line">        x2, y2 = get_fake_data(batch_size=<span class="number">32</span>)</span><br><span class="line">        plt.scatter(x2.cpu().numpy(), y2.cpu().numpy())  <span class="comment"># true data</span></span><br><span class="line"></span><br><span class="line">        plt.xlim(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line">        plt.ylim(<span class="number">0</span>, <span class="number">13</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">        plt.pause(<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'w: '</span>, w.item(), <span class="string">'b: '</span>, b.item())</span><br></pre></td></tr></table></figure></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本节实现一个训练线性回归参数的例子，线性回归的损失函数为：$loss=\frac{1}{2} \sum_{i=1}^{N}(y_{i}-(wx_{i}+b))^2$，然后利用随机梯度下降法更新参数 $w$ 和 $b$ 来最小化损失函数，最终学得 $w$ 和 $b$ 的值。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（八）</title>
    <link href="http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch8/"/>
    <id>http://blog.keeplearning.group/2018/11/17/2018/11-17-pytorch8/</id>
    <published>2018-11-17T01:49:00.000Z</published>
    <updated>2018-11-17T04:27:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。<br><a id="more"></a><br>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.storage())</span><br><span class="line"></span><br><span class="line">b = a.view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个对象的 id 值可以看作它在内存中的地址</span></span><br><span class="line"><span class="comment"># storage 的内存地址一样，即是同一个 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># a 改变，b 也随之改变，因为它们共享 storage</span></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">c = a[<span class="number">2</span>:]</span><br><span class="line">print(c.storage())</span><br><span class="line"></span><br><span class="line"><span class="comment"># data_ptr 返回 tensor 首元素的内存地址</span></span><br><span class="line"><span class="comment"># 可以看出相差 16，这是因为 2*8=16</span></span><br><span class="line"><span class="comment"># 相差两个元素，每个元素占 8 个字节（long）</span></span><br><span class="line">print(c.data_ptr(), a.data_ptr())</span><br><span class="line"></span><br><span class="line">c[<span class="number">0</span>] = <span class="number">-100</span>  <span class="comment"># c[0] 的内存地址对应 a[2]</span></span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">d = t.Tensor(c.storage().float())</span><br><span class="line">print(id(c.storage()) == id(d.storage()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面 3 个 tensor 共享 storage</span></span><br><span class="line">print(id(a.storage()) == id(b.storage()) == id(c.storage))</span><br><span class="line"></span><br><span class="line">print(a.storage_offset(), c.storage_offset())</span><br><span class="line"></span><br><span class="line">e = b[::<span class="number">2</span>, ::<span class="number">2</span>]  <span class="comment"># 隔 2 行/列取一个元素</span></span><br><span class="line">print(id(e.storage()) == id(a.storage()))</span><br><span class="line"></span><br><span class="line">print(b.stride(), e.stride())</span><br><span class="line"></span><br><span class="line">print(e.is_contiguous())</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fxauj7ug2xj20wu0u0acf.jpg" alt="">  </p><p>可见绝大多数操作并不修改 tensor 的数据，而只是修改了 tensor 的头信息。这种做法更节省内存，同时提升了处理速度。此外有些操作会导致 tensor 不连续，这时需要调用 <code>tensor.contiguous</code> 方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享 storage。  </p><p>2 . tensor 可以随意地在 GPU/CPU 上传输，使用 <code>tensor.cuda(device_id)</code> 或者 <code>tensor.cpu()</code>，另外一个更通用的方法是 <code>tensor.to(device)</code>。  </p><ul><li>尽量使用 <code>tensor.to(device)</code>，将 <code>device</code> 设为一个可配置的参数，这样可以很轻松地使程序同时兼容 GPU 和 CPU；</li><li>数据在 GPU 之中传输的速度要远快于内存（CPU）到显存（GPU），所以尽量避免在内存和显存之间传输数据。</li></ul><p>3 . tensor 的保存和加载十分简单，使用 <code>torch.save</code> 和 <code>torch.load</code> 即可完成相应的功能。在 save/load 时可以指定使用的 pickle 模块，在 load 时还可以将 GPU tensor 映射到 CPU 或者其他 GPU 上。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> t.cuda.is_available():</span><br><span class="line">    a = a.cuda(<span class="number">1</span>)   <span class="comment"># 把 a 转为 GPU1 上的 tensor</span></span><br><span class="line">    t.save(a, <span class="string">'a.pth'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 加载 b，存储于 GPU1 上（因为保存时 tensor 就在 GPU1 上）</span></span><br><span class="line">    b = t.load(<span class="string">'a.pth'</span>)</span><br><span class="line">    <span class="comment"># 加载为 c，存储于 CPU 上</span></span><br><span class="line">    c = t.load(<span class="string">'a.pth'</span>, map_location=<span class="keyword">lambda</span> storage, loc: storage)</span><br><span class="line">    <span class="comment"># 加载为 d，存储于 GPU0 上</span></span><br><span class="line">    d = t.load(<span class="string">'a.pth'</span>, map_location=&#123;<span class="string">'cuda:1'</span>:<span class="string">'cuda:0'</span>&#125;)</span><br></pre></td></tr></table></figure><p>4 . 关于 tensor 还有几点需要注意：  </p><ul><li>大多数 <code>torch.function</code> 都有一个参数 <code>out</code>，这时候产生的结果将保存在 <code>out</code> 指定的 tensor 之中；</li><li><code>torch.set_num_threads</code> 可以设置 PyTorch 进行 CPU 多线程并行计算时候所占用的线程数，这个可以用来限制 PyTorch 所占用的 CPU 数目；</li><li><code>torch.set_printoptions</code> 可以用来设置打印 tensor 时的数值精度和格式。</li></ul><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">t.set_printoptions(precision=<span class="number">10</span>)</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx2.sinaimg.cn/mw690/79225320gy1fxay2fzkx2j20sq04uaan.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>理解 Momentum</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-momentum/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-momentum/</id>
    <published>2018-11-16T13:39:00.000Z</published>
    <updated>2018-11-17T01:29:47.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>随机梯度下降（SGD）方法的一个缺点是其更新方向完全依赖于当前 batch 计算出的梯度，因而十分不稳定，Momentum 算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向，这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优的能力。数学表达式如下：  </p><p>$$v_{t}=\gamma v_{t-1}+\alpha \cdot \nabla_{\theta }J(\theta)$$  $$\theta=\theta -v_{t}$$  </p><p>Momentum 算法会观察历史梯度 $v_{t-1}$，若当前梯度的方向与历史梯度一致，表明当前样本不太可能为异常点，则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。<br><a id="more"></a></p><h2 id="Nesterov-Momentum"><a href="#Nesterov-Momentum" class="headerlink" title="Nesterov Momentum"></a>Nesterov Momentum</h2><p>在小球向下滚的过程中，我们希望小球能够提前知道在哪些方向坡面会上升，这样在遇到上升坡面之前，小球就开始减速，这方法就是 Nesterov Momentum，其在凸优化中有较强的理论保证收敛，并且，在实践中 Nesterov Momentum 也要比单纯的 Momentum 的效果好。数学表达式如下：  </p><p>$$v_{t}=\gamma v_{t-1} + \alpha \cdot \nabla_{\theta}J(\theta-\gamma v_{t-1})$$  $$\theta=\theta-v_{t}$$  </p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Momentum&quot;&gt;&lt;a href=&quot;#Momentum&quot; class=&quot;headerlink&quot; title=&quot;Momentum&quot;&gt;&lt;/a&gt;Momentum&lt;/h2&gt;&lt;p&gt;随机梯度下降（SGD）方法的一个缺点是其更新方向完全依赖于当前 batch 计算出的梯度，因而十分不稳定，Momentum 算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向，这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优的能力。数学表达式如下：  &lt;/p&gt;
&lt;p&gt;$$v_{t}=\gamma v_{t-1}+\alpha \cdot \nabla_{\theta }J(\theta)$$  $$\theta=\theta -v_{t}$$  &lt;/p&gt;
&lt;p&gt;Momentum 算法会观察历史梯度 $v_{t-1}$，若当前梯度的方向与历史梯度一致，表明当前样本不太可能为异常点，则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。&lt;br&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Momentum" scheme="http://blog.keeplearning.group/tags/Momentum/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（七）</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch7/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch7/</id>
    <published>2018-11-16T08:01:00.000Z</published>
    <updated>2018-11-16T13:18:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">abs/sqrt/div/exp/fmod/log/pow…</td><td style="text-align:center">绝对值/平方根/除法/指数/求余/求幂…</td></tr><tr><td style="text-align:center">cos/sin/asin/atan2/cosh…</td><td style="text-align:center">相关三角函数</td></tr><tr><td style="text-align:center">ceil/round/floor/trunc</td><td style="text-align:center">上取整/四舍五入/下取整/只保留整数部分</td></tr><tr><td style="text-align:center">clamp(input, min, max)</td><td style="text-align:center">超过 min 和 max 部分截断</td></tr><tr><td style="text-align:center">sigmod/tanh..</td><td style="text-align:center">激活函数</td></tr></tbody></table><p>对于很多操作，例如 <code>div</code>、<code>mul</code>、<code>pow</code>、<code>fmod</code> 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 <code>a ** 2</code> 等价于 <code>torch.pow(a, 2)</code>，<code>a * 2</code> 等价于 <code>torch.mul(a, 2)</code>。<br><a id="more"></a><br>2 . 归并操作会使输出形状小于输入形状，并可以沿着某一维度进行执行操作，如加法 <code>sum</code>，既可以计算整个 tensor 的和，也可以计算 tensor 中每一行或每一列的和，常用的归并操作如下表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">mean/sum/median/mode</td><td style="text-align:center">均值/和/中位数/众数</td></tr><tr><td style="text-align:center">norm/dist</td><td style="text-align:center">范数/距离</td></tr><tr><td style="text-align:center">std/var</td><td style="text-align:center">标准差/方差</td></tr><tr><td style="text-align:center">cumsum/cumprod</td><td style="text-align:center">累加/累乘</td></tr></tbody></table><p>以上大多数函数都有一个参数 <code>dim</code>，用来指定这些操作是在哪个维度上执行的。  </p><p>假设输入的形状是 (m, n, k)：</p><ul><li>如果指定 <code>dim=0</code>，输出形状就是 (1, n ,k) 或者 (n, k)</li><li>如果指定 <code>dim=1</code>，输出形状就是 (m, 1, k) 或者 (m, k)</li><li>如果指定 <code>dim=2</code>，输出形状就是 (m, n, 1) 或者 (m, n)</li></ul><p>size 中是否有 “1”，取决于参数 <code>keepdim</code>,<code>keepdim=True</code> 会保留维度 1，注意，以上只是经验总结，并非所有的函数都符合这种形状变化方式。</p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">   </span><br><span class="line">b = t.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">True</span>))  <span class="comment"># 保留维度 1</span></span><br><span class="line">print(b.sum(dim=<span class="number">0</span>, keepdim=<span class="keyword">False</span>))  <span class="comment"># 不保留维度 1</span></span><br><span class="line"></span><br><span class="line">print(b.sum(dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(a.cumsum(dim=<span class="number">1</span>))  <span class="comment"># 沿着行累加</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fxa4du79t4j216408e3ze.jpg" alt=""></p><p>3 . 常用的比较函数表。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">gt/lt/ge/le/eq/ne</td><td style="text-align:center">大于/小于/大于等于/小于等于/等于/不等</td></tr><tr><td style="text-align:center">topk</td><td style="text-align:center">最大的 k 个数</td></tr><tr><td style="text-align:center">sort</td><td style="text-align:center">排序</td></tr><tr><td style="text-align:center">max/min</td><td style="text-align:center">比较两个 tensor 最大最小值</td></tr></tbody></table><p>表中第一行的比较操作已经实现了运算符重载，因此可以使用 <code>a&gt;=b</code>、<code>a&gt;b</code>、<code>a!=b</code>、<code>a==b</code>，其返回结果是一个 ByteTensor，可用来选取元素。max/min 这两个操作比较特殊，以 <code>max</code> 为例说明如下：</p><ul><li><code>torch.max(tensor)</code>：返回 tensor 中最大的那个数；</li><li><code>torch.max(tensor, dim)</code>：指定维度上最大的数，同时返回 tensor 和下标；</li><li><code>torch.max(tensor1, tensor2)</code>：返回两个 tensor 相比较大的元素。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.linspace(<span class="number">0</span>, <span class="number">15</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">b = t.linspace(<span class="number">15</span>, <span class="number">0</span>, <span class="number">6</span>).view(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">print(a &gt; b)</span><br><span class="line"></span><br><span class="line">print(a[a &gt; b])  <span class="comment"># a 中对应位置大于 b 的元素</span></span><br><span class="line"></span><br><span class="line">print(t.max(a))</span><br><span class="line"></span><br><span class="line">print(t.max(b, dim=<span class="number">1</span>))  <span class="comment"># 分别返回对应维度的最大值和最大值所在的下标</span></span><br><span class="line"></span><br><span class="line">print(t.max(a, b))</span><br><span class="line"></span><br><span class="line">print(t.clamp(a, min=<span class="number">10</span>))  <span class="comment"># 比较 a 和 10 较大的元素</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa5kkgcmbj215y0emq4l.jpg" alt=""></p><p>4 . 常用的线性代数函数表如下。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">trace</td><td style="text-align:center">矩阵的迹</td></tr><tr><td style="text-align:center">diag</td><td style="text-align:center">对角线元素</td></tr><tr><td style="text-align:center">triu/tril</td><td style="text-align:center">矩阵的上三角/下三角，可指定偏移量</td></tr><tr><td style="text-align:center">mm/bmm</td><td style="text-align:center">矩阵乘法，batch 的矩阵乘法</td></tr><tr><td style="text-align:center">addmm/addbmm/addmv/addr/badbmm…</td><td style="text-align:center">矩阵运算</td></tr><tr><td style="text-align:center">t</td><td style="text-align:center">转置</td></tr><tr><td style="text-align:center">dot/cross</td><td style="text-align:center">內积/外积</td></tr><tr><td style="text-align:center">inverse</td><td style="text-align:center">求逆矩阵</td></tr><tr><td style="text-align:center">svd</td><td style="text-align:center">奇异值分解</td></tr></tbody></table><p>需要注意的是，矩阵的转置会导致存储空间不连续，需要调用 <code>.contiguous</code> 方法将其转为连续。  </p><p>5 . Numpy 和 Tensor 可以相互转换，共享内存，但是当 Numpy 的数据类型和 Tensor 的数据类型不一样的时候，数据会被复制，不会共享内存。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">print(a.dtype)</span><br><span class="line"></span><br><span class="line">b = t.Tensor(a)   <span class="comment"># 此处进行拷贝，不共享内存</span></span><br><span class="line">print(b.dtype)</span><br><span class="line"></span><br><span class="line">c = t.from_numpy(a)  <span class="comment"># 注意 c 的类型（DoubleTensor）</span></span><br><span class="line">print(c.dtype)</span><br><span class="line"></span><br><span class="line">a[<span class="number">0</span>, <span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)   <span class="comment"># b 与 a 不共享内存，a 改变但 b 不变</span></span><br><span class="line">print(c)   <span class="comment"># c 与 a 共享内存</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320gy1fxa6jr9iznj20ue07ywf2.jpg" alt="">  </p><p>6 . 广播法则是科学计算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。  </p><p>Numpy 的广播法则定义如下：  </p><ul><li>让所有输入数组都向其中 shape 最长的数组看齐，shape 中不足的部分通过在前面加 1 补齐；</li><li>两个数组在某一维度的长度要么一致，要么其中一个为 1，否则不能计算；</li><li>当输入数组的某个维度的长度为 1 时，计算时沿此维度复制扩充成一样的形状。</li></ul><p>PyTorch 已经支持了自动广播法则，但是我们还是通过以下两个函数手动实现一下广播法则以加深理解吧。  </p><ul><li><code>unsqueeze</code> 或者 <code>view</code>，或者 <code>tensor[None]</code>，为数据某一维的形状补 1，实现法则 1；</li><li><code>expand</code> 或者 <code>expand_as</code>，重复数组，实现法则 3，该操作不会复制数组，所以不会占用额外的空间。</li></ul><p>注意：<code>repeat</code> 实现与 <code>expand</code> 相类似的功能，但是 <code>repeat</code> 会把相同数据复制多份，因此会占用额外的空间。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">b = t.zeros(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动广播法则</span></span><br><span class="line"><span class="comment"># 第一步： a 是 2 维，b 是 3 维，所以先在较小的 a 前面补 1，</span></span><br><span class="line"><span class="comment"># 即： a.unsqueeze(0)，a 的形状变为 (1, 3, 2)，b 的形状是 (2, 3, 1)</span></span><br><span class="line"><span class="comment"># 第二步： a 和 b 在第一维和第三维形状不一致，其中一个为 1，</span></span><br><span class="line"><span class="comment"># 可以利用广播法则，两者都扩展成 (2, 3, 2)</span></span><br><span class="line"></span><br><span class="line">print(a + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动广播法则</span></span><br><span class="line"><span class="comment"># 或者 a.view(1, 3, 2).expand(2, 3, 2) + b.expand(2, 3, 2)</span></span><br><span class="line">print(a[<span class="keyword">None</span>].expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>) + b.expand(<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fxa7an8bd0j20ug0fkgm5.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;函数&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;功能&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;abs/sqrt/div/exp/fmod/log/pow…&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;绝对值/平方根/除法/指数/求余/求幂…&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;cos/sin/asin/atan2/cosh…&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;相关三角函数&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;ceil/round/floor/trunc&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;上取整/四舍五入/下取整/只保留整数部分&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;clamp(input, min, max)&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;超过 min 和 max 部分截断&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;sigmod/tanh..&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;激活函数&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;对于很多操作，例如 &lt;code&gt;div&lt;/code&gt;、&lt;code&gt;mul&lt;/code&gt;、&lt;code&gt;pow&lt;/code&gt;、&lt;code&gt;fmod&lt;/code&gt; 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 &lt;code&gt;a ** 2&lt;/code&gt; 等价于 &lt;code&gt;torch.pow(a, 2)&lt;/code&gt;，&lt;code&gt;a * 2&lt;/code&gt; 等价于 &lt;code&gt;torch.mul(a, 2)&lt;/code&gt;。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（六）</title>
    <link href="http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch6/"/>
    <id>http://blog.keeplearning.group/2018/11/16/2018/11-16-pytorch6/</id>
    <published>2018-11-16T01:15:00.000Z</published>
    <updated>2018-11-16T07:15:13.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . Tensor 支持与 <code>numpy.ndarray</code> 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>])    <span class="comment"># 第 0 行，下标从 0 开始</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="number">0</span>])     <span class="comment"># 第 0 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>, <span class="number">2</span>])   <span class="comment"># 第 0 行第 2 个元素，等价于 a[0][2]</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>][<span class="number">-1</span>])   <span class="comment"># 第 0 行最后一个元素</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>])    <span class="comment"># 前两行</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>])   <span class="comment"># 前两行，第 0,1 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>:<span class="number">1</span>, :<span class="number">2</span>])  <span class="comment"># 第 0 行，前两列</span></span><br><span class="line">print(a[<span class="number">0</span>, :<span class="number">2</span>])   <span class="comment"># 注意两者的区别，形状不同</span></span><br></pre></td></tr></table></figure><a id="more"></a>  <p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320gy1fx9neqhzwaj214f0ddjt6.jpg" alt="">  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># None 类似于 np.newaxis，为 a 新增了一个轴</span></span><br><span class="line"><span class="comment"># 等价于 a.view(1, a.shape[0], a.shape[1])</span></span><br><span class="line">print(a[<span class="keyword">None</span>].shape)  <span class="comment"># 等价于 a[None,:,:]</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :].shape)</span><br><span class="line"></span><br><span class="line">print(a[:, <span class="keyword">None</span>, :, <span class="keyword">None</span>, <span class="keyword">None</span>].shape)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320ly1fx9nscpu79j214h03kq33.jpg" alt=""></p><p>2 . 常用的选择函数。  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>index_select(input, dim, index)</code></td><td style="text-align:center">在指定维度 dim 上选取，比如选取某些行、某些列</td></tr><tr><td style="text-align:center"><code>masked_select(input, mask)</code></td><td style="text-align:center">使用 ByteTensor 进行选取</td></tr><tr><td style="text-align:center"><code>non_zero(input)</code></td><td style="text-align:center">非 0 元素的下标</td></tr><tr><td style="text-align:center"><code>gather(input, dim, index)</code></td><td style="text-align:center">根据 index，在 dim 维度上选取数据，输出的 size 与 index 一样</td></tr></tbody></table><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a &gt; <span class="number">1</span>)  <span class="comment"># 返回一个 ByteTensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于 a.masked_select(a&gt;1)</span></span><br><span class="line">print(a[a &gt; <span class="number">1</span>])  <span class="comment"># 选择结果与原 tensor 不共享内存空间</span></span><br><span class="line"></span><br><span class="line">print(a[t.LongTensor([<span class="number">0</span>, <span class="number">1</span>])])  <span class="comment"># 第 0 行和第 1 行</span></span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">Output:  </span><br><span class="line">  </span><br><span class="line">![](http://wx3.sinaimg.cn/mw690/<span class="number">79225320</span>gy1fx9osqbau0j214807lgmc.jpg)  </span><br><span class="line">  </span><br><span class="line">`gather` 是一个比较复杂的操作，对于一个 <span class="number">2</span> 维的 tensor，输出的每个元素如下：</span><br></pre></td></tr></table></figure><p>out[i][j] = input[index[i][j]][j]   # dim=0<br>out[i][j] = input[i][index[i][j]]   # dim=1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">三维 tensor 的 `gather` 操作同理。  </span><br><span class="line">  </span><br><span class="line">`gather(input, dim, index)` 中的 dim 表示的就是第几维度，在二维的例子中，如果 dim=0，那么它表示的就是你接下来的操作是对第一维度进行的，也就是行；如果 dim=1，那么它表示的就是你接下来的操作是对第二个维度进行的，也就是列。index 的大小和 input 的大小是一样的，它表示的是你所选择的维度上的操作。特别注意，index 必须是 LongTensor 类型。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line">import torch as t</span><br><span class="line"></span><br><span class="line">a = t.arange(0, 16).view(4, 4)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line"># 选取对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3]])</span><br><span class="line">print(a.gather(0, index))</span><br><span class="line"></span><br><span class="line"># 选取反对角线上的元素</span><br><span class="line">index = t.LongTensor([[3, 2, 1, 0]]).t()</span><br><span class="line">print(a.gather(1, index))</span><br><span class="line"></span><br><span class="line"># 选取两个对角线上的元素</span><br><span class="line">index = t.LongTensor([[0, 1, 2, 3], [3, 2, 1, 0]]).t()</span><br><span class="line">b = a.gather(1, index)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure></p><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9purwv3xj214j0d8gmb.jpg" alt="">  </p><p>与 <code>gather</code> 相对应的逆操作是 <code>scatter_</code>，<code>gather</code> 把数据从 input 中按照 index 取出，而 <code>scatter_</code> 是把取出的数据再放回去（inplace 操作）。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把两个对角线的元素放回到指定位置</span></span><br><span class="line">c = t.zeros(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">b = b.float()  <span class="comment"># 将 b 转换成 FloatTensor</span></span><br><span class="line">c.scatter_(<span class="number">1</span>, index, b)</span><br><span class="line">print(c)</span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">对 tensor 的任何索引操作仍是一个 tensor，想要获取标准的 tensor 对象数值，需要调用 `tensor.item()`，这个方法只对包含一个元素的 tensor 适用。</span><br><span class="line">  </span><br><span class="line"><span class="number">3</span> . PyTorch 目前已支持绝大多数 numpy 的高级索引，高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的 Tensor 共享内存。  </span><br><span class="line">  </span><br><span class="line">Input:  </span><br><span class="line">  </span><br><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.arange(<span class="number">0</span>, <span class="number">27</span>).view(<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(x)</span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">0</span>]])  <span class="comment"># x[1, 1, 2] 和 x[2, 2, 0]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])  <span class="comment"># x[2, 0, 1]，x[1, 0, 1], x[0, 0, 1]</span></span><br><span class="line"></span><br><span class="line">print(x[[<span class="number">0</span>, <span class="number">2</span>], ...])   <span class="comment"># x[0] 和 x[2]</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx9wkft7f2j21570kf0tu.jpg" alt=""></p><p>4 . Tensor 有不同的数据类型，每种类型分别对应有 CPU 和GPU 版本（HalfTensor 除外），默认的 tensor 都是 FloatTensor，可通过 <code>torch.set_default_tensor_type</code> 来修改默认 tensor 类型，如果默认类型为 GPU tensor，则所有操作都在 GPU 上进行，HalftTensor 是专门为 GPU 版本设计的，同样的元素个数，显存占用只有 FloatTensor 的一半，所以可以极大缓解 GPU 显存不足的问题，但由于其数值大小和精度有限，所以可能出现溢出等问题。  </p><p>各数据类型之间可以相互转换，<code>type(new_type)</code> 是通用的做法，同时还有 <code>float</code>、<code>long</code>、<code>half</code> 等快捷方法，CPU tensor 和 GPU tensor 之间的相互转换通过 <code>tensor.cuda</code> 和 <code>tensor.cpu</code> 方法来实现，此外还可以使用 <code>tensor.to(device)</code>。  </p><p>Tensor 还有一个 <code>new</code> 方法，用法与 <code>t.Tensor</code> 一样，会调用该 tensor 对应类型的构造函数，生成与当前 tensor 类型一致的 tensor，<code>torch.*_like(tensor)</code> 可以生成和 tensor 拥有同样属性（类型、形状、CPU?GPU）的新 tensor。<code>tensor.new_*(new_shape)</code> 新建一个不同形状的 tensor。  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . Tensor 支持与 &lt;code&gt;numpy.ndarray&lt;/code&gt; 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  &lt;/p&gt;
&lt;p&gt;Input:  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;import&lt;/span&gt; torch &lt;span class=&quot;keyword&quot;&gt;as&lt;/span&gt; t&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;a = t.randn(&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])    &lt;span class=&quot;comment&quot;&gt;# 第 0 行，下标从 0 开始&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;])     &lt;span class=&quot;comment&quot;&gt;# 第 0 列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 第 0 行第 2 个元素，等价于 a[0][2]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;][&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 第 0 行最后一个元素&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])    &lt;span class=&quot;comment&quot;&gt;# 前两行&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 前两行，第 0,1 列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;:&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, :&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])  &lt;span class=&quot;comment&quot;&gt;# 第 0 行，前两列&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a[&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, :&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])   &lt;span class=&quot;comment&quot;&gt;# 注意两者的区别，形状不同&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（五）</title>
    <link href="http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch5/"/>
    <id>http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch5/</id>
    <published>2018-11-15T11:57:00.000Z</published>
    <updated>2018-11-15T13:44:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 通过 <code>tensor.view</code> 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另一个也会跟着改变。  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.view(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当某一维为 -1 时，会自动计算它的大小</span></span><br><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line">print(b.shape)</span><br></pre></td></tr></table></figure><a id="more"></a>  <p>2 . 在实际应用中可能需要添加或减少某一维度，这时 <code>squeeze</code> 和 <code>unsqueeze</code> 两个函数就派上用场了。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">print(b)</span><br><span class="line">print(b.shape)</span><br><span class="line"></span><br><span class="line">print(b.unsqueeze(<span class="number">1</span>))  <span class="comment"># 在第 1 维（下标从 0 开始）上增加 "1"</span></span><br><span class="line"><span class="comment"># 等价于 b[:,None]</span></span><br><span class="line">print(b[:, <span class="keyword">None</span>].shape)</span><br><span class="line"></span><br><span class="line">print(b.unsqueeze(<span class="number">-2</span>))  <span class="comment"># -2 表示倒数第二个维度</span></span><br><span class="line"></span><br><span class="line">c = b.view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">print(c)</span><br><span class="line">print(c.squeeze(<span class="number">0</span>))   <span class="comment"># 压缩第 0 维的 "1"</span></span><br><span class="line"></span><br><span class="line">print(c.squeeze())   <span class="comment"># 把所有维度为 "1" 的都压缩掉</span></span><br><span class="line"></span><br><span class="line">a[<span class="number">1</span>] = <span class="number">100</span></span><br><span class="line">print(b)   <span class="comment"># b 作为 view 之后的也跟着被修改</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx92fwjyk7j213s0icgmx.jpg" alt="">  </p><p>3 . <code>resize</code> 是另一种可用来调整 <code>size</code> 的方法，但是与 <code>view</code> 不同，它可以修改 tensor 的大小，如果新大小超过了原大小，会自动分配新的存储空间，而如果新大小小于原大小，则之前的数据依旧会被保存。  </p><p>Input:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">b = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">b.resize_(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">b.resize_(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx4.sinaimg.cn/mw690/79225320ly1fx92noka8jj214404udfy.jpg" alt=""></p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 通过 &lt;code&gt;tensor.view&lt;/code&gt; 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另一个也会跟着改变。  &lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a = t.arange(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;6&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(a.view(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;))&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 当某一维为 -1 时，会自动计算它的大小&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b = a.view(&lt;span class=&quot;number&quot;&gt;-1&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;print(b.shape)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（四）</title>
    <link href="http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch4/"/>
    <id>http://blog.keeplearning.group/2018/11/15/2018/11-15-pytorch4/</id>
    <published>2018-11-15T01:18:00.000Z</published>
    <updated>2018-11-15T07:29:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>1 . 从接口的角度来讲，对 tensor 的操作可分为两类：</p><ul><li><code>torch.function</code>，如 <code>torch.save</code> 等；</li><li>另一类是 <code>tensor.function</code>，如 <code>tensor.view</code> 等。</li></ul><p>为了方便使用，对 tensor 的大部分操作同时支持这两类接口，如 <code>torch.sum(torch.sum(a, b))</code> 与 <code>tensor.sum(a.sum(b))</code> 功能等价。</p><a id="more"></a><p>2 . 在 Pytorch 中新建 tensor 的方法具体有很多，如下表：  </p><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center"><code>Tensor(*sizes)</code></td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center"><code>tensor(data)</code></td><td style="text-align:center">类似 <code>np.array</code> 的构造函数</td></tr><tr><td style="text-align:center"><code>ones(*sizes)</code></td><td style="text-align:center">全 1 Tensor</td></tr><tr><td style="text-align:center"><code>zeros(*sizes)</code></td><td style="text-align:center">全 0  Tensor</td></tr><tr><td style="text-align:center"><code>eye(*sizes)</code></td><td style="text-align:center">对角线为 1，其他为 0</td></tr><tr><td style="text-align:center"><code>arange(s, e, steps)</code></td><td style="text-align:center">从 s  到 e，步长为 step</td></tr><tr><td style="text-align:center"><code>linspace(s, e, steps)</code></td><td style="text-align:center">从 s 到 e，均匀切分成 steps 份</td></tr><tr><td style="text-align:center"><code>rand/randn(*sizes)</code></td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center"><code>normal(mean, std)/uniform(from, to)</code></td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center"><code>randperm(m)</code></td><td style="text-align:center">随机排列</td></tr></tbody></table><p>这些创建方法都可以在创建的时候指定数据类型 <code>dtype</code> 和存放 device(cpu/gpu)。　　</p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.Tensor(<span class="number">2</span>, <span class="number">3</span>)   <span class="comment"># 指定 tensor 的形状，其数值取决于内存空间的状态，print 的时候可能 overflow</span></span><br><span class="line"></span><br><span class="line">b = t.Tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])   <span class="comment"># 用 list 的数据创建 tensor</span></span><br><span class="line">print(b)</span><br><span class="line"></span><br><span class="line">print(b.tolist())  <span class="comment"># 把 tensor 转为 list</span></span><br><span class="line"></span><br><span class="line">b_size = b.size()</span><br><span class="line">print(b_size)</span><br><span class="line"></span><br><span class="line">print(b.numel())   <span class="comment"># 返回 b 中元素总个数，等价于 b.nelement()</span></span><br><span class="line"></span><br><span class="line">c = t.Tensor(b_size)   <span class="comment"># 创建一个和 b 形状一样的 tensor</span></span><br><span class="line">d = t.Tensor((<span class="number">2</span>, <span class="number">3</span>))   <span class="comment"># 创建一个元素为 2 和 3 的 tensor</span></span><br><span class="line"></span><br><span class="line">print(c.shape)    <span class="comment"># 与 c.size() 等价</span></span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx1.sinaimg.cn/mw690/79225320ly1fx8r4lflf0j214i06o74m.jpg" alt="">  </p><p><code>t.Tensor(*sizes)</code>　创建 tensor 时，系统不会马上分配空间，只会计算剩余的内存是否足够使用，使用到 tensor 时才会分配，而其他操作都是在创建完 tensor 之后马上进行空间分配。  </p><p>Input:  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">print(t.ones(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.zeros(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.arange(<span class="number">1</span>, <span class="number">6</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">print(t.linspace(<span class="number">1</span>, <span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">print(t.randn(<span class="number">2</span>, <span class="number">3</span>, device=t.device(<span class="string">'cpu'</span>)))</span><br><span class="line"></span><br><span class="line">print(t.randperm(<span class="number">5</span>))    <span class="comment"># 长度为 5 的随机排列</span></span><br><span class="line"></span><br><span class="line">print(t.eye(<span class="number">2</span>, <span class="number">3</span>, dtype=t.int))  <span class="comment"># 对角线为 1，不要求行数与列数一致</span></span><br><span class="line"></span><br><span class="line">scalar = t.tensor(<span class="number">3.14159</span>)</span><br><span class="line">print(<span class="string">'scalar: %s, shape of scalar: %s'</span> % (scalar, scalar.shape))</span><br><span class="line"></span><br><span class="line">vector = t.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(<span class="string">'vector: %s, shape of vector: %s'</span> % (vector, vector.shape))</span><br><span class="line"></span><br><span class="line">tensor = t.Tensor(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">print(tensor.shape)</span><br><span class="line"></span><br><span class="line">matrix = t.tensor([[<span class="number">0.1</span>, <span class="number">1.2</span>], [<span class="number">2.2</span>, <span class="number">3.1</span>], [<span class="number">4.9</span>, <span class="number">5.2</span>]])</span><br><span class="line">print(matrix)</span><br><span class="line">print(matrix.shape)</span><br><span class="line"></span><br><span class="line">ten = t.tensor([[<span class="number">0.11111</span>, <span class="number">0.22222</span>, <span class="number">0.33333</span>]], dtype=t.float64, device=t.device(<span class="string">'cpu'</span>))</span><br><span class="line">print(ten)</span><br><span class="line"></span><br><span class="line">empty_tensor = t.tensor([])</span><br><span class="line">print(empty_tensor.shape)</span><br></pre></td></tr></table></figure><p>Output:  </p><p><img src="http://wx3.sinaimg.cn/mw690/79225320gy1fx8rusct9tj214h0kkgo1.jpg" alt="">  </p><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;1 . 从接口的角度来讲，对 tensor 的操作可分为两类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;torch.function&lt;/code&gt;，如 &lt;code&gt;torch.save&lt;/code&gt; 等；&lt;/li&gt;
&lt;li&gt;另一类是 &lt;code&gt;tensor.function&lt;/code&gt;，如 &lt;code&gt;tensor.view&lt;/code&gt; 等。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了方便使用，对 tensor 的大部分操作同时支持这两类接口，如 &lt;code&gt;torch.sum(torch.sum(a, b))&lt;/code&gt; 与 &lt;code&gt;tensor.sum(a.sum(b))&lt;/code&gt; 功能等价。&lt;/p&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch 学习笔记（三）</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch3/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-pytorch3/</id>
    <published>2018-11-14T14:00:00.000Z</published>
    <updated>2018-11-15T07:27:43.000Z</updated>
    
    <content type="html"><![CDATA[<p>对于常用的数据集，PyTorch 提供了封装好的接口供用户快速调用，这些主要保存在 <code>torchvision</code> 中。<code>torchvision</code> 实现了常用的图像数据加载功能，例如 Imagenet、CIFAR10、MNIST 等，以及常用的数据转换操作，这极大方便了数据加载，并且具有可重用性。<br><a id="more"></a><br>下面的程序训练网络 LeNet 对 CIFAR-10 数据集分类：  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision.transforms <span class="keyword">import</span> ToPILImage</span><br><span class="line"></span><br><span class="line">show = ToPILImage()  <span class="comment"># 可以把 Tensor 转成 Image，方便可视化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义对数据的预处理</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),  <span class="comment"># 转为 Tensor</span></span><br><span class="line">    transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),  <span class="comment"># 归一化</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">trainset = tv.datasets.CIFAR10(</span><br><span class="line">    root=<span class="string">'/home/abnerwang/tmp/data/'</span>,</span><br><span class="line">    train=<span class="keyword">True</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = t.utils.data.DataLoader(</span><br><span class="line">    trainset,</span><br><span class="line">    batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="keyword">True</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">testset = tv.datasets.CIFAR10(</span><br><span class="line">    <span class="string">'/home/abnerwang/tmp/data'</span>,</span><br><span class="line">    train=<span class="keyword">False</span>,</span><br><span class="line">    transform=transform)</span><br><span class="line"></span><br><span class="line">testloader = t.utils.data.DataLoader(</span><br><span class="line">    testset,</span><br><span class="line">    batch_size=<span class="number">4</span>,</span><br><span class="line">    shuffle=<span class="keyword">False</span>,</span><br><span class="line">    num_workers=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>)</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)  <span class="comment"># 定义优化器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU 训练网络</span></span><br><span class="line">device = t.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> t.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">net.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># 输入数据</span></span><br><span class="line">        images, labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 梯度清零</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward</span></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数</span></span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 打印 log 信息</span></span><br><span class="line">        <span class="comment"># loss 是一个 scalar，需要使用 loss.item() 来获取数值，不能使用 loss[idx]</span></span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>:  <span class="comment"># 每 2000 个 batch 打印一下训练状态</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> % (epoch + <span class="number">1</span>, i + <span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 网络在测试集上的效果</span></span><br><span class="line">correct = <span class="number">0</span>  <span class="comment"># 预测正确的图片数</span></span><br><span class="line">total = <span class="number">0</span>  <span class="comment"># 总共的图片数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由于测试的时候不需要求导，可以暂时关闭 autograd，提高速度，节约内存</span></span><br><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        images = images.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        outputs = net(images)</span><br><span class="line">        _, predicted = t.max(outputs, <span class="number">1</span>)   <span class="comment"># predicted 为每行概率最大值的索引，_ 为最大概率值</span></span><br><span class="line">        total += labels.size(<span class="number">0</span>)</span><br><span class="line">        correct += (predicted == labels).sum()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'10000 张测试集中的准确率为： %d %%'</span> % (<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure><hr><p>笔记来源：<a href="https://github.com/chenyuntc/pytorch-book" target="_blank" rel="noopener">《pytorch-book》</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于常用的数据集，PyTorch 提供了封装好的接口供用户快速调用，这些主要保存在 &lt;code&gt;torchvision&lt;/code&gt; 中。&lt;code&gt;torchvision&lt;/code&gt; 实现了常用的图像数据加载功能，例如 Imagenet、CIFAR10、MNIST 等，以及常用的数据转换操作，这极大方便了数据加载，并且具有可重用性。&lt;br&gt;
    
    </summary>
    
      <category term="编程语言" scheme="http://blog.keeplearning.group/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="PyTorch" scheme="http://blog.keeplearning.group/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>理解 batch_size</title>
    <link href="http://blog.keeplearning.group/2018/11/14/2018/11-14-batch_size/"/>
    <id>http://blog.keeplearning.group/2018/11/14/2018/11-14-batch_size/</id>
    <published>2018-11-14T12:52:00.000Z</published>
    <updated>2018-11-14T13:20:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>梯度下降主要有以下三种：  </p><ul><li>batch gradient descent：即批梯度下降，其计算开销大，计算速度慢，不支持在线学习；</li><li>stochastic gradient descent：即随机梯度下降，收敛性能不太好，可能 hit 不到最优点；</li><li>mini-batch gradient descent：即小批量梯度下降，通常所说的 batch_size 指的就是这个批量的大小。</li></ul><a id="more"></a><p>batch_size = 1 时称为在线学习。</p><p>batch_size 的选择，首先决定的是下降方向，如果数据集较小，完全可以采用全数据集的形式。</p><p>增大 batch_size 的好处：</p><ul><li>内存的利用率提高了，大矩阵乘法的并行化效率提高了；</li><li>跑完一次 epoch（即全数据集）所需迭代次数减少，对于相同数据量的处理速度进一步加快；</li><li>在一定范围内，batch_size 越大，其确定的下降方向就越准确，引起训练震荡越小。</li></ul><p>盲目增大 batch_size 的坏处：</p><ul><li>当数据集太大时，内存撑不住；</li><li>跑完全数据集所需迭代次数减少了，但要达到相同的精度，时间开销大，参数的修正更加缓慢；</li><li>batch_size 增大到一定程度，其确定的下降方向已基本不再变化。</li></ul><p><strong>总结：</strong>  </p><p>1) batch_size 太小，而类别又比较多的时候，可能会导致 loss 函数震荡而不收敛，尤其是网络比较复杂的时候；</p><p>2) 随着 batch_size 的增大，处理相同数据量的速度越快；</p><p>3) 随着 batch_size 的增大，达到相同的精度所需的训练 epoch 数量越多；</p><p>4) 由于上述两种因素的矛盾，batch_size 增加到某个时候，达到时间上的最优；</p><p>5) 由于最终收敛精度会陷入不同的局部极值，因此，batch_size 增大到某些时候，达到最终收敛精度上的最优；</p><p>6) 过大的 batch_size 会使网络收敛到一些不好的局部最优点，同样太小的 batch_size 会使网络收敛太慢、不易收敛等；</p><p>7) 具体 batch_size 的选取和训练集的样本数目有关。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;梯度下降主要有以下三种：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;batch gradient descent：即批梯度下降，其计算开销大，计算速度慢，不支持在线学习；&lt;/li&gt;
&lt;li&gt;stochastic gradient descent：即随机梯度下降，收敛性能不太好，可能 hit 不到最优点；&lt;/li&gt;
&lt;li&gt;mini-batch gradient descent：即小批量梯度下降，通常所说的 batch_size 指的就是这个批量的大小。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="机器学习与深度学习" scheme="http://blog.keeplearning.group/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://blog.keeplearning.group/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="batch_size" scheme="http://blog.keeplearning.group/tags/batch-size/"/>
    
  </entry>
  
</feed>
