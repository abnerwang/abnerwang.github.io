<!DOCTYPE html><html><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Qiyexuxu</title><meta name="description" content="王小平的个人博客"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://blog.keeplearning.group/atom.xml" title="Qiyexuxu"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/logo.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/abnerwang" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/about" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><ul class="home post-list"><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/18/2018/11-18-pytorch11/" class="post-title-link">PyTorch 学习笔记（十一）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 18, 2018</div><div class="post-content"><p>1 . 如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录，则使用 tensor.data 进行操作。  </p>
<p>Input:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">b = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">c = a * b</span><br><span class="line"></span><br><span class="line">print(a.data)  <span class="comment"># 还是同一个 tensor</span></span><br><span class="line"></span><br><span class="line">print(a.data.requires_grad)  <span class="comment"># 但是已经独立于计算图之外了</span></span><br><span class="line"></span><br><span class="line">d = a.data.sigmoid_()  <span class="comment"># sigmoid_ 是一个 inplace 操作，会修改 a 自身的值</span></span><br><span class="line">print(a)</span><br><span class="line">print(d.requires_grad)</span><br><span class="line"></span><br><span class="line">print(a.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 近似于 tensor = a.data，但是如果 tensor 被修改，backward 可能会报错</span></span><br><span class="line">tensor = a.detach()</span><br><span class="line">print(tensor.requires_grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计 tensor 的一些指标，不希望被记录</span></span><br><span class="line">mean = tensor.mean()</span><br><span class="line">std = tensor.std()</span><br><span class="line">maximum = tensor.max()</span><br><span class="line">print(mean, std, maximum)</span><br><span class="line"></span><br><span class="line">tensor[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">print(a)</span><br><span class="line"><span class="comment"># 下面会报错： RuntimeError: one of the variables needed for gradient</span></span><br><span class="line"><span class="comment">#              computation has been modified by an inplace operation.</span></span><br><span class="line"><span class="comment"># 因为 c = a * b，b 的梯度取决于 a，现在修改了 tensor，其实也就是修改了 a，梯度不再准确</span></span><br><span class="line"><span class="comment"># c.sum().backward()</span></span><br></pre></td></tr></table></figure></p></div><a href="/2018/11/18/2018/11-18-pytorch11/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/17/2018/11-17-pytorch10/" class="post-title-link">PyTorch 学习笔记（十）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 17, 2018</div><div class="post-content"><p>1 . <code>torch.autograd</code> 为方便用户使用而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在创建 tensor 的时候指定 requires_grad</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">a.requires_grad=<span class="keyword">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">b = t.zeros(<span class="number">3</span>, <span class="number">4</span>).requires_grad_()</span><br><span class="line"></span><br><span class="line">c = a.add(b)   <span class="comment"># 也可以写成 c = a + b</span></span><br><span class="line"></span><br><span class="line">d = c.sum()</span><br><span class="line">d.backward()   <span class="comment"># 反向传播</span></span><br><span class="line">print(d.requires_grad)   <span class="comment"># d 还是一个 requires_grad=True 的 tensor，对它的操作需要慎重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处虽然没有指定 c 需要求导，但 c 依赖于 a，而 a 需要求导，</span></span><br><span class="line"><span class="comment"># 因此 c 的 requires_grad 属性会自动设置为 True</span></span><br><span class="line">print(a.requires_grad, b.requires_grad, c.requires_grad)   <span class="comment"># (True, True, True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否为叶子节点</span></span><br><span class="line">print(a.is_leaf, b.is_leaf, c.is_leaf)   <span class="comment"># (True, True, False)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c.grad 是 None，因为 c 不是叶子节点，它的梯度是用来计算 a 的梯度，</span></span><br><span class="line"><span class="comment"># 所以虽然 c.requires_grad = True，但其梯度计算完之后就被释放了</span></span><br><span class="line">print(c.grad <span class="keyword">is</span> <span class="keyword">None</span>)    <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>2 . 验证 autograd 的计算结果与利用公式手动计算的结果一致。  </p>
<p>$y=x^2 \cdot e^x$ 的导函数是：$\frac{d_{y}}{d_{x}}=2x \cdot e^x + x^2 \cdot e^x$，来看看 autograd 的计算结果与手动求导的计算结果是否有误差。  </p></div><a href="/2018/11/17/2018/11-17-pytorch10/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/17/2018/11-17-pytorch9/" class="post-title-link">PyTorch 学习笔记（九）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 17, 2018</div><div class="post-content"><p>本节实现一个训练线性回归参数的例子，线性回归的损失函数为：$loss=\frac{1}{2} \sum_{i=1}^{N}(y_{i}-(wx_{i}+b))^2$，然后利用随机梯度下降法更新参数 $w$ 和 $b$ 来最小化损失函数，最终学得 $w$ 和 $b$ 的值。<br></p></div><a href="/2018/11/17/2018/11-17-pytorch9/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/17/2018/11-17-pytorch8/" class="post-title-link">PyTorch 学习笔记（八）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 17, 2018</div><div class="post-content"><p>1 . tensor 的数据结构分为头信息区和存储区，信息区主要保存着 tensor 形状、步长、数据类型等信息，而真正的数据则保存成连续数组存放在存储区。一般来说一个 tensor 有着与之对应的 storage，storage 是在 data 之上封装的接口，便于使用，而不同 tensor 的头信息一般不同，但却可能使用相同的数据。<br></p></div><a href="/2018/11/17/2018/11-17-pytorch8/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/16/2018/11-16-momentum/" class="post-title-link">理解 Momentum</a></h2><div class="tags"><a href="/tags/机器学习/" class="tag-title">#机器学习</a><a href="/tags/Momentum/" class="tag-title">#Momentum</a></div><div class="post-info">Nov 16, 2018</div><div class="post-content"><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>随机梯度下降（SGD）方法的一个缺点是其更新方向完全依赖于当前 batch 计算出的梯度，因而十分不稳定，Momentum 算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前 batch 的梯度微调最终的更新方向，这样一来，可以在一定程度上增加稳定性，从而学习得更快，并且还有一定摆脱局部最优的能力。数学表达式如下：  </p>
<p>$$v_{t}=\gamma v_{t-1}+\alpha \cdot \nabla_{\theta }J(\theta)$$  $$\theta=\theta -v_{t}$$  </p>
<p>Momentum 算法会观察历史梯度 $v_{t-1}$，若当前梯度的方向与历史梯度一致，表明当前样本不太可能为异常点，则会增强这个方向的梯度，若当前梯度与历史梯度方向不一致，则梯度会衰减。<br></p></div><a href="/2018/11/16/2018/11-16-momentum/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/16/2018/11-16-pytorch7/" class="post-title-link">PyTorch 学习笔记（七）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 16, 2018</div><div class="post-content"><p>1 . 逐元素操作的输入和输出形状一致。常见的操作如下表。  </p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">abs/sqrt/div/exp/fmod/log/pow…</td>
<td style="text-align:center">绝对值/平方根/除法/指数/求余/求幂…</td>
</tr>
<tr>
<td style="text-align:center">cos/sin/asin/atan2/cosh…</td>
<td style="text-align:center">相关三角函数</td>
</tr>
<tr>
<td style="text-align:center">ceil/round/floor/trunc</td>
<td style="text-align:center">上取整/四舍五入/下取整/只保留整数部分</td>
</tr>
<tr>
<td style="text-align:center">clamp(input, min, max)</td>
<td style="text-align:center">超过 min 和 max 部分截断</td>
</tr>
<tr>
<td style="text-align:center">sigmod/tanh..</td>
<td style="text-align:center">激活函数</td>
</tr>
</tbody>
</table>
<p>对于很多操作，例如 <code>div</code>、<code>mul</code>、<code>pow</code>、<code>fmod</code> 等， PyTorch 都实现了运算符重载，所以可以直接使用运算符。如 <code>a ** 2</code> 等价于 <code>torch.pow(a, 2)</code>，<code>a * 2</code> 等价于 <code>torch.mul(a, 2)</code>。<br></p></div><a href="/2018/11/16/2018/11-16-pytorch7/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/16/2018/11-16-pytorch6/" class="post-title-link">PyTorch 学习笔记（六）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 16, 2018</div><div class="post-content"><p>1 . Tensor 支持与 <code>numpy.ndarray</code> 类似的索引操作，如无特殊说明，索引出来的结果与原 tensor 共享内存，也即修改一个，另一个也会跟着修改。  </p>
<p>Input:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>])    <span class="comment"># 第 0 行，下标从 0 开始</span></span><br><span class="line"></span><br><span class="line">print(a[:, <span class="number">0</span>])     <span class="comment"># 第 0 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>, <span class="number">2</span>])   <span class="comment"># 第 0 行第 2 个元素，等价于 a[0][2]</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>][<span class="number">-1</span>])   <span class="comment"># 第 0 行最后一个元素</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>])    <span class="comment"># 前两行</span></span><br><span class="line"></span><br><span class="line">print(a[:<span class="number">2</span>, <span class="number">0</span>:<span class="number">2</span>])   <span class="comment"># 前两行，第 0,1 列</span></span><br><span class="line"></span><br><span class="line">print(a[<span class="number">0</span>:<span class="number">1</span>, :<span class="number">2</span>])  <span class="comment"># 第 0 行，前两列</span></span><br><span class="line">print(a[<span class="number">0</span>, :<span class="number">2</span>])   <span class="comment"># 注意两者的区别，形状不同</span></span><br></pre></td></tr></table></figure></div><a href="/2018/11/16/2018/11-16-pytorch6/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/15/2018/11-15-pytorch5/" class="post-title-link">PyTorch 学习笔记（五）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 15, 2018</div><div class="post-content"><p>1 . 通过 <code>tensor.view</code> 方法可以调整 tensor 的形状，但必须保证调整前后元素总数一致，返回的新 tensor 与源 tensor 共享内存，即更改其中一个，另一个也会跟着改变。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = t.arange(<span class="number">0</span>, <span class="number">6</span>)</span><br><span class="line">print(a.view(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当某一维为 -1 时，会自动计算它的大小</span></span><br><span class="line">b = a.view(<span class="number">-1</span>, <span class="number">3</span>)</span><br><span class="line">print(b.shape)</span><br></pre></td></tr></table></figure></div><a href="/2018/11/15/2018/11-15-pytorch5/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/15/2018/11-15-pytorch4/" class="post-title-link">PyTorch 学习笔记（四）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 15, 2018</div><div class="post-content"><p>1 . 从接口的角度来讲，对 tensor 的操作可分为两类：</p>
<ul>
<li><code>torch.function</code>，如 <code>torch.save</code> 等；</li>
<li>另一类是 <code>tensor.function</code>，如 <code>tensor.view</code> 等。</li>
</ul>
<p>为了方便使用，对 tensor 的大部分操作同时支持这两类接口，如 <code>torch.sum(torch.sum(a, b))</code> 与 <code>tensor.sum(a.sum(b))</code> 功能等价。</p></div><a href="/2018/11/15/2018/11-15-pytorch4/" class="read-more">MORE</a></article></li><li class="post-list-item"><article class="post-block"><h2 class="post-title"><a href="/2018/11/14/2018/11-14-pytorch3/" class="post-title-link">PyTorch 学习笔记（三）</a></h2><div class="tags"><a href="/tags/PyTorch/" class="tag-title">#PyTorch</a></div><div class="post-info">Nov 14, 2018</div><div class="post-content"><p>对于常用的数据集，PyTorch 提供了封装好的接口供用户快速调用，这些主要保存在 <code>torchvision</code> 中。<code>torchvision</code> 实现了常用的图像数据加载功能，例如 Imagenet、CIFAR10、MNIST 等，以及常用的数据转换操作，这极大方便了数据加载，并且具有可重用性。<br></p></div><a href="/2018/11/14/2018/11-14-pytorch3/" class="read-more">MORE</a></article></li></ul></main><footer><div class="paginator"><a href="/" class="prev">PREV</a><a href="/page/3/" class="next">NEXT</a></div><div class="copyright"><p>© 2014 - 2019 <a href="http://blog.keeplearning.group">David Wang</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
tex2jax: { inlineMath: [ ["$", "$"], ["\\\(","\\)"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno",skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']},
TeX: { noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } }, Macros: { href: "{}" } },
messageStyle: "none"
});
</script></body></html>